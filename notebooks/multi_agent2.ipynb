{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synchronous Multi-Agent Models\n",
    "\n",
    "Here, I'll put together a multi-agent environment that has synchronous actions to bridge the gap to our AlphaDow implementation.\n",
    "\n",
    "## Environment\n",
    "\n",
    "To make it more similar to AlphaDow, we'll devise a simple, two-stage scheduling problem. Both stages: Stage 0 and Stage 1 are represented by separate agents. Stage 0 produces long campaigns while Stage 1 packs the products in smaller, shorter time steps. Each Stage 0 action takes two time steps while each Stage 1 action takes 1 time step. If Stage 1 chooses to pack an item that is not yet in inventory, nothing happens. Actions for each stage are simply whether or not to produce a product.\n",
    "\n",
    "There are two products available for Stage 0 and Stage 1 can pack each into one of two different types. For every time, an order is received for a finished product which must be satisfied to receive a reward. Each fulfilled order yields a reward of +10, but is lost if it can't be fulfilled. Additionally, inventory costs of -1 are incurred for every unit of inventory the system is holding.\n",
    "\n",
    "Observations are the total demand and inventory for each product (**look into allowing individual orders to comprise the state too**). Each agent receives the same observation.\n",
    "\n",
    "Episodes last for 30 time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStageSchedSyncEnv(MultiAgentEnv):\n",
    "    '''\n",
    "    Actions and rewards are taken synchronously. For stage 0, odd timesteps\n",
    "    don't provide any update.\n",
    "    '''\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.num_stages = 2\n",
    "        self.num_agents = 2\n",
    "        self.n_zemis = 2\n",
    "        self.n_zfins = 4\n",
    "        self.zemis = np.arange(self.n_zemis)\n",
    "        self.zfins = np.arange(self.n_zfins)\n",
    "        self.zemi_qty = 2\n",
    "        self.zfin_qty = 1\n",
    "        \n",
    "        self.zfin_zemi_map = {3: 1,\n",
    "                              2: 1,\n",
    "                              1: 0,\n",
    "                              0: 0}\n",
    "        \n",
    "        self.max_steps = 30\n",
    "        self.obs_dim = 2 * (self.n_zemis + self.n_zfins) + 1\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=self.max_steps, \n",
    "            shape=(self.obs_dim,))\n",
    "        self.action_space0 = spaces.Discrete(self.n_zemis)\n",
    "        self.action_space1 = spaces.Discrete(self.n_zfins)\n",
    "        self.action_space = spaces.Dict({\n",
    "            0: self.action_space0,\n",
    "            1: self.action_space1\n",
    "        })\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def map_zfin_to_zemi(self, zfin):\n",
    "        return self.zfin_zemi_map[zfin]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.ts = 0\n",
    "        self.planned_production = {i: np.zeros((self.max_steps, j))\n",
    "            for i, j in zip(range(self.num_agents), [self.n_zemis, self.n_zfins])}\n",
    "        self.zemi_inv = np.zeros((self.max_steps, self.n_zemis))\n",
    "        self.zemi_inv[0] += 1\n",
    "        self.zfin_inv = np.zeros((self.max_steps, self.n_zfins))\n",
    "        self.zfin_inv[0] += 1\n",
    "        self.inventory = np.zeros((self.max_steps, self.n_zemis + self.n_zfins))\n",
    "        self.demand = np.random.choice(np.arange(self.n_zfins), size=self.max_steps)\n",
    "        self.state = self._update_state()\n",
    "        return self.state\n",
    "    \n",
    "    def _update_state(self):\n",
    "        if self.ts > 0:\n",
    "            self.zemi_inv[self.ts] += self.zemi_inv[self.ts-1]\n",
    "            self.zfin_inv[self.ts] += self.zfin_inv[self.ts-1]\n",
    "        self.inventory[self.ts] += np.hstack(\n",
    "            [self.zemi_inv[self.ts], self.zfin_inv[self.ts]])\n",
    "        prod_vec = np.hstack([v[self.ts-1] for v in self.planned_production.values()])\n",
    "        return {i: np.hstack([prod_vec, self.demand[self.ts], self.inventory[self.ts]]) for \n",
    "               i in range(self.num_agents)}\n",
    "        \n",
    "    \n",
    "    def step(self, action_dict):\n",
    "        r = 0\n",
    "        done = False\n",
    "        d = self.demand[self.ts] # One unit of demand\n",
    "        # Begin production to be completed at the next time step.\n",
    "        for i in range(self.num_agents):\n",
    "            if i == 0 and self.ts % 2 != 0:\n",
    "                self.planned_production[i][self.ts] += self.planned_production[i][self.ts-1]\n",
    "                continue\n",
    "                \n",
    "            action = action_dict[i]\n",
    "            # Add finished products to inventory\n",
    "            if i == 0 and self.ts > 0:\n",
    "                self.zemi_inv[self.ts] += self.planned_production[i][self.ts-1]\n",
    "            elif i > 0 and self.ts > 0:\n",
    "                self.zfin_inv[self.ts] += self.planned_production[i][self.ts-1]\n",
    "            \n",
    "            # Log actions\n",
    "            if i == 0:\n",
    "                self.planned_production[i][self.ts][action] += self.zemi_qty\n",
    "            else:\n",
    "                # Check raw materials first\n",
    "                zemi = self.map_zfin_to_zemi(action)\n",
    "                if self.zemi_inv[self.ts, zemi] >= self.zfin_qty:\n",
    "                    self.planned_production[i][self.ts][action] += self.zfin_qty\n",
    "                    self.zemi_inv[self.ts, zemi] -= self.zfin_qty\n",
    "                               \n",
    "        # Fulfill demand\n",
    "        if self.zfin_inv[self.ts, d] > 0:\n",
    "            self.zfin_inv[self.ts, d] -= 1\n",
    "            r += 10\n",
    "            \n",
    "        # Calculate inventory costs\n",
    "        r -= self.zfin_inv[self.ts].sum() + self.zemi_inv[self.ts].sum()\n",
    "        \n",
    "        self.ts += 1\n",
    "        self.state = self._update_state()\n",
    "        \n",
    "        if self.ts >= self.max_steps-1:\n",
    "            done = True\n",
    "        \n",
    "        rewards = {i: r for i in range(self.num_agents)}\n",
    "        dones = {i: done for i in range(self.num_agents)}\n",
    "        dones['__all__'] = done\n",
    "        info = {i: {} for i in range(self.num_agents)}\n",
    "        \n",
    "        return self.state, rewards, dones, info\n",
    "        \n",
    "    \n",
    "env = MultiStageSchedSyncEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test env\n",
    "env.reset()\n",
    "while True:\n",
    "#     print(env.ts)\n",
    "    actions = env.action_space.sample()\n",
    "    s, r, d, _ = env.step(actions)\n",
    "    if d['__all__']:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: array([2., 0., 0., 0., 0., 0., 1., 0., 5., 1., 1., 2., 3.]),\n",
       "  1: array([2., 0., 0., 0., 0., 0., 1., 0., 5., 1., 1., 2., 3.])},\n",
       " {0: -2.0, 1: -2.0},\n",
       " {0: True, 1: True, '__all__': True},\n",
       " {0: {}, 1: {}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s, r, d, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator(*args, **kwargs):\n",
    "    return MultiStageSchedSyncEnv()\n",
    "env_name = \"SyncSched\"\n",
    "tune.register_env(env_name, env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env_creator()\n",
    "\n",
    "def policy_gen(agent_id):\n",
    "    if agent_id == 0:\n",
    "        return (None, env.observation_space, env.action_space0, {})\n",
    "    elif agent_id == 1:\n",
    "        return (None, env.observation_space, env.action_space1, {})\n",
    "\n",
    "policy_graphs = {f'agent-{i}': policy_gen(i) for i in range(env.num_agents)}\n",
    "\n",
    "def policy_mapping_fn(agent_id):\n",
    "    return f'agent-{agent_id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-14 15:49:24,767\tINFO resource_spec.py:212 -- Starting Ray with 32.37 GiB memory available for workers and up to 16.19 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-12-14 15:49:25,027\tWARNING services.py:923 -- Redis failed to start, retrying now.\n",
      "2020-12-14 15:49:25,278\tINFO services.py:1165 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 1.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2283)\u001b[0m 2020-12-14 15:49:27,991\tINFO trainer.py:585 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=2283)\u001b[0m 2020-12-14 15:49:27,991\tINFO trainer.py:612 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=2283)\u001b[0m 2020-12-14 15:49:34,132\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-49-41\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 120.0\n",
      "  episode_reward_mean: -103.38888888888889\n",
      "  episode_reward_min: -384.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 144\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6468084454536438\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.050289809703826904\n",
      "        model: {}\n",
      "        policy_loss: -0.09830820560455322\n",
      "        total_loss: 2408.65625\n",
      "        vf_explained_var: -1.1175870895385742e-07\n",
      "        vf_loss: 2408.744140625\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.3242270946502686\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.06460921466350555\n",
      "        model: {}\n",
      "        policy_loss: -0.12160031497478485\n",
      "        total_loss: 2431.98193359375\n",
      "        vf_explained_var: -5.4016709327697754e-08\n",
      "        vf_loss: 2432.09033203125\n",
      "    num_steps_sampled: 4200\n",
      "    num_steps_trained: 4200\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.93636363636362\n",
      "    ram_util_percent: 5.200000000000001\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 60.0\n",
      "    agent-1: 60.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: -51.69444444444444\n",
      "    agent-1: -51.69444444444444\n",
      "  policy_reward_min:\n",
      "    agent-0: -192.0\n",
      "    agent-1: -192.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1409740752048388\n",
      "    mean_inference_ms: 1.2077336761743263\n",
      "    mean_processing_ms: 0.2953598336495021\n",
      "  time_since_restore: 7.480388164520264\n",
      "  time_this_iter_s: 7.480388164520264\n",
      "  time_total_s: 7.480388164520264\n",
      "  timers:\n",
      "    learn_throughput: 912.545\n",
      "    learn_time_ms: 4602.512\n",
      "    load_throughput: 37837.492\n",
      "    load_time_ms: 111.001\n",
      "    sample_throughput: 1679.855\n",
      "    sample_time_ms: 2500.216\n",
      "    update_time_ms: 2.884\n",
      "  timestamp: 1607960981\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4200\n",
      "  training_iteration: 1\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.48039</td><td style=\"text-align: right;\">4200</td><td style=\"text-align: right;\">-103.389</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-49-47\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 146.0\n",
      "  episode_reward_mean: -77.5\n",
      "  episode_reward_min: -384.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 288\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5870267152786255\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.06724965572357178\n",
      "        model: {}\n",
      "        policy_loss: -0.1476057767868042\n",
      "        total_loss: 1697.93408203125\n",
      "        vf_explained_var: 1.5087425708770752e-07\n",
      "        vf_loss: 1698.0615234375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.2660149335861206\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.06826834380626678\n",
      "        model: {}\n",
      "        policy_loss: -0.177049458026886\n",
      "        total_loss: 1699.42041015625\n",
      "        vf_explained_var: -6.891787052154541e-08\n",
      "        vf_loss: 1699.5771484375\n",
      "    num_steps_sampled: 8400\n",
      "    num_steps_trained: 8400\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.94444444444444\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 73.0\n",
      "    agent-1: 73.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: -38.75\n",
      "    agent-1: -38.75\n",
      "  policy_reward_min:\n",
      "    agent-0: -192.0\n",
      "    agent-1: -192.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13894270420585178\n",
      "    mean_inference_ms: 1.1937013342595306\n",
      "    mean_processing_ms: 0.28983068142733975\n",
      "  time_since_restore: 13.57662582397461\n",
      "  time_this_iter_s: 6.096237659454346\n",
      "  time_total_s: 13.57662582397461\n",
      "  timers:\n",
      "    learn_throughput: 1006.44\n",
      "    learn_time_ms: 4173.126\n",
      "    load_throughput: 73641.65\n",
      "    load_time_ms: 57.033\n",
      "    sample_throughput: 1737.452\n",
      "    sample_time_ms: 2417.333\n",
      "    update_time_ms: 3.007\n",
      "  timestamp: 1607960987\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8400\n",
      "  training_iteration: 2\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         13.5766</td><td style=\"text-align: right;\">8400</td><td style=\"text-align: right;\">   -77.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-49-53\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 146.0\n",
      "  episode_reward_mean: -60.55555555555556\n",
      "  episode_reward_min: -274.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 432\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5488165020942688\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.06546467542648315\n",
      "        model: {}\n",
      "        policy_loss: -0.14890405535697937\n",
      "        total_loss: 1142.114013671875\n",
      "        vf_explained_var: 0.11276251077651978\n",
      "        vf_loss: 1142.2335205078125\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.2130303382873535\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.06614577770233154\n",
      "        model: {}\n",
      "        policy_loss: -0.19370155036449432\n",
      "        total_loss: 1211.03173828125\n",
      "        vf_explained_var: 2.1047890186309814e-07\n",
      "        vf_loss: 1211.1956787109375\n",
      "    num_steps_sampled: 12600\n",
      "    num_steps_trained: 12600\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.26666666666667\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 73.0\n",
      "    agent-1: 73.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: -30.27777777777778\n",
      "    agent-1: -30.27777777777778\n",
      "  policy_reward_min:\n",
      "    agent-0: -137.0\n",
      "    agent-1: -137.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13876212907860908\n",
      "    mean_inference_ms: 1.1837496073068368\n",
      "    mean_processing_ms: 0.288867727211711\n",
      "  time_since_restore: 19.742619276046753\n",
      "  time_this_iter_s: 6.1659934520721436\n",
      "  time_total_s: 19.742619276046753\n",
      "  timers:\n",
      "    learn_throughput: 1034.594\n",
      "    learn_time_ms: 4059.564\n",
      "    load_throughput: 107376.473\n",
      "    load_time_ms: 39.115\n",
      "    sample_throughput: 1762.583\n",
      "    sample_time_ms: 2382.866\n",
      "    update_time_ms: 3.014\n",
      "  timestamp: 1607960993\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12600\n",
      "  training_iteration: 3\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         19.7426</td><td style=\"text-align: right;\">12600</td><td style=\"text-align: right;\">-60.5556</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-50-00\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 138.0\n",
      "  episode_reward_mean: -67.5374149659864\n",
      "  episode_reward_min: -324.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 579\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5353095531463623\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04791826754808426\n",
      "        model: {}\n",
      "        policy_loss: -0.140385240316391\n",
      "        total_loss: 896.4613037109375\n",
      "        vf_explained_var: 0.22712361812591553\n",
      "        vf_loss: 896.5693359375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1896581649780273\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.051693327724933624\n",
      "        model: {}\n",
      "        policy_loss: -0.18439309298992157\n",
      "        total_loss: 937.5867919921875\n",
      "        vf_explained_var: 0.16499167680740356\n",
      "        vf_loss: 937.736328125\n",
      "    num_steps_sampled: 16800\n",
      "    num_steps_trained: 16800\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.33333333333333\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 69.0\n",
      "    agent-1: 69.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: -33.7687074829932\n",
      "    agent-1: -33.7687074829932\n",
      "  policy_reward_min:\n",
      "    agent-0: -162.0\n",
      "    agent-1: -162.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13797314246623266\n",
      "    mean_inference_ms: 1.1725937377297653\n",
      "    mean_processing_ms: 0.2875184470795737\n",
      "  time_since_restore: 25.818738222122192\n",
      "  time_this_iter_s: 6.0761189460754395\n",
      "  time_total_s: 25.818738222122192\n",
      "  timers:\n",
      "    learn_throughput: 1050.978\n",
      "    learn_time_ms: 3996.279\n",
      "    load_throughput: 139504.828\n",
      "    load_time_ms: 30.106\n",
      "    sample_throughput: 1787.09\n",
      "    sample_time_ms: 2350.19\n",
      "    update_time_ms: 3.004\n",
      "  timestamp: 1607961000\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16800\n",
      "  training_iteration: 4\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         25.8187</td><td style=\"text-align: right;\">16800</td><td style=\"text-align: right;\">-67.5374</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-50-06\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 144.0\n",
      "  episode_reward_mean: -45.486111111111114\n",
      "  episode_reward_min: -282.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 723\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5319505929946899\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.037711530923843384\n",
      "        model: {}\n",
      "        policy_loss: -0.12071430683135986\n",
      "        total_loss: 732.2716064453125\n",
      "        vf_explained_var: 0.30993056297302246\n",
      "        vf_loss: 732.3541870117188\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1531298160552979\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04135729372501373\n",
      "        model: {}\n",
      "        policy_loss: -0.16786012053489685\n",
      "        total_loss: 749.5763549804688\n",
      "        vf_explained_var: 0.29157596826553345\n",
      "        vf_loss: 749.7022705078125\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.575\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 72.0\n",
      "    agent-1: 72.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: -22.743055555555557\n",
      "    agent-1: -22.743055555555557\n",
      "  policy_reward_min:\n",
      "    agent-0: -141.0\n",
      "    agent-1: -141.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13744066551981135\n",
      "    mean_inference_ms: 1.164181734308574\n",
      "    mean_processing_ms: 0.2860781726783578\n",
      "  time_since_restore: 31.851855278015137\n",
      "  time_this_iter_s: 6.033117055892944\n",
      "  time_total_s: 31.851855278015137\n",
      "  timers:\n",
      "    learn_throughput: 1062.846\n",
      "    learn_time_ms: 3951.655\n",
      "    load_throughput: 169854.086\n",
      "    load_time_ms: 24.727\n",
      "    sample_throughput: 1803.757\n",
      "    sample_time_ms: 2328.473\n",
      "    update_time_ms: 3.034\n",
      "  timestamp: 1607961006\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 5\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         31.8519</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">-45.4861</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-50-12\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 138.0\n",
      "  episode_reward_mean: -43.291666666666664\n",
      "  episode_reward_min: -260.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 867\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5384894013404846\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.028982579708099365\n",
      "        model: {}\n",
      "        policy_loss: -0.11979325860738754\n",
      "        total_loss: 665.7239990234375\n",
      "        vf_explained_var: 0.33249449729919434\n",
      "        vf_loss: 665.7998046875\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1344114542007446\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03247467428445816\n",
      "        model: {}\n",
      "        policy_loss: -0.16352644562721252\n",
      "        total_loss: 663.693115234375\n",
      "        vf_explained_var: 0.33509641885757446\n",
      "        vf_loss: 663.8073120117188\n",
      "    num_steps_sampled: 25200\n",
      "    num_steps_trained: 25200\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.51111111111112\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 69.0\n",
      "    agent-1: 69.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: -21.645833333333332\n",
      "    agent-1: -21.645833333333332\n",
      "  policy_reward_min:\n",
      "    agent-0: -130.0\n",
      "    agent-1: -130.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1370666263851103\n",
      "    mean_inference_ms: 1.1590628833405223\n",
      "    mean_processing_ms: 0.28589113985683806\n",
      "  time_since_restore: 37.945629596710205\n",
      "  time_this_iter_s: 6.093774318695068\n",
      "  time_total_s: 37.945629596710205\n",
      "  timers:\n",
      "    learn_throughput: 1068.789\n",
      "    learn_time_ms: 3929.68\n",
      "    load_throughput: 198439.207\n",
      "    load_time_ms: 21.165\n",
      "    sample_throughput: 1813.445\n",
      "    sample_time_ms: 2316.033\n",
      "    update_time_ms: 3.046\n",
      "  timestamp: 1607961012\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25200\n",
      "  training_iteration: 6\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         37.9456</td><td style=\"text-align: right;\">25200</td><td style=\"text-align: right;\">-43.2917</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-50-18\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 178.0\n",
      "  episode_reward_mean: -35.5\n",
      "  episode_reward_min: -386.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 1011\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5427248477935791\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019328322261571884\n",
      "        model: {}\n",
      "        policy_loss: -0.10370069742202759\n",
      "        total_loss: 900.68994140625\n",
      "        vf_explained_var: 0.32090306282043457\n",
      "        vf_loss: 900.7495727539062\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1343854665756226\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023944523185491562\n",
      "        model: {}\n",
      "        policy_loss: -0.15151676535606384\n",
      "        total_loss: 899.5591430664062\n",
      "        vf_explained_var: 0.3237397074699402\n",
      "        vf_loss: 899.6561279296875\n",
      "    num_steps_sampled: 29400\n",
      "    num_steps_trained: 29400\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.04444444444444\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 89.0\n",
      "    agent-1: 89.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: -17.75\n",
      "    agent-1: -17.75\n",
      "  policy_reward_min:\n",
      "    agent-0: -193.0\n",
      "    agent-1: -193.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1370467989800621\n",
      "    mean_inference_ms: 1.1538876297806118\n",
      "    mean_processing_ms: 0.2861304643652583\n",
      "  time_since_restore: 43.96573090553284\n",
      "  time_this_iter_s: 6.020101308822632\n",
      "  time_total_s: 43.96573090553284\n",
      "  timers:\n",
      "    learn_throughput: 1075.975\n",
      "    learn_time_ms: 3903.435\n",
      "    load_throughput: 226082.745\n",
      "    load_time_ms: 18.577\n",
      "    sample_throughput: 1820.342\n",
      "    sample_time_ms: 2307.259\n",
      "    update_time_ms: 3.035\n",
      "  timestamp: 1607961018\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29400\n",
      "  training_iteration: 7\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         43.9657</td><td style=\"text-align: right;\">29400</td><td style=\"text-align: right;\">   -35.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-50-24\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 164.0\n",
      "  episode_reward_mean: -35.034013605442176\n",
      "  episode_reward_min: -330.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 1158\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5493513345718384\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017868708819150925\n",
      "        model: {}\n",
      "        policy_loss: -0.10665792226791382\n",
      "        total_loss: 774.7830810546875\n",
      "        vf_explained_var: 0.3720773160457611\n",
      "        vf_loss: 774.84912109375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1330616474151611\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0163329616189003\n",
      "        model: {}\n",
      "        policy_loss: -0.12783633172512054\n",
      "        total_loss: 761.8055419921875\n",
      "        vf_explained_var: 0.372951865196228\n",
      "        vf_loss: 761.8775634765625\n",
      "    num_steps_sampled: 33600\n",
      "    num_steps_trained: 33600\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.6875\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 82.0\n",
      "    agent-1: 82.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: -17.517006802721088\n",
      "    agent-1: -17.517006802721088\n",
      "  policy_reward_min:\n",
      "    agent-0: -165.0\n",
      "    agent-1: -165.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13686070764177954\n",
      "    mean_inference_ms: 1.149689580409175\n",
      "    mean_processing_ms: 0.28571421108944584\n",
      "  time_since_restore: 50.03089141845703\n",
      "  time_this_iter_s: 6.065160512924194\n",
      "  time_total_s: 50.03089141845703\n",
      "  timers:\n",
      "    learn_throughput: 1079.036\n",
      "    learn_time_ms: 3892.365\n",
      "    load_throughput: 252275.414\n",
      "    load_time_ms: 16.648\n",
      "    sample_throughput: 1827.786\n",
      "    sample_time_ms: 2297.862\n",
      "    update_time_ms: 2.994\n",
      "  timestamp: 1607961024\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33600\n",
      "  training_iteration: 8\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         50.0309</td><td style=\"text-align: right;\">33600</td><td style=\"text-align: right;\"> -35.034</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-50-30\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 180.0\n",
      "  episode_reward_mean: -40.611111111111114\n",
      "  episode_reward_min: -422.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 1302\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5515351295471191\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018439875915646553\n",
      "        model: {}\n",
      "        policy_loss: -0.10486474633216858\n",
      "        total_loss: 965.5531005859375\n",
      "        vf_explained_var: 0.3696783185005188\n",
      "        vf_loss: 965.615966796875\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.133829951286316\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01671167090535164\n",
      "        model: {}\n",
      "        policy_loss: -0.13209012150764465\n",
      "        total_loss: 948.021484375\n",
      "        vf_explained_var: 0.3731050193309784\n",
      "        vf_loss: 948.096435546875\n",
      "    num_steps_sampled: 37800\n",
      "    num_steps_trained: 37800\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.6\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 90.0\n",
      "    agent-1: 90.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: -20.305555555555557\n",
      "    agent-1: -20.305555555555557\n",
      "  policy_reward_min:\n",
      "    agent-0: -211.0\n",
      "    agent-1: -211.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13673157615390175\n",
      "    mean_inference_ms: 1.1462878509857313\n",
      "    mean_processing_ms: 0.2849003002416096\n",
      "  time_since_restore: 56.11445450782776\n",
      "  time_this_iter_s: 6.0835630893707275\n",
      "  time_total_s: 56.11445450782776\n",
      "  timers:\n",
      "    learn_throughput: 1080.447\n",
      "    learn_time_ms: 3887.279\n",
      "    load_throughput: 273177.235\n",
      "    load_time_ms: 15.375\n",
      "    sample_throughput: 1835.019\n",
      "    sample_time_ms: 2288.805\n",
      "    update_time_ms: 2.997\n",
      "  timestamp: 1607961030\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37800\n",
      "  training_iteration: 9\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         56.1145</td><td style=\"text-align: right;\">37800</td><td style=\"text-align: right;\">-40.6111</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-50-36\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 206.0\n",
      "  episode_reward_mean: -32.083333333333336\n",
      "  episode_reward_min: -298.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 1446\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5489659309387207\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01821638084948063\n",
      "        model: {}\n",
      "        policy_loss: -0.10797180235385895\n",
      "        total_loss: 860.3479614257812\n",
      "        vf_explained_var: 0.3666471540927887\n",
      "        vf_loss: 860.4144287109375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1241573095321655\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016270939260721207\n",
      "        model: {}\n",
      "        policy_loss: -0.1371474266052246\n",
      "        total_loss: 854.386474609375\n",
      "        vf_explained_var: 0.3729844391345978\n",
      "        vf_loss: 854.468017578125\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.44444444444444\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 103.0\n",
      "    agent-1: 103.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: -16.041666666666668\n",
      "    agent-1: -16.041666666666668\n",
      "  policy_reward_min:\n",
      "    agent-0: -149.0\n",
      "    agent-1: -149.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13659510292235044\n",
      "    mean_inference_ms: 1.1426949533392095\n",
      "    mean_processing_ms: 0.28441773821415683\n",
      "  time_since_restore: 62.16512942314148\n",
      "  time_this_iter_s: 6.050674915313721\n",
      "  time_total_s: 62.16512942314148\n",
      "  timers:\n",
      "    learn_throughput: 1082.365\n",
      "    learn_time_ms: 3880.39\n",
      "    load_throughput: 296661.342\n",
      "    load_time_ms: 14.158\n",
      "    sample_throughput: 1841.024\n",
      "    sample_time_ms: 2281.339\n",
      "    update_time_ms: 2.961\n",
      "  timestamp: 1607961036\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 10\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         62.1651</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\">-32.0833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-50-42\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 244.0\n",
      "  episode_reward_mean: -25.26530612244898\n",
      "  episode_reward_min: -312.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 1593\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5487400889396667\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017869209870696068\n",
      "        model: {}\n",
      "        policy_loss: -0.10476996004581451\n",
      "        total_loss: 691.6472778320312\n",
      "        vf_explained_var: 0.39779701828956604\n",
      "        vf_loss: 691.7113647460938\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1161695718765259\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01721995510160923\n",
      "        model: {}\n",
      "        policy_loss: -0.137337327003479\n",
      "        total_loss: 692.7508544921875\n",
      "        vf_explained_var: 0.40516209602355957\n",
      "        vf_loss: 692.829345703125\n",
      "    num_steps_sampled: 46200\n",
      "    num_steps_trained: 46200\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.15555555555555\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 122.0\n",
      "    agent-1: 122.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: -12.63265306122449\n",
      "    agent-1: -12.63265306122449\n",
      "  policy_reward_min:\n",
      "    agent-0: -156.0\n",
      "    agent-1: -156.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13640037700758015\n",
      "    mean_inference_ms: 1.141488003817982\n",
      "    mean_processing_ms: 0.2841803995718691\n",
      "  time_since_restore: 68.2332615852356\n",
      "  time_this_iter_s: 6.068132162094116\n",
      "  time_total_s: 68.2332615852356\n",
      "  timers:\n",
      "    learn_throughput: 1105.059\n",
      "    learn_time_ms: 3800.701\n",
      "    load_throughput: 1243370.751\n",
      "    load_time_ms: 3.378\n",
      "    sample_throughput: 1861.989\n",
      "    sample_time_ms: 2255.652\n",
      "    update_time_ms: 2.959\n",
      "  timestamp: 1607961042\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46200\n",
      "  training_iteration: 11\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         68.2333</td><td style=\"text-align: right;\">46200</td><td style=\"text-align: right;\">-25.2653</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-50-49\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 190.0\n",
      "  episode_reward_mean: -20.76388888888889\n",
      "  episode_reward_min: -396.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 1737\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5437732338905334\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01833447441458702\n",
      "        model: {}\n",
      "        policy_loss: -0.1069386899471283\n",
      "        total_loss: 629.4480590820312\n",
      "        vf_explained_var: 0.3998923599720001\n",
      "        vf_loss: 629.51318359375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1247260570526123\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016251958906650543\n",
      "        model: {}\n",
      "        policy_loss: -0.1330394446849823\n",
      "        total_loss: 625.0473022460938\n",
      "        vf_explained_var: 0.4195806384086609\n",
      "        vf_loss: 625.124755859375\n",
      "    num_steps_sampled: 50400\n",
      "    num_steps_trained: 50400\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.5875\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 95.0\n",
      "    agent-1: 95.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: -10.381944444444445\n",
      "    agent-1: -10.381944444444445\n",
      "  policy_reward_min:\n",
      "    agent-0: -198.0\n",
      "    agent-1: -198.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13639600381532757\n",
      "    mean_inference_ms: 1.1406189850951236\n",
      "    mean_processing_ms: 0.28433843356518623\n",
      "  time_since_restore: 74.3301682472229\n",
      "  time_this_iter_s: 6.096906661987305\n",
      "  time_total_s: 74.3301682472229\n",
      "  timers:\n",
      "    learn_throughput: 1102.76\n",
      "    learn_time_ms: 3808.627\n",
      "    load_throughput: 1231988.251\n",
      "    load_time_ms: 3.409\n",
      "    sample_throughput: 1868.67\n",
      "    sample_time_ms: 2247.588\n",
      "    update_time_ms: 2.93\n",
      "  timestamp: 1607961049\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50400\n",
      "  training_iteration: 12\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         74.3302</td><td style=\"text-align: right;\">50400</td><td style=\"text-align: right;\">-20.7639</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-50-55\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 212.0\n",
      "  episode_reward_mean: -14.583333333333334\n",
      "  episode_reward_min: -266.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 1881\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5483161211013794\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019248196855187416\n",
      "        model: {}\n",
      "        policy_loss: -0.11214014887809753\n",
      "        total_loss: 691.7591552734375\n",
      "        vf_explained_var: 0.40511101484298706\n",
      "        vf_loss: 691.8275146484375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.109347939491272\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016036245971918106\n",
      "        model: {}\n",
      "        policy_loss: -0.13429462909698486\n",
      "        total_loss: 663.9205322265625\n",
      "        vf_explained_var: 0.43593430519104004\n",
      "        vf_loss: 664.0\n",
      "    num_steps_sampled: 54600\n",
      "    num_steps_trained: 54600\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.10000000000001\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 106.0\n",
      "    agent-1: 106.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: -7.291666666666667\n",
      "    agent-1: -7.291666666666667\n",
      "  policy_reward_min:\n",
      "    agent-0: -133.0\n",
      "    agent-1: -133.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13635983482908423\n",
      "    mean_inference_ms: 1.1397459510350656\n",
      "    mean_processing_ms: 0.28410954822357026\n",
      "  time_since_restore: 80.43866610527039\n",
      "  time_this_iter_s: 6.108497858047485\n",
      "  time_total_s: 80.43866610527039\n",
      "  timers:\n",
      "    learn_throughput: 1102.417\n",
      "    learn_time_ms: 3809.811\n",
      "    load_throughput: 1231264.935\n",
      "    load_time_ms: 3.411\n",
      "    sample_throughput: 1874.431\n",
      "    sample_time_ms: 2240.68\n",
      "    update_time_ms: 2.936\n",
      "  timestamp: 1607961055\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 54600\n",
      "  training_iteration: 13\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         80.4387</td><td style=\"text-align: right;\">54600</td><td style=\"text-align: right;\">-14.5833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-51-01\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 238.0\n",
      "  episode_reward_mean: -12.11111111111111\n",
      "  episode_reward_min: -246.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 2025\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.543534517288208\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018359705805778503\n",
      "        model: {}\n",
      "        policy_loss: -0.10861790180206299\n",
      "        total_loss: 639.6550903320312\n",
      "        vf_explained_var: 0.38510453701019287\n",
      "        vf_loss: 639.7218627929688\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1118366718292236\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016947362571954727\n",
      "        model: {}\n",
      "        policy_loss: -0.1353953629732132\n",
      "        total_loss: 611.5136108398438\n",
      "        vf_explained_var: 0.4193170964717865\n",
      "        vf_loss: 611.591064453125\n",
      "    num_steps_sampled: 58800\n",
      "    num_steps_trained: 58800\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.17777777777778\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 119.0\n",
      "    agent-1: 119.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: -6.055555555555555\n",
      "    agent-1: -6.055555555555555\n",
      "  policy_reward_min:\n",
      "    agent-0: -123.0\n",
      "    agent-1: -123.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13652458127342923\n",
      "    mean_inference_ms: 1.1392290704950108\n",
      "    mean_processing_ms: 0.2840898123034477\n",
      "  time_since_restore: 86.50343155860901\n",
      "  time_this_iter_s: 6.064765453338623\n",
      "  time_total_s: 86.50343155860901\n",
      "  timers:\n",
      "    learn_throughput: 1102.4\n",
      "    learn_time_ms: 3809.871\n",
      "    load_throughput: 1226754.838\n",
      "    load_time_ms: 3.424\n",
      "    sample_throughput: 1875.543\n",
      "    sample_time_ms: 2239.352\n",
      "    update_time_ms: 2.935\n",
      "  timestamp: 1607961061\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 58800\n",
      "  training_iteration: 14\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         86.5034</td><td style=\"text-align: right;\">58800</td><td style=\"text-align: right;\">-12.1111</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-51-07\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 202.0\n",
      "  episode_reward_mean: -14.802721088435375\n",
      "  episode_reward_min: -272.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 2172\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5458962917327881\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017683595418930054\n",
      "        model: {}\n",
      "        policy_loss: -0.11023260653018951\n",
      "        total_loss: 739.0779418945312\n",
      "        vf_explained_var: 0.4284663498401642\n",
      "        vf_loss: 739.1478271484375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0986173152923584\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01630229316651821\n",
      "        model: {}\n",
      "        policy_loss: -0.1325262188911438\n",
      "        total_loss: 735.2611083984375\n",
      "        vf_explained_var: 0.43820860981941223\n",
      "        vf_loss: 735.3379516601562\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 63000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.63333333333334\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 101.0\n",
      "    agent-1: 101.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: -7.401360544217687\n",
      "    agent-1: -7.401360544217687\n",
      "  policy_reward_min:\n",
      "    agent-0: -136.0\n",
      "    agent-1: -136.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13643925605065393\n",
      "    mean_inference_ms: 1.1370927987075157\n",
      "    mean_processing_ms: 0.28389755787172877\n",
      "  time_since_restore: 92.52481579780579\n",
      "  time_this_iter_s: 6.021384239196777\n",
      "  time_total_s: 92.52481579780579\n",
      "  timers:\n",
      "    learn_throughput: 1101.389\n",
      "    learn_time_ms: 3813.367\n",
      "    load_throughput: 1226276.621\n",
      "    load_time_ms: 3.425\n",
      "    sample_throughput: 1879.32\n",
      "    sample_time_ms: 2234.85\n",
      "    update_time_ms: 2.909\n",
      "  timestamp: 1607961067\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 15\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         92.5248</td><td style=\"text-align: right;\">63000</td><td style=\"text-align: right;\">-14.8027</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-51-13\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 5.402777777777778\n",
      "  episode_reward_min: -240.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 2316\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5518624782562256\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01931685023009777\n",
      "        model: {}\n",
      "        policy_loss: -0.10536570847034454\n",
      "        total_loss: 573.2001342773438\n",
      "        vf_explained_var: 0.42706966400146484\n",
      "        vf_loss: 573.261474609375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0961495637893677\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016467243432998657\n",
      "        model: {}\n",
      "        policy_loss: -0.1296353042125702\n",
      "        total_loss: 575.7098388671875\n",
      "        vf_explained_var: 0.4498472511768341\n",
      "        vf_loss: 575.783203125\n",
      "    num_steps_sampled: 67200\n",
      "    num_steps_trained: 67200\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.125\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 100.0\n",
      "    agent-1: 100.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 2.701388888888889\n",
      "    agent-1: 2.701388888888889\n",
      "  policy_reward_min:\n",
      "    agent-0: -120.0\n",
      "    agent-1: -120.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13635014607122542\n",
      "    mean_inference_ms: 1.1369887823512392\n",
      "    mean_processing_ms: 0.2838203407243209\n",
      "  time_since_restore: 98.63577127456665\n",
      "  time_this_iter_s: 6.110955476760864\n",
      "  time_total_s: 98.63577127456665\n",
      "  timers:\n",
      "    learn_throughput: 1100.86\n",
      "    learn_time_ms: 3815.2\n",
      "    load_throughput: 1233031.666\n",
      "    load_time_ms: 3.406\n",
      "    sample_throughput: 1879.266\n",
      "    sample_time_ms: 2234.915\n",
      "    update_time_ms: 2.901\n",
      "  timestamp: 1607961073\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 67200\n",
      "  training_iteration: 16\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         98.6358</td><td style=\"text-align: right;\">67200</td><td style=\"text-align: right;\"> 5.40278</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-51-19\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 222.0\n",
      "  episode_reward_mean: 6.055555555555555\n",
      "  episode_reward_min: -290.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 2460\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5513695478439331\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01865234039723873\n",
      "        model: {}\n",
      "        policy_loss: -0.10654600709676743\n",
      "        total_loss: 579.2684936523438\n",
      "        vf_explained_var: 0.410443514585495\n",
      "        vf_loss: 579.3326416015625\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0914280414581299\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015937354415655136\n",
      "        model: {}\n",
      "        policy_loss: -0.13052476942539215\n",
      "        total_loss: 572.663818359375\n",
      "        vf_explained_var: 0.4264775514602661\n",
      "        vf_loss: 572.7398681640625\n",
      "    num_steps_sampled: 71400\n",
      "    num_steps_trained: 71400\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.96666666666667\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 111.0\n",
      "    agent-1: 111.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 3.0277777777777777\n",
      "    agent-1: 3.0277777777777777\n",
      "  policy_reward_min:\n",
      "    agent-0: -145.0\n",
      "    agent-1: -145.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13632942841767332\n",
      "    mean_inference_ms: 1.1364018474825839\n",
      "    mean_processing_ms: 0.28364442953243274\n",
      "  time_since_restore: 104.64194822311401\n",
      "  time_this_iter_s: 6.006176948547363\n",
      "  time_total_s: 104.64194822311401\n",
      "  timers:\n",
      "    learn_throughput: 1100.391\n",
      "    learn_time_ms: 3816.826\n",
      "    load_throughput: 1220905.335\n",
      "    load_time_ms: 3.44\n",
      "    sample_throughput: 1882.011\n",
      "    sample_time_ms: 2231.655\n",
      "    update_time_ms: 2.941\n",
      "  timestamp: 1607961079\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71400\n",
      "  training_iteration: 17\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         104.642</td><td style=\"text-align: right;\">71400</td><td style=\"text-align: right;\"> 6.05556</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-51-25\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 210.0\n",
      "  episode_reward_mean: 10.791666666666666\n",
      "  episode_reward_min: -290.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 2604\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5489010810852051\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01820126175880432\n",
      "        model: {}\n",
      "        policy_loss: -0.10862959921360016\n",
      "        total_loss: 595.3056640625\n",
      "        vf_explained_var: 0.4347565472126007\n",
      "        vf_loss: 595.372802734375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.090916395187378\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016073070466518402\n",
      "        model: {}\n",
      "        policy_loss: -0.12523841857910156\n",
      "        total_loss: 572.2447509765625\n",
      "        vf_explained_var: 0.457952618598938\n",
      "        vf_loss: 572.3150634765625\n",
      "    num_steps_sampled: 75600\n",
      "    num_steps_trained: 75600\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.5\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 105.0\n",
      "    agent-1: 105.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 5.395833333333333\n",
      "    agent-1: 5.395833333333333\n",
      "  policy_reward_min:\n",
      "    agent-0: -145.0\n",
      "    agent-1: -145.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1363658088888029\n",
      "    mean_inference_ms: 1.136780819523854\n",
      "    mean_processing_ms: 0.2839519203591407\n",
      "  time_since_restore: 110.89456248283386\n",
      "  time_this_iter_s: 6.252614259719849\n",
      "  time_total_s: 110.89456248283386\n",
      "  timers:\n",
      "    learn_throughput: 1095.938\n",
      "    learn_time_ms: 3832.333\n",
      "    load_throughput: 1201273.606\n",
      "    load_time_ms: 3.496\n",
      "    sample_throughput: 1879.514\n",
      "    sample_time_ms: 2234.62\n",
      "    update_time_ms: 2.982\n",
      "  timestamp: 1607961085\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75600\n",
      "  training_iteration: 18\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         110.895</td><td style=\"text-align: right;\">75600</td><td style=\"text-align: right;\"> 10.7917</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-51-32\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 292.0\n",
      "  episode_reward_mean: 16.612244897959183\n",
      "  episode_reward_min: -280.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 2751\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5503052473068237\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01765695959329605\n",
      "        model: {}\n",
      "        policy_loss: -0.1074676588177681\n",
      "        total_loss: 589.908447265625\n",
      "        vf_explained_var: 0.42243269085884094\n",
      "        vf_loss: 589.9757690429688\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.08402419090271\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015783127397298813\n",
      "        model: {}\n",
      "        policy_loss: -0.12795330584049225\n",
      "        total_loss: 589.4122924804688\n",
      "        vf_explained_var: 0.44110140204429626\n",
      "        vf_loss: 589.486328125\n",
      "    num_steps_sampled: 79800\n",
      "    num_steps_trained: 79800\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.38888888888889\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 146.0\n",
      "    agent-1: 146.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 8.306122448979592\n",
      "    agent-1: 8.306122448979592\n",
      "  policy_reward_min:\n",
      "    agent-0: -140.0\n",
      "    agent-1: -140.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13647960448069987\n",
      "    mean_inference_ms: 1.136713965579089\n",
      "    mean_processing_ms: 0.2840949098530558\n",
      "  time_since_restore: 117.06065249443054\n",
      "  time_this_iter_s: 6.16609001159668\n",
      "  time_total_s: 117.06065249443054\n",
      "  timers:\n",
      "    learn_throughput: 1094.55\n",
      "    learn_time_ms: 3837.195\n",
      "    load_throughput: 1268676.221\n",
      "    load_time_ms: 3.311\n",
      "    sample_throughput: 1876.503\n",
      "    sample_time_ms: 2238.205\n",
      "    update_time_ms: 2.995\n",
      "  timestamp: 1607961092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79800\n",
      "  training_iteration: 19\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         117.061</td><td style=\"text-align: right;\">79800</td><td style=\"text-align: right;\"> 16.6122</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-51-38\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 208.0\n",
      "  episode_reward_mean: 6.652777777777778\n",
      "  episode_reward_min: -426.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 2895\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5516566038131714\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017792459577322006\n",
      "        model: {}\n",
      "        policy_loss: -0.10348436236381531\n",
      "        total_loss: 634.744873046875\n",
      "        vf_explained_var: 0.41599005460739136\n",
      "        vf_loss: 634.8077392578125\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0867063999176025\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015649747103452682\n",
      "        model: {}\n",
      "        policy_loss: -0.13161641359329224\n",
      "        total_loss: 612.565185546875\n",
      "        vf_explained_var: 0.442565381526947\n",
      "        vf_loss: 612.643310546875\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.1888888888889\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 104.0\n",
      "    agent-1: 104.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 3.326388888888889\n",
      "    agent-1: 3.326388888888889\n",
      "  policy_reward_min:\n",
      "    agent-0: -213.0\n",
      "    agent-1: -213.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13640029852460364\n",
      "    mean_inference_ms: 1.135975017792602\n",
      "    mean_processing_ms: 0.28411461061198584\n",
      "  time_since_restore: 123.2106077671051\n",
      "  time_this_iter_s: 6.1499552726745605\n",
      "  time_total_s: 123.2106077671051\n",
      "  timers:\n",
      "    learn_throughput: 1092.854\n",
      "    learn_time_ms: 3843.149\n",
      "    load_throughput: 1272920.696\n",
      "    load_time_ms: 3.299\n",
      "    sample_throughput: 1873.144\n",
      "    sample_time_ms: 2242.22\n",
      "    update_time_ms: 3.038\n",
      "  timestamp: 1607961098\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 20\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         123.211</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\"> 6.65278</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-51-44\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 230.0\n",
      "  episode_reward_mean: 3.763888888888889\n",
      "  episode_reward_min: -466.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 3039\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5501554012298584\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01792009547352791\n",
      "        model: {}\n",
      "        policy_loss: -0.10396568477153778\n",
      "        total_loss: 730.5654296875\n",
      "        vf_explained_var: 0.4534037709236145\n",
      "        vf_loss: 730.6286010742188\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0786783695220947\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016147788614034653\n",
      "        model: {}\n",
      "        policy_loss: -0.12708114087581635\n",
      "        total_loss: 744.123779296875\n",
      "        vf_explained_var: 0.43909627199172974\n",
      "        vf_loss: 744.1956787109375\n",
      "    num_steps_sampled: 88200\n",
      "    num_steps_trained: 88200\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.68888888888888\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 115.0\n",
      "    agent-1: 115.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 1.8819444444444444\n",
      "    agent-1: 1.8819444444444444\n",
      "  policy_reward_min:\n",
      "    agent-0: -233.0\n",
      "    agent-1: -233.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13637066714589535\n",
      "    mean_inference_ms: 1.1353901332356433\n",
      "    mean_processing_ms: 0.2840921811048818\n",
      "  time_since_restore: 129.23949790000916\n",
      "  time_this_iter_s: 6.028890132904053\n",
      "  time_total_s: 129.23949790000916\n",
      "  timers:\n",
      "    learn_throughput: 1093.705\n",
      "    learn_time_ms: 3840.157\n",
      "    load_throughput: 1277730.964\n",
      "    load_time_ms: 3.287\n",
      "    sample_throughput: 1873.868\n",
      "    sample_time_ms: 2241.353\n",
      "    update_time_ms: 3.055\n",
      "  timestamp: 1607961104\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88200\n",
      "  training_iteration: 21\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         129.239</td><td style=\"text-align: right;\">88200</td><td style=\"text-align: right;\"> 3.76389</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-51-50\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 262.0\n",
      "  episode_reward_mean: 19.496598639455783\n",
      "  episode_reward_min: -312.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 3186\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5497788190841675\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017961086705327034\n",
      "        model: {}\n",
      "        policy_loss: -0.11031798273324966\n",
      "        total_loss: 542.4020385742188\n",
      "        vf_explained_var: 0.4509994685649872\n",
      "        vf_loss: 542.471435546875\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0650129318237305\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0158100426197052\n",
      "        model: {}\n",
      "        policy_loss: -0.13212761282920837\n",
      "        total_loss: 546.380859375\n",
      "        vf_explained_var: 0.446839302778244\n",
      "        vf_loss: 546.458984375\n",
      "    num_steps_sampled: 92400\n",
      "    num_steps_trained: 92400\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.6875\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 131.0\n",
      "    agent-1: 131.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 9.748299319727892\n",
      "    agent-1: 9.748299319727892\n",
      "  policy_reward_min:\n",
      "    agent-0: -156.0\n",
      "    agent-1: -156.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13634935428588876\n",
      "    mean_inference_ms: 1.134473054505501\n",
      "    mean_processing_ms: 0.2839494605315343\n",
      "  time_since_restore: 135.27335667610168\n",
      "  time_this_iter_s: 6.033858776092529\n",
      "  time_total_s: 135.27335667610168\n",
      "  timers:\n",
      "    learn_throughput: 1094.562\n",
      "    learn_time_ms: 3837.151\n",
      "    load_throughput: 1282708.472\n",
      "    load_time_ms: 3.274\n",
      "    sample_throughput: 1876.429\n",
      "    sample_time_ms: 2238.294\n",
      "    update_time_ms: 3.076\n",
      "  timestamp: 1607961110\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92400\n",
      "  training_iteration: 22\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         135.273</td><td style=\"text-align: right;\">92400</td><td style=\"text-align: right;\"> 19.4966</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-51-56\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 208.0\n",
      "  episode_reward_mean: 3.3055555555555554\n",
      "  episode_reward_min: -262.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 3330\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5471131801605225\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018365859985351562\n",
      "        model: {}\n",
      "        policy_loss: -0.10958503186702728\n",
      "        total_loss: 566.6507568359375\n",
      "        vf_explained_var: 0.44802695512771606\n",
      "        vf_loss: 566.7184448242188\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0701994895935059\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016564493998885155\n",
      "        model: {}\n",
      "        policy_loss: -0.13176485896110535\n",
      "        total_loss: 572.9146728515625\n",
      "        vf_explained_var: 0.45730167627334595\n",
      "        vf_loss: 572.9898681640625\n",
      "    num_steps_sampled: 96600\n",
      "    num_steps_trained: 96600\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.25555555555555\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 104.0\n",
      "    agent-1: 104.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 1.6527777777777777\n",
      "    agent-1: 1.6527777777777777\n",
      "  policy_reward_min:\n",
      "    agent-0: -131.0\n",
      "    agent-1: -131.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13627259700170935\n",
      "    mean_inference_ms: 1.1340038029301982\n",
      "    mean_processing_ms: 0.28382655166315285\n",
      "  time_since_restore: 141.22961831092834\n",
      "  time_this_iter_s: 5.95626163482666\n",
      "  time_total_s: 141.22961831092834\n",
      "  timers:\n",
      "    learn_throughput: 1098.308\n",
      "    learn_time_ms: 3824.064\n",
      "    load_throughput: 1274477.059\n",
      "    load_time_ms: 3.295\n",
      "    sample_throughput: 1878.167\n",
      "    sample_time_ms: 2236.223\n",
      "    update_time_ms: 3.053\n",
      "  timestamp: 1607961116\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96600\n",
      "  training_iteration: 23\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">          141.23</td><td style=\"text-align: right;\">96600</td><td style=\"text-align: right;\"> 3.30556</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-52-02\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 224.0\n",
      "  episode_reward_mean: 28.430555555555557\n",
      "  episode_reward_min: -176.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 3474\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5431183576583862\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018462032079696655\n",
      "        model: {}\n",
      "        policy_loss: -0.1069534420967102\n",
      "        total_loss: 443.0136413574219\n",
      "        vf_explained_var: 0.49509578943252563\n",
      "        vf_loss: 443.0785217285156\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0683529376983643\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016359636560082436\n",
      "        model: {}\n",
      "        policy_loss: -0.12790870666503906\n",
      "        total_loss: 434.7127380371094\n",
      "        vf_explained_var: 0.49340471625328064\n",
      "        vf_loss: 434.78472900390625\n",
      "    num_steps_sampled: 100800\n",
      "    num_steps_trained: 100800\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.35000000000001\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 112.0\n",
      "    agent-1: 112.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 14.215277777777779\n",
      "    agent-1: 14.215277777777779\n",
      "  policy_reward_min:\n",
      "    agent-0: -88.0\n",
      "    agent-1: -88.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1362176806796902\n",
      "    mean_inference_ms: 1.1334090488621125\n",
      "    mean_processing_ms: 0.2837093815676233\n",
      "  time_since_restore: 147.209566116333\n",
      "  time_this_iter_s: 5.979947805404663\n",
      "  time_total_s: 147.209566116333\n",
      "  timers:\n",
      "    learn_throughput: 1100.811\n",
      "    learn_time_ms: 3815.369\n",
      "    load_throughput: 1271322.253\n",
      "    load_time_ms: 3.304\n",
      "    sample_throughput: 1878.152\n",
      "    sample_time_ms: 2236.241\n",
      "    update_time_ms: 3.053\n",
      "  timestamp: 1607961122\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100800\n",
      "  training_iteration: 24\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">          147.21</td><td style=\"text-align: right;\">100800</td><td style=\"text-align: right;\"> 28.4306</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-52-08\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 204.0\n",
      "  episode_reward_mean: 31.52777777777778\n",
      "  episode_reward_min: -168.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 3618\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5486974716186523\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0188867449760437\n",
      "        model: {}\n",
      "        policy_loss: -0.10915270447731018\n",
      "        total_loss: 502.34979248046875\n",
      "        vf_explained_var: 0.45859479904174805\n",
      "        vf_loss: 502.4158935546875\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0619268417358398\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01590314693748951\n",
      "        model: {}\n",
      "        policy_loss: -0.13511721789836884\n",
      "        total_loss: 503.5380859375\n",
      "        vf_explained_var: 0.4639347195625305\n",
      "        vf_loss: 503.6188659667969\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.8777777777778\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 102.0\n",
      "    agent-1: 102.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 15.76388888888889\n",
      "    agent-1: 15.76388888888889\n",
      "  policy_reward_min:\n",
      "    agent-0: -84.0\n",
      "    agent-1: -84.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13624535432701135\n",
      "    mean_inference_ms: 1.1339581728074661\n",
      "    mean_processing_ms: 0.2838716236031417\n",
      "  time_since_restore: 153.3616499900818\n",
      "  time_this_iter_s: 6.152083873748779\n",
      "  time_total_s: 153.3616499900818\n",
      "  timers:\n",
      "    learn_throughput: 1099.392\n",
      "    learn_time_ms: 3820.292\n",
      "    load_throughput: 1282064.336\n",
      "    load_time_ms: 3.276\n",
      "    sample_throughput: 1871.388\n",
      "    sample_time_ms: 2244.324\n",
      "    update_time_ms: 3.091\n",
      "  timestamp: 1607961128\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 25\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         153.362</td><td style=\"text-align: right;\">105000</td><td style=\"text-align: right;\"> 31.5278</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-52-15\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 244.0\n",
      "  episode_reward_mean: 35.197278911564624\n",
      "  episode_reward_min: -278.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 3765\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5466225743293762\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017359022051095963\n",
      "        model: {}\n",
      "        policy_loss: -0.10499466955661774\n",
      "        total_loss: 530.5297241210938\n",
      "        vf_explained_var: 0.46756473183631897\n",
      "        vf_loss: 530.59521484375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.050590991973877\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014799922704696655\n",
      "        model: {}\n",
      "        policy_loss: -0.1319415122270584\n",
      "        total_loss: 511.66644287109375\n",
      "        vf_explained_var: 0.4848359227180481\n",
      "        vf_loss: 511.747802734375\n",
      "    num_steps_sampled: 109200\n",
      "    num_steps_trained: 109200\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.55555555555556\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 122.0\n",
      "    agent-1: 122.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 17.598639455782312\n",
      "    agent-1: 17.598639455782312\n",
      "  policy_reward_min:\n",
      "    agent-0: -139.0\n",
      "    agent-1: -139.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13615887838583562\n",
      "    mean_inference_ms: 1.132999723132687\n",
      "    mean_processing_ms: 0.28366097102445514\n",
      "  time_since_restore: 159.4160029888153\n",
      "  time_this_iter_s: 6.0543529987335205\n",
      "  time_total_s: 159.4160029888153\n",
      "  timers:\n",
      "    learn_throughput: 1099.493\n",
      "    learn_time_ms: 3819.942\n",
      "    load_throughput: 1274403.299\n",
      "    load_time_ms: 3.296\n",
      "    sample_throughput: 1875.772\n",
      "    sample_time_ms: 2239.078\n",
      "    update_time_ms: 3.07\n",
      "  timestamp: 1607961135\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 109200\n",
      "  training_iteration: 26\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         159.416</td><td style=\"text-align: right;\">109200</td><td style=\"text-align: right;\"> 35.1973</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-52-21\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 194.0\n",
      "  episode_reward_mean: 32.18055555555556\n",
      "  episode_reward_min: -176.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 3909\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5434567928314209\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01837778091430664\n",
      "        model: {}\n",
      "        policy_loss: -0.10704624652862549\n",
      "        total_loss: 400.0548400878906\n",
      "        vf_explained_var: 0.4780697226524353\n",
      "        vf_loss: 400.1200256347656\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0453554391860962\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015632687136530876\n",
      "        model: {}\n",
      "        policy_loss: -0.12889546155929565\n",
      "        total_loss: 399.82550048828125\n",
      "        vf_explained_var: 0.47989875078201294\n",
      "        vf_loss: 399.9009704589844\n",
      "    num_steps_sampled: 113400\n",
      "    num_steps_trained: 113400\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.44444444444444\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 97.0\n",
      "    agent-1: 97.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 16.09027777777778\n",
      "    agent-1: 16.09027777777778\n",
      "  policy_reward_min:\n",
      "    agent-0: -88.0\n",
      "    agent-1: -88.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13623304003686443\n",
      "    mean_inference_ms: 1.132710667937613\n",
      "    mean_processing_ms: 0.28349533031269836\n",
      "  time_since_restore: 165.45251560211182\n",
      "  time_this_iter_s: 6.036512613296509\n",
      "  time_total_s: 165.45251560211182\n",
      "  timers:\n",
      "    learn_throughput: 1098.804\n",
      "    learn_time_ms: 3822.338\n",
      "    load_throughput: 1291122.603\n",
      "    load_time_ms: 3.253\n",
      "    sample_throughput: 1874.967\n",
      "    sample_time_ms: 2240.039\n",
      "    update_time_ms: 3.039\n",
      "  timestamp: 1607961141\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 113400\n",
      "  training_iteration: 27\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         165.453</td><td style=\"text-align: right;\">113400</td><td style=\"text-align: right;\"> 32.1806</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-52-27\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 226.0\n",
      "  episode_reward_mean: 41.166666666666664\n",
      "  episode_reward_min: -368.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 4053\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5490097403526306\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017283357679843903\n",
      "        model: {}\n",
      "        policy_loss: -0.09647288173437119\n",
      "        total_loss: 513.406494140625\n",
      "        vf_explained_var: 0.4553876519203186\n",
      "        vf_loss: 513.463623046875\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0364078283309937\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014637904241681099\n",
      "        model: {}\n",
      "        policy_loss: -0.12087169289588928\n",
      "        total_loss: 502.9882507324219\n",
      "        vf_explained_var: 0.4748047888278961\n",
      "        vf_loss: 503.05908203125\n",
      "    num_steps_sampled: 117600\n",
      "    num_steps_trained: 117600\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.21249999999999\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 113.0\n",
      "    agent-1: 113.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 20.583333333333332\n",
      "    agent-1: 20.583333333333332\n",
      "  policy_reward_min:\n",
      "    agent-0: -184.0\n",
      "    agent-1: -184.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13626998648098507\n",
      "    mean_inference_ms: 1.1327565965673305\n",
      "    mean_processing_ms: 0.28349435008208856\n",
      "  time_since_restore: 171.52180814743042\n",
      "  time_this_iter_s: 6.0692925453186035\n",
      "  time_total_s: 171.52180814743042\n",
      "  timers:\n",
      "    learn_throughput: 1103.435\n",
      "    learn_time_ms: 3806.295\n",
      "    load_throughput: 1314966.245\n",
      "    load_time_ms: 3.194\n",
      "    sample_throughput: 1876.696\n",
      "    sample_time_ms: 2237.976\n",
      "    update_time_ms: 3.013\n",
      "  timestamp: 1607961147\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 117600\n",
      "  training_iteration: 28\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         171.522</td><td style=\"text-align: right;\">117600</td><td style=\"text-align: right;\"> 41.1667</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-52-33\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 224.0\n",
      "  episode_reward_mean: 29.959183673469386\n",
      "  episode_reward_min: -246.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 4200\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5537590384483337\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018123682588338852\n",
      "        model: {}\n",
      "        policy_loss: -0.10532642900943756\n",
      "        total_loss: 566.8357543945312\n",
      "        vf_explained_var: 0.44636765122413635\n",
      "        vf_loss: 566.8998413085938\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0350306034088135\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014918394386768341\n",
      "        model: {}\n",
      "        policy_loss: -0.1169285923242569\n",
      "        total_loss: 557.7938842773438\n",
      "        vf_explained_var: 0.46150898933410645\n",
      "        vf_loss: 557.8598022460938\n",
      "    num_steps_sampled: 121800\n",
      "    num_steps_trained: 121800\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.03333333333335\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 112.0\n",
      "    agent-1: 112.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 14.979591836734693\n",
      "    agent-1: 14.979591836734693\n",
      "  policy_reward_min:\n",
      "    agent-0: -123.0\n",
      "    agent-1: -123.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1362617564520609\n",
      "    mean_inference_ms: 1.1329292790067216\n",
      "    mean_processing_ms: 0.2835734348978768\n",
      "  time_since_restore: 177.5824315547943\n",
      "  time_this_iter_s: 6.060623407363892\n",
      "  time_total_s: 177.5824315547943\n",
      "  timers:\n",
      "    learn_throughput: 1106.666\n",
      "    learn_time_ms: 3795.183\n",
      "    load_throughput: 1316509.114\n",
      "    load_time_ms: 3.19\n",
      "    sample_throughput: 1876.11\n",
      "    sample_time_ms: 2238.675\n",
      "    update_time_ms: 2.971\n",
      "  timestamp: 1607961153\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 121800\n",
      "  training_iteration: 29\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         177.582</td><td style=\"text-align: right;\">121800</td><td style=\"text-align: right;\"> 29.9592</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-52-39\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 210.0\n",
      "  episode_reward_mean: 33.666666666666664\n",
      "  episode_reward_min: -210.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 4344\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5509975552558899\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01812410168349743\n",
      "        model: {}\n",
      "        policy_loss: -0.10904952883720398\n",
      "        total_loss: 477.3050842285156\n",
      "        vf_explained_var: 0.48293831944465637\n",
      "        vf_loss: 477.3728332519531\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0461530685424805\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015731194987893105\n",
      "        model: {}\n",
      "        policy_loss: -0.12275928258895874\n",
      "        total_loss: 485.927734375\n",
      "        vf_explained_var: 0.4869881272315979\n",
      "        vf_loss: 485.9967041015625\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.11111111111111\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 105.0\n",
      "    agent-1: 105.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 16.833333333333332\n",
      "    agent-1: 16.833333333333332\n",
      "  policy_reward_min:\n",
      "    agent-0: -105.0\n",
      "    agent-1: -105.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13629977088500078\n",
      "    mean_inference_ms: 1.1326327440887272\n",
      "    mean_processing_ms: 0.2835814058215385\n",
      "  time_since_restore: 183.63514828681946\n",
      "  time_this_iter_s: 6.0527167320251465\n",
      "  time_total_s: 183.63514828681946\n",
      "  timers:\n",
      "    learn_throughput: 1109.021\n",
      "    learn_time_ms: 3787.122\n",
      "    load_throughput: 1312311.568\n",
      "    load_time_ms: 3.2\n",
      "    sample_throughput: 1877.453\n",
      "    sample_time_ms: 2237.073\n",
      "    update_time_ms: 2.959\n",
      "  timestamp: 1607961159\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 30\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         183.635</td><td style=\"text-align: right;\">126000</td><td style=\"text-align: right;\"> 33.6667</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-52-45\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 222.0\n",
      "  episode_reward_mean: 28.458333333333332\n",
      "  episode_reward_min: -386.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 4488\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5533599257469177\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01756133884191513\n",
      "        model: {}\n",
      "        policy_loss: -0.10689489543437958\n",
      "        total_loss: 606.7750244140625\n",
      "        vf_explained_var: 0.4651739001274109\n",
      "        vf_loss: 606.8419189453125\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0456256866455078\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015067987143993378\n",
      "        model: {}\n",
      "        policy_loss: -0.11900708079338074\n",
      "        total_loss: 631.0926513671875\n",
      "        vf_explained_var: 0.43716347217559814\n",
      "        vf_loss: 631.16015625\n",
      "    num_steps_sampled: 130200\n",
      "    num_steps_trained: 130200\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.14444444444445\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 111.0\n",
      "    agent-1: 111.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 14.229166666666666\n",
      "    agent-1: 14.229166666666666\n",
      "  policy_reward_min:\n",
      "    agent-0: -193.0\n",
      "    agent-1: -193.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13631321644598626\n",
      "    mean_inference_ms: 1.1322587039766712\n",
      "    mean_processing_ms: 0.2835405920941365\n",
      "  time_since_restore: 189.66737604141235\n",
      "  time_this_iter_s: 6.0322277545928955\n",
      "  time_total_s: 189.66737604141235\n",
      "  timers:\n",
      "    learn_throughput: 1108.595\n",
      "    learn_time_ms: 3788.578\n",
      "    load_throughput: 1289988.049\n",
      "    load_time_ms: 3.256\n",
      "    sample_throughput: 1878.546\n",
      "    sample_time_ms: 2235.772\n",
      "    update_time_ms: 2.955\n",
      "  timestamp: 1607961165\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 130200\n",
      "  training_iteration: 31\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         189.667</td><td style=\"text-align: right;\">130200</td><td style=\"text-align: right;\"> 28.4583</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-52-51\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 230.0\n",
      "  episode_reward_mean: 39.083333333333336\n",
      "  episode_reward_min: -174.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 4632\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5533646941184998\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017053799703717232\n",
      "        model: {}\n",
      "        policy_loss: -0.10908438265323639\n",
      "        total_loss: 513.9638061523438\n",
      "        vf_explained_var: 0.4636134207248688\n",
      "        vf_loss: 514.0339965820312\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0402815341949463\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015485291369259357\n",
      "        model: {}\n",
      "        policy_loss: -0.1258828043937683\n",
      "        total_loss: 511.1697082519531\n",
      "        vf_explained_var: 0.4717601239681244\n",
      "        vf_loss: 511.2427062988281\n",
      "    num_steps_sampled: 134400\n",
      "    num_steps_trained: 134400\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.9625\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 115.0\n",
      "    agent-1: 115.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 19.541666666666668\n",
      "    agent-1: 19.541666666666668\n",
      "  policy_reward_min:\n",
      "    agent-0: -87.0\n",
      "    agent-1: -87.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13627084294240938\n",
      "    mean_inference_ms: 1.132008041374797\n",
      "    mean_processing_ms: 0.28352079732566554\n",
      "  time_since_restore: 195.63223123550415\n",
      "  time_this_iter_s: 5.964855194091797\n",
      "  time_total_s: 195.63223123550415\n",
      "  timers:\n",
      "    learn_throughput: 1110.614\n",
      "    learn_time_ms: 3781.691\n",
      "    load_throughput: 1291491.763\n",
      "    load_time_ms: 3.252\n",
      "    sample_throughput: 1878.506\n",
      "    sample_time_ms: 2235.819\n",
      "    update_time_ms: 2.976\n",
      "  timestamp: 1607961171\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 134400\n",
      "  training_iteration: 32\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         195.632</td><td style=\"text-align: right;\">134400</td><td style=\"text-align: right;\"> 39.0833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-52-57\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 218.0\n",
      "  episode_reward_mean: 39.63265306122449\n",
      "  episode_reward_min: -304.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 4779\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5481579899787903\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01657411828637123\n",
      "        model: {}\n",
      "        policy_loss: -0.10735737532377243\n",
      "        total_loss: 497.5323181152344\n",
      "        vf_explained_var: 0.4587920904159546\n",
      "        vf_loss: 497.6019287109375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.022849202156067\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014823218807578087\n",
      "        model: {}\n",
      "        policy_loss: -0.12517984211444855\n",
      "        total_loss: 496.9834289550781\n",
      "        vf_explained_var: 0.44470101594924927\n",
      "        vf_loss: 497.0579833984375\n",
      "    num_steps_sampled: 138600\n",
      "    num_steps_trained: 138600\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.76666666666667\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 109.0\n",
      "    agent-1: 109.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 19.816326530612244\n",
      "    agent-1: 19.816326530612244\n",
      "  policy_reward_min:\n",
      "    agent-0: -152.0\n",
      "    agent-1: -152.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1362355718017291\n",
      "    mean_inference_ms: 1.1314984978947853\n",
      "    mean_processing_ms: 0.28351478342443914\n",
      "  time_since_restore: 201.66323518753052\n",
      "  time_this_iter_s: 6.031003952026367\n",
      "  time_total_s: 201.66323518753052\n",
      "  timers:\n",
      "    learn_throughput: 1108.566\n",
      "    learn_time_ms: 3788.679\n",
      "    load_throughput: 1307713.427\n",
      "    load_time_ms: 3.212\n",
      "    sample_throughput: 1878.036\n",
      "    sample_time_ms: 2236.379\n",
      "    update_time_ms: 2.998\n",
      "  timestamp: 1607961177\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 138600\n",
      "  training_iteration: 33\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         201.663</td><td style=\"text-align: right;\">138600</td><td style=\"text-align: right;\"> 39.6327</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-53-03\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 254.0\n",
      "  episode_reward_mean: 49.583333333333336\n",
      "  episode_reward_min: -160.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 4923\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5477275848388672\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01760678179562092\n",
      "        model: {}\n",
      "        policy_loss: -0.10612031817436218\n",
      "        total_loss: 472.37884521484375\n",
      "        vf_explained_var: 0.4952234625816345\n",
      "        vf_loss: 472.44488525390625\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0321012735366821\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015324808657169342\n",
      "        model: {}\n",
      "        policy_loss: -0.12486471980810165\n",
      "        total_loss: 481.60589599609375\n",
      "        vf_explained_var: 0.4876297116279602\n",
      "        vf_loss: 481.67840576171875\n",
      "    num_steps_sampled: 142800\n",
      "    num_steps_trained: 142800\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.84444444444446\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 127.0\n",
      "    agent-1: 127.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 24.791666666666668\n",
      "    agent-1: 24.791666666666668\n",
      "  policy_reward_min:\n",
      "    agent-0: -80.0\n",
      "    agent-1: -80.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1362936336940762\n",
      "    mean_inference_ms: 1.1316327066402074\n",
      "    mean_processing_ms: 0.28349188670814346\n",
      "  time_since_restore: 207.72779989242554\n",
      "  time_this_iter_s: 6.0645647048950195\n",
      "  time_total_s: 207.72779989242554\n",
      "  timers:\n",
      "    learn_throughput: 1106.208\n",
      "    learn_time_ms: 3796.754\n",
      "    load_throughput: 1318854.901\n",
      "    load_time_ms: 3.185\n",
      "    sample_throughput: 1877.424\n",
      "    sample_time_ms: 2237.107\n",
      "    update_time_ms: 2.987\n",
      "  timestamp: 1607961183\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 142800\n",
      "  training_iteration: 34\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         207.728</td><td style=\"text-align: right;\">142800</td><td style=\"text-align: right;\"> 49.5833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-53-09\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 278.0\n",
      "  episode_reward_mean: 52.40277777777778\n",
      "  episode_reward_min: -232.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 5067\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5434399247169495\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0179365836083889\n",
      "        model: {}\n",
      "        policy_loss: -0.09981068968772888\n",
      "        total_loss: 529.087646484375\n",
      "        vf_explained_var: 0.47719594836235046\n",
      "        vf_loss: 529.1466064453125\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.027498722076416\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014938805252313614\n",
      "        model: {}\n",
      "        policy_loss: -0.12451788783073425\n",
      "        total_loss: 539.7344970703125\n",
      "        vf_explained_var: 0.46394017338752747\n",
      "        vf_loss: 539.8079223632812\n",
      "    num_steps_sampled: 147000\n",
      "    num_steps_trained: 147000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.32499999999999\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 139.0\n",
      "    agent-1: 139.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 26.20138888888889\n",
      "    agent-1: 26.20138888888889\n",
      "  policy_reward_min:\n",
      "    agent-0: -116.0\n",
      "    agent-1: -116.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13635313637481106\n",
      "    mean_inference_ms: 1.1322473247546734\n",
      "    mean_processing_ms: 0.28359365717240426\n",
      "  time_since_restore: 213.87712335586548\n",
      "  time_this_iter_s: 6.149323463439941\n",
      "  time_total_s: 213.87712335586548\n",
      "  timers:\n",
      "    learn_throughput: 1106.643\n",
      "    learn_time_ms: 3795.262\n",
      "    load_throughput: 1304392.145\n",
      "    load_time_ms: 3.22\n",
      "    sample_throughput: 1876.772\n",
      "    sample_time_ms: 2237.885\n",
      "    update_time_ms: 2.946\n",
      "  timestamp: 1607961189\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 147000\n",
      "  training_iteration: 35\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         213.877</td><td style=\"text-align: right;\">147000</td><td style=\"text-align: right;\"> 52.4028</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-53-16\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 264.0\n",
      "  episode_reward_mean: 66.58333333333333\n",
      "  episode_reward_min: -106.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 5211\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5408719778060913\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01687869429588318\n",
      "        model: {}\n",
      "        policy_loss: -0.10511330515146255\n",
      "        total_loss: 396.4179382324219\n",
      "        vf_explained_var: 0.4974491000175476\n",
      "        vf_loss: 396.48455810546875\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0242104530334473\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015871137380599976\n",
      "        model: {}\n",
      "        policy_loss: -0.12123896181583405\n",
      "        total_loss: 367.1363525390625\n",
      "        vf_explained_var: 0.4912417232990265\n",
      "        vf_loss: 367.203369140625\n",
      "    num_steps_sampled: 151200\n",
      "    num_steps_trained: 151200\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.33333333333333\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 132.0\n",
      "    agent-1: 132.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 33.291666666666664\n",
      "    agent-1: 33.291666666666664\n",
      "  policy_reward_min:\n",
      "    agent-0: -53.0\n",
      "    agent-1: -53.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13634448705734664\n",
      "    mean_inference_ms: 1.1322475284322033\n",
      "    mean_processing_ms: 0.28364686407183576\n",
      "  time_since_restore: 220.01929783821106\n",
      "  time_this_iter_s: 6.142174482345581\n",
      "  time_total_s: 220.01929783821106\n",
      "  timers:\n",
      "    learn_throughput: 1105.61\n",
      "    learn_time_ms: 3798.809\n",
      "    load_throughput: 1318321.931\n",
      "    load_time_ms: 3.186\n",
      "    sample_throughput: 1872.39\n",
      "    sample_time_ms: 2243.123\n",
      "    update_time_ms: 2.964\n",
      "  timestamp: 1607961196\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 151200\n",
      "  training_iteration: 36\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         220.019</td><td style=\"text-align: right;\">151200</td><td style=\"text-align: right;\"> 66.5833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-53-22\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 218.0\n",
      "  episode_reward_mean: 43.904761904761905\n",
      "  episode_reward_min: -222.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 5358\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5501120090484619\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017359625548124313\n",
      "        model: {}\n",
      "        policy_loss: -0.10524541139602661\n",
      "        total_loss: 522.3272705078125\n",
      "        vf_explained_var: 0.4705876410007477\n",
      "        vf_loss: 522.3929443359375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0220463275909424\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015073727816343307\n",
      "        model: {}\n",
      "        policy_loss: -0.12388245016336441\n",
      "        total_loss: 524.2359619140625\n",
      "        vf_explained_var: 0.4565795063972473\n",
      "        vf_loss: 524.3082885742188\n",
      "    num_steps_sampled: 155400\n",
      "    num_steps_trained: 155400\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.04444444444444\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 109.0\n",
      "    agent-1: 109.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 21.952380952380953\n",
      "    agent-1: 21.952380952380953\n",
      "  policy_reward_min:\n",
      "    agent-0: -111.0\n",
      "    agent-1: -111.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13631354854007835\n",
      "    mean_inference_ms: 1.1319459218378645\n",
      "    mean_processing_ms: 0.2836400124444982\n",
      "  time_since_restore: 226.0355007648468\n",
      "  time_this_iter_s: 6.016202926635742\n",
      "  time_total_s: 226.0355007648468\n",
      "  timers:\n",
      "    learn_throughput: 1105.967\n",
      "    learn_time_ms: 3797.58\n",
      "    load_throughput: 1290280.951\n",
      "    load_time_ms: 3.255\n",
      "    sample_throughput: 1873.059\n",
      "    sample_time_ms: 2242.321\n",
      "    update_time_ms: 2.934\n",
      "  timestamp: 1607961202\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 155400\n",
      "  training_iteration: 37\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         226.036</td><td style=\"text-align: right;\">155400</td><td style=\"text-align: right;\"> 43.9048</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-53-28\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 224.0\n",
      "  episode_reward_mean: 47.27777777777778\n",
      "  episode_reward_min: -218.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 5502\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5482217669487\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01764588989317417\n",
      "        model: {}\n",
      "        policy_loss: -0.11253185570240021\n",
      "        total_loss: 527.775146484375\n",
      "        vf_explained_var: 0.4690457880496979\n",
      "        vf_loss: 527.8474731445312\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0122240781784058\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014815962873399258\n",
      "        model: {}\n",
      "        policy_loss: -0.1343284547328949\n",
      "        total_loss: 528.6136474609375\n",
      "        vf_explained_var: 0.46124833822250366\n",
      "        vf_loss: 528.6973266601562\n",
      "    num_steps_sampled: 159600\n",
      "    num_steps_trained: 159600\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.23333333333332\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 112.0\n",
      "    agent-1: 112.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 23.63888888888889\n",
      "    agent-1: 23.63888888888889\n",
      "  policy_reward_min:\n",
      "    agent-0: -109.0\n",
      "    agent-1: -109.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13630742246300684\n",
      "    mean_inference_ms: 1.1320056235175617\n",
      "    mean_processing_ms: 0.2836847983792107\n",
      "  time_since_restore: 232.17237162590027\n",
      "  time_this_iter_s: 6.136870861053467\n",
      "  time_total_s: 232.17237162590027\n",
      "  timers:\n",
      "    learn_throughput: 1104.678\n",
      "    learn_time_ms: 3802.011\n",
      "    load_throughput: 1291842.187\n",
      "    load_time_ms: 3.251\n",
      "    sample_throughput: 1871.164\n",
      "    sample_time_ms: 2244.592\n",
      "    update_time_ms: 3.031\n",
      "  timestamp: 1607961208\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159600\n",
      "  training_iteration: 38\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         232.172</td><td style=\"text-align: right;\">159600</td><td style=\"text-align: right;\"> 47.2778</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-53-34\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 238.0\n",
      "  episode_reward_mean: 47.833333333333336\n",
      "  episode_reward_min: -162.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 5646\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5436066389083862\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01719227246940136\n",
      "        model: {}\n",
      "        policy_loss: -0.10757365822792053\n",
      "        total_loss: 479.8758544921875\n",
      "        vf_explained_var: 0.45419836044311523\n",
      "        vf_loss: 479.9442443847656\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0077205896377563\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014588305726647377\n",
      "        model: {}\n",
      "        policy_loss: -0.12258675694465637\n",
      "        total_loss: 476.8631591796875\n",
      "        vf_explained_var: 0.4432576894760132\n",
      "        vf_loss: 476.9358825683594\n",
      "    num_steps_sampled: 163800\n",
      "    num_steps_trained: 163800\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.27777777777777\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 119.0\n",
      "    agent-1: 119.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 23.916666666666668\n",
      "    agent-1: 23.916666666666668\n",
      "  policy_reward_min:\n",
      "    agent-0: -81.0\n",
      "    agent-1: -81.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1362604280487882\n",
      "    mean_inference_ms: 1.1322864865190982\n",
      "    mean_processing_ms: 0.28363645881290517\n",
      "  time_since_restore: 238.2649359703064\n",
      "  time_this_iter_s: 6.092564344406128\n",
      "  time_total_s: 238.2649359703064\n",
      "  timers:\n",
      "    learn_throughput: 1103.177\n",
      "    learn_time_ms: 3807.186\n",
      "    load_throughput: 1290895.533\n",
      "    load_time_ms: 3.254\n",
      "    sample_throughput: 1872.889\n",
      "    sample_time_ms: 2242.524\n",
      "    update_time_ms: 3.038\n",
      "  timestamp: 1607961214\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 163800\n",
      "  training_iteration: 39\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         238.265</td><td style=\"text-align: right;\">163800</td><td style=\"text-align: right;\"> 47.8333</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-53-40\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 256.0\n",
      "  episode_reward_mean: 59.85034013605442\n",
      "  episode_reward_min: -290.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 5793\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5469669103622437\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01760011352598667\n",
      "        model: {}\n",
      "        policy_loss: -0.1039639562368393\n",
      "        total_loss: 536.951171875\n",
      "        vf_explained_var: 0.45841842889785767\n",
      "        vf_loss: 537.0150756835938\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0012444257736206\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014283034950494766\n",
      "        model: {}\n",
      "        policy_loss: -0.11712570488452911\n",
      "        total_loss: 508.50372314453125\n",
      "        vf_explained_var: 0.481906920671463\n",
      "        vf_loss: 508.5720520019531\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.875\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 128.0\n",
      "    agent-1: 128.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 29.92517006802721\n",
      "    agent-1: 29.92517006802721\n",
      "  policy_reward_min:\n",
      "    agent-0: -145.0\n",
      "    agent-1: -145.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1362629112967115\n",
      "    mean_inference_ms: 1.1325341440702168\n",
      "    mean_processing_ms: 0.2837329915772385\n",
      "  time_since_restore: 244.4943962097168\n",
      "  time_this_iter_s: 6.2294602394104\n",
      "  time_total_s: 244.4943962097168\n",
      "  timers:\n",
      "    learn_throughput: 1099.286\n",
      "    learn_time_ms: 3820.663\n",
      "    load_throughput: 1293122.375\n",
      "    load_time_ms: 3.248\n",
      "    sample_throughput: 1870.17\n",
      "    sample_time_ms: 2245.786\n",
      "    update_time_ms: 3.066\n",
      "  timestamp: 1607961220\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 40\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         244.494</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\"> 59.8503</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-53-46\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 210.0\n",
      "  episode_reward_mean: 47.791666666666664\n",
      "  episode_reward_min: -180.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 5937\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5478082299232483\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017467636615037918\n",
      "        model: {}\n",
      "        policy_loss: -0.10692015290260315\n",
      "        total_loss: 379.24041748046875\n",
      "        vf_explained_var: 0.4850291311740875\n",
      "        vf_loss: 379.3075256347656\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.013060450553894\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01547916978597641\n",
      "        model: {}\n",
      "        policy_loss: -0.1210516095161438\n",
      "        total_loss: 373.86285400390625\n",
      "        vf_explained_var: 0.49107423424720764\n",
      "        vf_loss: 373.9310302734375\n",
      "    num_steps_sampled: 172200\n",
      "    num_steps_trained: 172200\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.63333333333334\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 105.0\n",
      "    agent-1: 105.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 23.895833333333332\n",
      "    agent-1: 23.895833333333332\n",
      "  policy_reward_min:\n",
      "    agent-0: -90.0\n",
      "    agent-1: -90.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1362821410256289\n",
      "    mean_inference_ms: 1.1334678598863381\n",
      "    mean_processing_ms: 0.28377496584862494\n",
      "  time_since_restore: 250.55078601837158\n",
      "  time_this_iter_s: 6.056389808654785\n",
      "  time_total_s: 250.55078601837158\n",
      "  timers:\n",
      "    learn_throughput: 1101.004\n",
      "    learn_time_ms: 3814.702\n",
      "    load_throughput: 1308558.542\n",
      "    load_time_ms: 3.21\n",
      "    sample_throughput: 1863.094\n",
      "    sample_time_ms: 2254.315\n",
      "    update_time_ms: 3.044\n",
      "  timestamp: 1607961226\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172200\n",
      "  training_iteration: 41\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         250.551</td><td style=\"text-align: right;\">172200</td><td style=\"text-align: right;\"> 47.7917</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-53-53\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 274.0\n",
      "  episode_reward_mean: 47.34722222222222\n",
      "  episode_reward_min: -240.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 6081\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5465425848960876\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018180478364229202\n",
      "        model: {}\n",
      "        policy_loss: -0.11074092239141464\n",
      "        total_loss: 504.50311279296875\n",
      "        vf_explained_var: 0.5144107341766357\n",
      "        vf_loss: 504.5724182128906\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0021851062774658\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01541958749294281\n",
      "        model: {}\n",
      "        policy_loss: -0.12142180651426315\n",
      "        total_loss: 516.7451782226562\n",
      "        vf_explained_var: 0.4947672188282013\n",
      "        vf_loss: 516.81396484375\n",
      "    num_steps_sampled: 176400\n",
      "    num_steps_trained: 176400\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.37777777777778\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 137.0\n",
      "    agent-1: 137.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 23.67361111111111\n",
      "    agent-1: 23.67361111111111\n",
      "  policy_reward_min:\n",
      "    agent-0: -120.0\n",
      "    agent-1: -120.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13627769756442024\n",
      "    mean_inference_ms: 1.1338257183709146\n",
      "    mean_processing_ms: 0.2838410602039186\n",
      "  time_since_restore: 256.6823933124542\n",
      "  time_this_iter_s: 6.131607294082642\n",
      "  time_total_s: 256.6823933124542\n",
      "  timers:\n",
      "    learn_throughput: 1097.948\n",
      "    learn_time_ms: 3825.318\n",
      "    load_throughput: 1306656.144\n",
      "    load_time_ms: 3.214\n",
      "    sample_throughput: 1858.281\n",
      "    sample_time_ms: 2260.154\n",
      "    update_time_ms: 3.107\n",
      "  timestamp: 1607961233\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176400\n",
      "  training_iteration: 42\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         256.682</td><td style=\"text-align: right;\">176400</td><td style=\"text-align: right;\"> 47.3472</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-53-59\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 278.0\n",
      "  episode_reward_mean: 55.513888888888886\n",
      "  episode_reward_min: -192.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 6225\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5480654835700989\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017793498933315277\n",
      "        model: {}\n",
      "        policy_loss: -0.10714778304100037\n",
      "        total_loss: 528.0802001953125\n",
      "        vf_explained_var: 0.4673784375190735\n",
      "        vf_loss: 528.1468505859375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9888142347335815\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01570182293653488\n",
      "        model: {}\n",
      "        policy_loss: -0.11901502311229706\n",
      "        total_loss: 526.0448608398438\n",
      "        vf_explained_var: 0.4650966227054596\n",
      "        vf_loss: 526.1102294921875\n",
      "    num_steps_sampled: 180600\n",
      "    num_steps_trained: 180600\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.86666666666666\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 139.0\n",
      "    agent-1: 139.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 27.756944444444443\n",
      "    agent-1: 27.756944444444443\n",
      "  policy_reward_min:\n",
      "    agent-0: -96.0\n",
      "    agent-1: -96.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13625215131073462\n",
      "    mean_inference_ms: 1.1337445980838392\n",
      "    mean_processing_ms: 0.2838185917837881\n",
      "  time_since_restore: 262.8051698207855\n",
      "  time_this_iter_s: 6.122776508331299\n",
      "  time_total_s: 262.8051698207855\n",
      "  timers:\n",
      "    learn_throughput: 1095.734\n",
      "    learn_time_ms: 3833.046\n",
      "    load_throughput: 1299849.237\n",
      "    load_time_ms: 3.231\n",
      "    sample_throughput: 1857.123\n",
      "    sample_time_ms: 2261.563\n",
      "    update_time_ms: 3.113\n",
      "  timestamp: 1607961239\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180600\n",
      "  training_iteration: 43\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         262.805</td><td style=\"text-align: right;\">180600</td><td style=\"text-align: right;\"> 55.5139</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-54-05\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 270.0\n",
      "  episode_reward_mean: 58.42176870748299\n",
      "  episode_reward_min: -238.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 6372\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5393708348274231\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017119310796260834\n",
      "        model: {}\n",
      "        policy_loss: -0.11161643266677856\n",
      "        total_loss: 415.206787109375\n",
      "        vf_explained_var: 0.48902183771133423\n",
      "        vf_loss: 415.27935791015625\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9880492091178894\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01460011675953865\n",
      "        model: {}\n",
      "        policy_loss: -0.12155017256736755\n",
      "        total_loss: 407.8697204589844\n",
      "        vf_explained_var: 0.480862557888031\n",
      "        vf_loss: 407.94140625\n",
      "    num_steps_sampled: 184800\n",
      "    num_steps_trained: 184800\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.43333333333334\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 135.0\n",
      "    agent-1: 135.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 29.210884353741495\n",
      "    agent-1: 29.210884353741495\n",
      "  policy_reward_min:\n",
      "    agent-0: -119.0\n",
      "    agent-1: -119.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1362243486960317\n",
      "    mean_inference_ms: 1.1339604705897075\n",
      "    mean_processing_ms: 0.2839150850925982\n",
      "  time_since_restore: 269.0611472129822\n",
      "  time_this_iter_s: 6.255977392196655\n",
      "  time_total_s: 269.0611472129822\n",
      "  timers:\n",
      "    learn_throughput: 1090.904\n",
      "    learn_time_ms: 3850.018\n",
      "    load_throughput: 1275464.417\n",
      "    load_time_ms: 3.293\n",
      "    sample_throughput: 1855.56\n",
      "    sample_time_ms: 2263.467\n",
      "    update_time_ms: 3.133\n",
      "  timestamp: 1607961245\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184800\n",
      "  training_iteration: 44\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         269.061</td><td style=\"text-align: right;\">184800</td><td style=\"text-align: right;\"> 58.4218</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-54-11\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 226.0\n",
      "  episode_reward_mean: 57.0\n",
      "  episode_reward_min: -174.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 6516\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.533581554889679\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017656289041042328\n",
      "        model: {}\n",
      "        policy_loss: -0.10537972301244736\n",
      "        total_loss: 451.92608642578125\n",
      "        vf_explained_var: 0.5049909353256226\n",
      "        vf_loss: 451.9912109375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.979951798915863\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01514396071434021\n",
      "        model: {}\n",
      "        policy_loss: -0.1184341311454773\n",
      "        total_loss: 458.1278076171875\n",
      "        vf_explained_var: 0.4902554750442505\n",
      "        vf_loss: 458.1944580078125\n",
      "    num_steps_sampled: 189000\n",
      "    num_steps_trained: 189000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.75555555555555\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 113.0\n",
      "    agent-1: 113.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 28.5\n",
      "    agent-1: 28.5\n",
      "  policy_reward_min:\n",
      "    agent-0: -87.0\n",
      "    agent-1: -87.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1362408414637362\n",
      "    mean_inference_ms: 1.1346233662272631\n",
      "    mean_processing_ms: 0.28401329692164173\n",
      "  time_since_restore: 275.2147579193115\n",
      "  time_this_iter_s: 6.153610706329346\n",
      "  time_total_s: 275.2147579193115\n",
      "  timers:\n",
      "    learn_throughput: 1091.158\n",
      "    learn_time_ms: 3849.122\n",
      "    load_throughput: 1283764.761\n",
      "    load_time_ms: 3.272\n",
      "    sample_throughput: 1854.327\n",
      "    sample_time_ms: 2264.973\n",
      "    update_time_ms: 3.131\n",
      "  timestamp: 1607961251\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 189000\n",
      "  training_iteration: 45\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         275.215</td><td style=\"text-align: right;\">189000</td><td style=\"text-align: right;\">      57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-54-17\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 244.0\n",
      "  episode_reward_mean: 64.97222222222223\n",
      "  episode_reward_min: -260.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 6660\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5318715572357178\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01686568558216095\n",
      "        model: {}\n",
      "        policy_loss: -0.10382760316133499\n",
      "        total_loss: 386.20849609375\n",
      "        vf_explained_var: 0.49178433418273926\n",
      "        vf_loss: 386.2738952636719\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9755696058273315\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013898193836212158\n",
      "        model: {}\n",
      "        policy_loss: -0.11378055810928345\n",
      "        total_loss: 393.35888671875\n",
      "        vf_explained_var: 0.4794524312019348\n",
      "        vf_loss: 393.4251708984375\n",
      "    num_steps_sampled: 193200\n",
      "    num_steps_trained: 193200\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.975\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 122.0\n",
      "    agent-1: 122.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 32.486111111111114\n",
      "    agent-1: 32.486111111111114\n",
      "  policy_reward_min:\n",
      "    agent-0: -130.0\n",
      "    agent-1: -130.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13625450203580552\n",
      "    mean_inference_ms: 1.135026444882989\n",
      "    mean_processing_ms: 0.2840944430653145\n",
      "  time_since_restore: 281.25628781318665\n",
      "  time_this_iter_s: 6.041529893875122\n",
      "  time_total_s: 281.25628781318665\n",
      "  timers:\n",
      "    learn_throughput: 1095.003\n",
      "    learn_time_ms: 3835.606\n",
      "    load_throughput: 1286652.702\n",
      "    load_time_ms: 3.264\n",
      "    sample_throughput: 1851.443\n",
      "    sample_time_ms: 2268.501\n",
      "    update_time_ms: 3.204\n",
      "  timestamp: 1607961257\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 193200\n",
      "  training_iteration: 46\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         281.256</td><td style=\"text-align: right;\">193200</td><td style=\"text-align: right;\"> 64.9722</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-54-24\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 238.0\n",
      "  episode_reward_mean: 64.43055555555556\n",
      "  episode_reward_min: -134.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 6804\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5334798097610474\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017557958140969276\n",
      "        model: {}\n",
      "        policy_loss: -0.10567944496870041\n",
      "        total_loss: 446.2112121582031\n",
      "        vf_explained_var: 0.49743562936782837\n",
      "        vf_loss: 446.2768859863281\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9733554124832153\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01496884785592556\n",
      "        model: {}\n",
      "        policy_loss: -0.12102700769901276\n",
      "        total_loss: 446.9830322265625\n",
      "        vf_explained_var: 0.48096466064453125\n",
      "        vf_loss: 447.0529479980469\n",
      "    num_steps_sampled: 197400\n",
      "    num_steps_trained: 197400\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.48888888888888\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 119.0\n",
      "    agent-1: 119.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 32.21527777777778\n",
      "    agent-1: 32.21527777777778\n",
      "  policy_reward_min:\n",
      "    agent-0: -67.0\n",
      "    agent-1: -67.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13633469621281857\n",
      "    mean_inference_ms: 1.1351168360086232\n",
      "    mean_processing_ms: 0.2840961115744683\n",
      "  time_since_restore: 287.4324359893799\n",
      "  time_this_iter_s: 6.176148176193237\n",
      "  time_total_s: 287.4324359893799\n",
      "  timers:\n",
      "    learn_throughput: 1092.33\n",
      "    learn_time_ms: 3844.991\n",
      "    load_throughput: 1302241.863\n",
      "    load_time_ms: 3.225\n",
      "    sample_throughput: 1845.958\n",
      "    sample_time_ms: 2275.241\n",
      "    update_time_ms: 3.223\n",
      "  timestamp: 1607961264\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 197400\n",
      "  training_iteration: 47\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         287.432</td><td style=\"text-align: right;\">197400</td><td style=\"text-align: right;\"> 64.4306</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-54-30\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 270.0\n",
      "  episode_reward_mean: 69.31972789115646\n",
      "  episode_reward_min: -198.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 6951\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5336368083953857\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016772452741861343\n",
      "        model: {}\n",
      "        policy_loss: -0.10744958370923996\n",
      "        total_loss: 531.0220947265625\n",
      "        vf_explained_var: 0.4857245683670044\n",
      "        vf_loss: 531.0913696289062\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9658475518226624\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013218679465353489\n",
      "        model: {}\n",
      "        policy_loss: -0.11680525541305542\n",
      "        total_loss: 507.30914306640625\n",
      "        vf_explained_var: 0.49430376291275024\n",
      "        vf_loss: 507.38079833984375\n",
      "    num_steps_sampled: 201600\n",
      "    num_steps_trained: 201600\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.42222222222223\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 135.0\n",
      "    agent-1: 135.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 34.65986394557823\n",
      "    agent-1: 34.65986394557823\n",
      "  policy_reward_min:\n",
      "    agent-0: -99.0\n",
      "    agent-1: -99.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13631516713737457\n",
      "    mean_inference_ms: 1.134735412924247\n",
      "    mean_processing_ms: 0.2840690889335483\n",
      "  time_since_restore: 293.4957437515259\n",
      "  time_this_iter_s: 6.063307762145996\n",
      "  time_total_s: 293.4957437515259\n",
      "  timers:\n",
      "    learn_throughput: 1093.128\n",
      "    learn_time_ms: 3842.185\n",
      "    load_throughput: 1299475.285\n",
      "    load_time_ms: 3.232\n",
      "    sample_throughput: 1849.948\n",
      "    sample_time_ms: 2270.334\n",
      "    update_time_ms: 3.196\n",
      "  timestamp: 1607961270\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 201600\n",
      "  training_iteration: 48\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         293.496</td><td style=\"text-align: right;\">201600</td><td style=\"text-align: right;\"> 69.3197</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-54-36\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 260.0\n",
      "  episode_reward_mean: 71.18055555555556\n",
      "  episode_reward_min: -146.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 7095\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5380346775054932\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017636260017752647\n",
      "        model: {}\n",
      "        policy_loss: -0.10643287003040314\n",
      "        total_loss: 474.0546875\n",
      "        vf_explained_var: 0.48524531722068787\n",
      "        vf_loss: 474.1209716796875\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9563516974449158\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014705014415085316\n",
      "        model: {}\n",
      "        policy_loss: -0.11776935309171677\n",
      "        total_loss: 463.87591552734375\n",
      "        vf_explained_var: 0.4807238280773163\n",
      "        vf_loss: 463.9434509277344\n",
      "    num_steps_sampled: 205800\n",
      "    num_steps_trained: 205800\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.42222222222222\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 130.0\n",
      "    agent-1: 130.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 35.59027777777778\n",
      "    agent-1: 35.59027777777778\n",
      "  policy_reward_min:\n",
      "    agent-0: -73.0\n",
      "    agent-1: -73.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13627689392137596\n",
      "    mean_inference_ms: 1.1346093261099361\n",
      "    mean_processing_ms: 0.2840168057210094\n",
      "  time_since_restore: 299.5881495475769\n",
      "  time_this_iter_s: 6.092405796051025\n",
      "  time_total_s: 299.5881495475769\n",
      "  timers:\n",
      "    learn_throughput: 1092.932\n",
      "    learn_time_ms: 3842.874\n",
      "    load_throughput: 1309443.682\n",
      "    load_time_ms: 3.207\n",
      "    sample_throughput: 1850.45\n",
      "    sample_time_ms: 2269.718\n",
      "    update_time_ms: 3.204\n",
      "  timestamp: 1607961276\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 205800\n",
      "  training_iteration: 49\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         299.588</td><td style=\"text-align: right;\">205800</td><td style=\"text-align: right;\"> 71.1806</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-54-42\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 242.0\n",
      "  episode_reward_mean: 67.73611111111111\n",
      "  episode_reward_min: -160.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 7239\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5354124307632446\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018261956050992012\n",
      "        model: {}\n",
      "        policy_loss: -0.10284318029880524\n",
      "        total_loss: 398.4210205078125\n",
      "        vf_explained_var: 0.5219817757606506\n",
      "        vf_loss: 398.48223876953125\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9611895680427551\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014785385690629482\n",
      "        model: {}\n",
      "        policy_loss: -0.1125997006893158\n",
      "        total_loss: 405.8623352050781\n",
      "        vf_explained_var: 0.4969736635684967\n",
      "        vf_loss: 405.9244079589844\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.14444444444445\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 121.0\n",
      "    agent-1: 121.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 33.86805555555556\n",
      "    agent-1: 33.86805555555556\n",
      "  policy_reward_min:\n",
      "    agent-0: -80.0\n",
      "    agent-1: -80.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13626534074284777\n",
      "    mean_inference_ms: 1.1347325980772007\n",
      "    mean_processing_ms: 0.2839483194161418\n",
      "  time_since_restore: 305.74627900123596\n",
      "  time_this_iter_s: 6.158129453659058\n",
      "  time_total_s: 305.74627900123596\n",
      "  timers:\n",
      "    learn_throughput: 1094.306\n",
      "    learn_time_ms: 3838.049\n",
      "    load_throughput: 1234760.199\n",
      "    load_time_ms: 3.401\n",
      "    sample_throughput: 1851.91\n",
      "    sample_time_ms: 2267.929\n",
      "    update_time_ms: 3.201\n",
      "  timestamp: 1607961282\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 50\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         305.746</td><td style=\"text-align: right;\">210000</td><td style=\"text-align: right;\"> 67.7361</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-54-48\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 306.0\n",
      "  episode_reward_mean: 80.82993197278911\n",
      "  episode_reward_min: -206.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 7386\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5382843017578125\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017011595889925957\n",
      "        model: {}\n",
      "        policy_loss: -0.09729644656181335\n",
      "        total_loss: 449.02618408203125\n",
      "        vf_explained_var: 0.5165581703186035\n",
      "        vf_loss: 449.084716796875\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9514620304107666\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014079161919653416\n",
      "        model: {}\n",
      "        policy_loss: -0.10617129504680634\n",
      "        total_loss: 447.8053283691406\n",
      "        vf_explained_var: 0.512436032295227\n",
      "        vf_loss: 447.8634033203125\n",
      "    num_steps_sampled: 214200\n",
      "    num_steps_trained: 214200\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.8625\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 153.0\n",
      "    agent-1: 153.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 40.414965986394556\n",
      "    agent-1: 40.414965986394556\n",
      "  policy_reward_min:\n",
      "    agent-0: -103.0\n",
      "    agent-1: -103.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13625253431865023\n",
      "    mean_inference_ms: 1.134896345943463\n",
      "    mean_processing_ms: 0.28393829818241684\n",
      "  time_since_restore: 311.8473391532898\n",
      "  time_this_iter_s: 6.101060152053833\n",
      "  time_total_s: 311.8473391532898\n",
      "  timers:\n",
      "    learn_throughput: 1091.779\n",
      "    learn_time_ms: 3846.933\n",
      "    load_throughput: 1247058.764\n",
      "    load_time_ms: 3.368\n",
      "    sample_throughput: 1855.467\n",
      "    sample_time_ms: 2263.581\n",
      "    update_time_ms: 3.234\n",
      "  timestamp: 1607961288\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 214200\n",
      "  training_iteration: 51\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         311.847</td><td style=\"text-align: right;\">214200</td><td style=\"text-align: right;\"> 80.8299</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-54-55\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 254.0\n",
      "  episode_reward_mean: 80.04166666666667\n",
      "  episode_reward_min: -186.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 7530\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5392531752586365\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016546305269002914\n",
      "        model: {}\n",
      "        policy_loss: -0.1023813933134079\n",
      "        total_loss: 446.3408203125\n",
      "        vf_explained_var: 0.5139358639717102\n",
      "        vf_loss: 446.405517578125\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9459123611450195\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014323171228170395\n",
      "        model: {}\n",
      "        policy_loss: -0.10770444571971893\n",
      "        total_loss: 454.4772644042969\n",
      "        vf_explained_var: 0.5022991895675659\n",
      "        vf_loss: 454.5360107421875\n",
      "    num_steps_sampled: 218400\n",
      "    num_steps_trained: 218400\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.15555555555555\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 127.0\n",
      "    agent-1: 127.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 40.020833333333336\n",
      "    agent-1: 40.020833333333336\n",
      "  policy_reward_min:\n",
      "    agent-0: -93.0\n",
      "    agent-1: -93.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13642132437543605\n",
      "    mean_inference_ms: 1.1352828219654751\n",
      "    mean_processing_ms: 0.2839968170580876\n",
      "  time_since_restore: 317.9783728122711\n",
      "  time_this_iter_s: 6.131033658981323\n",
      "  time_total_s: 317.9783728122711\n",
      "  timers:\n",
      "    learn_throughput: 1092.577\n",
      "    learn_time_ms: 3844.124\n",
      "    load_throughput: 1248287.071\n",
      "    load_time_ms: 3.365\n",
      "    sample_throughput: 1852.994\n",
      "    sample_time_ms: 2266.602\n",
      "    update_time_ms: 3.152\n",
      "  timestamp: 1607961295\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 218400\n",
      "  training_iteration: 52\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         317.978</td><td style=\"text-align: right;\">218400</td><td style=\"text-align: right;\"> 80.0417</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-55-01\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 284.0\n",
      "  episode_reward_mean: 65.5\n",
      "  episode_reward_min: -288.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 7674\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5371577143669128\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017270933836698532\n",
      "        model: {}\n",
      "        policy_loss: -0.1064981073141098\n",
      "        total_loss: 520.266845703125\n",
      "        vf_explained_var: 0.4860098361968994\n",
      "        vf_loss: 520.333984375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9543925523757935\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014914433471858501\n",
      "        model: {}\n",
      "        policy_loss: -0.10600513964891434\n",
      "        total_loss: 522.5883178710938\n",
      "        vf_explained_var: 0.48015081882476807\n",
      "        vf_loss: 522.643310546875\n",
      "    num_steps_sampled: 222600\n",
      "    num_steps_trained: 222600\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.3\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 142.0\n",
      "    agent-1: 142.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 32.75\n",
      "    agent-1: 32.75\n",
      "  policy_reward_min:\n",
      "    agent-0: -144.0\n",
      "    agent-1: -144.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1364188689677613\n",
      "    mean_inference_ms: 1.1351472814574184\n",
      "    mean_processing_ms: 0.2839662431896761\n",
      "  time_since_restore: 324.0616412162781\n",
      "  time_this_iter_s: 6.083268404006958\n",
      "  time_total_s: 324.0616412162781\n",
      "  timers:\n",
      "    learn_throughput: 1093.601\n",
      "    learn_time_ms: 3840.525\n",
      "    load_throughput: 1257348.189\n",
      "    load_time_ms: 3.34\n",
      "    sample_throughput: 1853.331\n",
      "    sample_time_ms: 2266.19\n",
      "    update_time_ms: 3.18\n",
      "  timestamp: 1607961301\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 222600\n",
      "  training_iteration: 53\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         324.062</td><td style=\"text-align: right;\">222600</td><td style=\"text-align: right;\">    65.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-55-07\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 250.0\n",
      "  episode_reward_mean: 79.66666666666667\n",
      "  episode_reward_min: -148.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 7818\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5375635623931885\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01704932190477848\n",
      "        model: {}\n",
      "        policy_loss: -0.09506125748157501\n",
      "        total_loss: 438.5611572265625\n",
      "        vf_explained_var: 0.488092303276062\n",
      "        vf_loss: 438.6174011230469\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9420615434646606\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013281486928462982\n",
      "        model: {}\n",
      "        policy_loss: -0.10571961849927902\n",
      "        total_loss: 429.8478088378906\n",
      "        vf_explained_var: 0.4863768517971039\n",
      "        vf_loss: 429.908203125\n",
      "    num_steps_sampled: 226800\n",
      "    num_steps_trained: 226800\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.03333333333335\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 125.0\n",
      "    agent-1: 125.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 39.833333333333336\n",
      "    agent-1: 39.833333333333336\n",
      "  policy_reward_min:\n",
      "    agent-0: -74.0\n",
      "    agent-1: -74.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13639030403225375\n",
      "    mean_inference_ms: 1.1345705697969182\n",
      "    mean_processing_ms: 0.28390600003870836\n",
      "  time_since_restore: 330.15368127822876\n",
      "  time_this_iter_s: 6.092040061950684\n",
      "  time_total_s: 330.15368127822876\n",
      "  timers:\n",
      "    learn_throughput: 1096.273\n",
      "    learn_time_ms: 3831.163\n",
      "    load_throughput: 1263344.578\n",
      "    load_time_ms: 3.325\n",
      "    sample_throughput: 1859.04\n",
      "    sample_time_ms: 2259.23\n",
      "    update_time_ms: 3.215\n",
      "  timestamp: 1607961307\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 226800\n",
      "  training_iteration: 54\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         330.154</td><td style=\"text-align: right;\">226800</td><td style=\"text-align: right;\"> 79.6667</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-55-13\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 250.0\n",
      "  episode_reward_mean: 67.79591836734694\n",
      "  episode_reward_min: -146.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 7965\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5409143567085266\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017461270093917847\n",
      "        model: {}\n",
      "        policy_loss: -0.10840007662773132\n",
      "        total_loss: 439.9100341796875\n",
      "        vf_explained_var: 0.5109729766845703\n",
      "        vf_loss: 439.9786682128906\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9507889747619629\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014272612519562244\n",
      "        model: {}\n",
      "        policy_loss: -0.11310084909200668\n",
      "        total_loss: 431.3511657714844\n",
      "        vf_explained_var: 0.5199096202850342\n",
      "        vf_loss: 431.4154968261719\n",
      "    num_steps_sampled: 231000\n",
      "    num_steps_trained: 231000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.73333333333333\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 125.0\n",
      "    agent-1: 125.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 33.89795918367347\n",
      "    agent-1: 33.89795918367347\n",
      "  policy_reward_min:\n",
      "    agent-0: -73.0\n",
      "    agent-1: -73.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13639186862549182\n",
      "    mean_inference_ms: 1.1345561500027121\n",
      "    mean_processing_ms: 0.283943252818447\n",
      "  time_since_restore: 336.277774810791\n",
      "  time_this_iter_s: 6.124093532562256\n",
      "  time_total_s: 336.277774810791\n",
      "  timers:\n",
      "    learn_throughput: 1095.829\n",
      "    learn_time_ms: 3832.715\n",
      "    load_throughput: 1261815.27\n",
      "    load_time_ms: 3.329\n",
      "    sample_throughput: 1862.595\n",
      "    sample_time_ms: 2254.918\n",
      "    update_time_ms: 3.237\n",
      "  timestamp: 1607961313\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 231000\n",
      "  training_iteration: 55\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         336.278</td><td style=\"text-align: right;\">231000</td><td style=\"text-align: right;\"> 67.7959</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-55-19\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 256.0\n",
      "  episode_reward_mean: 78.27777777777777\n",
      "  episode_reward_min: -224.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 8109\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5399375557899475\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017155846580863\n",
      "        model: {}\n",
      "        policy_loss: -0.09881934523582458\n",
      "        total_loss: 499.9189147949219\n",
      "        vf_explained_var: 0.5245000123977661\n",
      "        vf_loss: 499.9786376953125\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.946096658706665\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01364956796169281\n",
      "        model: {}\n",
      "        policy_loss: -0.11115962266921997\n",
      "        total_loss: 514.572021484375\n",
      "        vf_explained_var: 0.507667064666748\n",
      "        vf_loss: 514.6365966796875\n",
      "    num_steps_sampled: 235200\n",
      "    num_steps_trained: 235200\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.51249999999999\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 128.0\n",
      "    agent-1: 128.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 39.138888888888886\n",
      "    agent-1: 39.138888888888886\n",
      "  policy_reward_min:\n",
      "    agent-0: -112.0\n",
      "    agent-1: -112.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13639706305070654\n",
      "    mean_inference_ms: 1.1344756652227659\n",
      "    mean_processing_ms: 0.2839815907379337\n",
      "  time_since_restore: 342.3654749393463\n",
      "  time_this_iter_s: 6.087700128555298\n",
      "  time_total_s: 342.3654749393463\n",
      "  timers:\n",
      "    learn_throughput: 1093.41\n",
      "    learn_time_ms: 3841.193\n",
      "    load_throughput: 1246141.322\n",
      "    load_time_ms: 3.37\n",
      "    sample_throughput: 1865.939\n",
      "    sample_time_ms: 2250.877\n",
      "    update_time_ms: 3.149\n",
      "  timestamp: 1607961319\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 235200\n",
      "  training_iteration: 56\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         342.365</td><td style=\"text-align: right;\">235200</td><td style=\"text-align: right;\"> 78.2778</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-55-25\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 244.0\n",
      "  episode_reward_mean: 81.0\n",
      "  episode_reward_min: -206.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 8253\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5419869422912598\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017395440489053726\n",
      "        model: {}\n",
      "        policy_loss: -0.09971235692501068\n",
      "        total_loss: 451.9736328125\n",
      "        vf_explained_var: 0.49989181756973267\n",
      "        vf_loss: 452.03369140625\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9301647543907166\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013383054174482822\n",
      "        model: {}\n",
      "        policy_loss: -0.11127957701683044\n",
      "        total_loss: 456.50836181640625\n",
      "        vf_explained_var: 0.4845955967903137\n",
      "        vf_loss: 456.5738830566406\n",
      "    num_steps_sampled: 239400\n",
      "    num_steps_trained: 239400\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.1\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 122.0\n",
      "    agent-1: 122.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 40.5\n",
      "    agent-1: 40.5\n",
      "  policy_reward_min:\n",
      "    agent-0: -103.0\n",
      "    agent-1: -103.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13640029456989575\n",
      "    mean_inference_ms: 1.1345004255420894\n",
      "    mean_processing_ms: 0.2840146776113145\n",
      "  time_since_restore: 348.46169257164\n",
      "  time_this_iter_s: 6.096217632293701\n",
      "  time_total_s: 348.46169257164\n",
      "  timers:\n",
      "    learn_throughput: 1094.64\n",
      "    learn_time_ms: 3836.877\n",
      "    load_throughput: 1252752.245\n",
      "    load_time_ms: 3.353\n",
      "    sample_throughput: 1868.955\n",
      "    sample_time_ms: 2247.245\n",
      "    update_time_ms: 3.147\n",
      "  timestamp: 1607961325\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 239400\n",
      "  training_iteration: 57\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         348.462</td><td style=\"text-align: right;\">239400</td><td style=\"text-align: right;\">      81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-55-31\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 252.0\n",
      "  episode_reward_mean: 75.56462585034014\n",
      "  episode_reward_min: -162.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 8400\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5392526388168335\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016334600746631622\n",
      "        model: {}\n",
      "        policy_loss: -0.10176778584718704\n",
      "        total_loss: 424.4258117675781\n",
      "        vf_explained_var: 0.5062123537063599\n",
      "        vf_loss: 424.4903869628906\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9298846125602722\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013888310641050339\n",
      "        model: {}\n",
      "        policy_loss: -0.11549623310565948\n",
      "        total_loss: 431.74505615234375\n",
      "        vf_explained_var: 0.4958503246307373\n",
      "        vf_loss: 431.8131103515625\n",
      "    num_steps_sampled: 243600\n",
      "    num_steps_trained: 243600\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.27777777777777\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 126.0\n",
      "    agent-1: 126.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 37.78231292517007\n",
      "    agent-1: 37.78231292517007\n",
      "  policy_reward_min:\n",
      "    agent-0: -81.0\n",
      "    agent-1: -81.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13635739632905747\n",
      "    mean_inference_ms: 1.1339318124294577\n",
      "    mean_processing_ms: 0.283914793227832\n",
      "  time_since_restore: 354.5169014930725\n",
      "  time_this_iter_s: 6.055208921432495\n",
      "  time_total_s: 354.5169014930725\n",
      "  timers:\n",
      "    learn_throughput: 1093.806\n",
      "    learn_time_ms: 3839.803\n",
      "    load_throughput: 1248702.945\n",
      "    load_time_ms: 3.363\n",
      "    sample_throughput: 1871.726\n",
      "    sample_time_ms: 2243.919\n",
      "    update_time_ms: 3.101\n",
      "  timestamp: 1607961331\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 243600\n",
      "  training_iteration: 58\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         354.517</td><td style=\"text-align: right;\">243600</td><td style=\"text-align: right;\"> 75.5646</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-55-37\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 272.0\n",
      "  episode_reward_mean: 84.44444444444444\n",
      "  episode_reward_min: -174.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 8544\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5396305322647095\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016701439395546913\n",
      "        model: {}\n",
      "        policy_loss: -0.09880216419696808\n",
      "        total_loss: 420.31427001953125\n",
      "        vf_explained_var: 0.5189430713653564\n",
      "        vf_loss: 420.375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9287450909614563\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0138503797352314\n",
      "        model: {}\n",
      "        policy_loss: -0.10710902512073517\n",
      "        total_loss: 405.09210205078125\n",
      "        vf_explained_var: 0.5262216925621033\n",
      "        vf_loss: 405.15185546875\n",
      "    num_steps_sampled: 247800\n",
      "    num_steps_trained: 247800\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.42500000000001\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 136.0\n",
      "    agent-1: 136.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 42.22222222222222\n",
      "    agent-1: 42.22222222222222\n",
      "  policy_reward_min:\n",
      "    agent-0: -87.0\n",
      "    agent-1: -87.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13634462082129561\n",
      "    mean_inference_ms: 1.1336572554060649\n",
      "    mean_processing_ms: 0.283870032266963\n",
      "  time_since_restore: 360.53131675720215\n",
      "  time_this_iter_s: 6.014415264129639\n",
      "  time_total_s: 360.53131675720215\n",
      "  timers:\n",
      "    learn_throughput: 1095.374\n",
      "    learn_time_ms: 3834.308\n",
      "    load_throughput: 1241530.538\n",
      "    load_time_ms: 3.383\n",
      "    sample_throughput: 1873.791\n",
      "    sample_time_ms: 2241.446\n",
      "    update_time_ms: 3.117\n",
      "  timestamp: 1607961337\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 247800\n",
      "  training_iteration: 59\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         360.531</td><td style=\"text-align: right;\">247800</td><td style=\"text-align: right;\"> 84.4444</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-55-44\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 230.0\n",
      "  episode_reward_mean: 79.27777777777777\n",
      "  episode_reward_min: -132.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 8688\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5391956567764282\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017273420467972755\n",
      "        model: {}\n",
      "        policy_loss: -0.1008676066994667\n",
      "        total_loss: 409.4873962402344\n",
      "        vf_explained_var: 0.5154226422309875\n",
      "        vf_loss: 409.54888916015625\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9163773655891418\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014294564723968506\n",
      "        model: {}\n",
      "        policy_loss: -0.10738269239664078\n",
      "        total_loss: 424.26104736328125\n",
      "        vf_explained_var: 0.48581671714782715\n",
      "        vf_loss: 424.319580078125\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.33333333333333\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 115.0\n",
      "    agent-1: 115.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 39.638888888888886\n",
      "    agent-1: 39.638888888888886\n",
      "  policy_reward_min:\n",
      "    agent-0: -66.0\n",
      "    agent-1: -66.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13633228913451287\n",
      "    mean_inference_ms: 1.1335398382307682\n",
      "    mean_processing_ms: 0.2838458545554162\n",
      "  time_since_restore: 366.7757821083069\n",
      "  time_this_iter_s: 6.244465351104736\n",
      "  time_total_s: 366.7757821083069\n",
      "  timers:\n",
      "    learn_throughput: 1092.326\n",
      "    learn_time_ms: 3845.006\n",
      "    load_throughput: 1323641.26\n",
      "    load_time_ms: 3.173\n",
      "    sample_throughput: 1875.218\n",
      "    sample_time_ms: 2239.74\n",
      "    update_time_ms: 3.096\n",
      "  timestamp: 1607961344\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 60\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         366.776</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\"> 79.2778</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-55-50\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 290.0\n",
      "  episode_reward_mean: 81.86111111111111\n",
      "  episode_reward_min: -174.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 8832\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5365537405014038\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016629893332719803\n",
      "        model: {}\n",
      "        policy_loss: -0.09783629328012466\n",
      "        total_loss: 407.96240234375\n",
      "        vf_explained_var: 0.5085563659667969\n",
      "        vf_loss: 408.0223693847656\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9264683723449707\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013385244645178318\n",
      "        model: {}\n",
      "        policy_loss: -0.10470081865787506\n",
      "        total_loss: 426.8644104003906\n",
      "        vf_explained_var: 0.4861103296279907\n",
      "        vf_loss: 426.92340087890625\n",
      "    num_steps_sampled: 256200\n",
      "    num_steps_trained: 256200\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.23333333333333\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 145.0\n",
      "    agent-1: 145.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 40.93055555555556\n",
      "    agent-1: 40.93055555555556\n",
      "  policy_reward_min:\n",
      "    agent-0: -87.0\n",
      "    agent-1: -87.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13630930827612173\n",
      "    mean_inference_ms: 1.1335091886379196\n",
      "    mean_processing_ms: 0.2837900219208627\n",
      "  time_since_restore: 372.9166610240936\n",
      "  time_this_iter_s: 6.140878915786743\n",
      "  time_total_s: 372.9166610240936\n",
      "  timers:\n",
      "    learn_throughput: 1090.273\n",
      "    learn_time_ms: 3852.246\n",
      "    load_throughput: 1312849.473\n",
      "    load_time_ms: 3.199\n",
      "    sample_throughput: 1878.122\n",
      "    sample_time_ms: 2236.277\n",
      "    update_time_ms: 3.146\n",
      "  timestamp: 1607961350\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256200\n",
      "  training_iteration: 61\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         372.917</td><td style=\"text-align: right;\">256200</td><td style=\"text-align: right;\"> 81.8611</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-55-56\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 234.0\n",
      "  episode_reward_mean: 90.04081632653062\n",
      "  episode_reward_min: -128.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 8979\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5338550806045532\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01726512797176838\n",
      "        model: {}\n",
      "        policy_loss: -0.1016436293721199\n",
      "        total_loss: 371.1318359375\n",
      "        vf_explained_var: 0.5318284034729004\n",
      "        vf_loss: 371.19415283203125\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9191950559616089\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013795549049973488\n",
      "        model: {}\n",
      "        policy_loss: -0.10363565385341644\n",
      "        total_loss: 377.15606689453125\n",
      "        vf_explained_var: 0.5178265571594238\n",
      "        vf_loss: 377.2125549316406\n",
      "    num_steps_sampled: 260400\n",
      "    num_steps_trained: 260400\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.03333333333335\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 117.0\n",
      "    agent-1: 117.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 45.02040816326531\n",
      "    agent-1: 45.02040816326531\n",
      "  policy_reward_min:\n",
      "    agent-0: -64.0\n",
      "    agent-1: -64.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13634324751775084\n",
      "    mean_inference_ms: 1.1338050926063967\n",
      "    mean_processing_ms: 0.2838336455897206\n",
      "  time_since_restore: 379.0741662979126\n",
      "  time_this_iter_s: 6.15750527381897\n",
      "  time_total_s: 379.0741662979126\n",
      "  timers:\n",
      "    learn_throughput: 1089.033\n",
      "    learn_time_ms: 3856.631\n",
      "    load_throughput: 1312233.364\n",
      "    load_time_ms: 3.201\n",
      "    sample_throughput: 1879.778\n",
      "    sample_time_ms: 2234.307\n",
      "    update_time_ms: 3.118\n",
      "  timestamp: 1607961356\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260400\n",
      "  training_iteration: 62\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         379.074</td><td style=\"text-align: right;\">260400</td><td style=\"text-align: right;\"> 90.0408</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-56-02\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 264.0\n",
      "  episode_reward_mean: 74.22222222222223\n",
      "  episode_reward_min: -172.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 9123\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5345091223716736\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016638781875371933\n",
      "        model: {}\n",
      "        policy_loss: -0.0993480384349823\n",
      "        total_loss: 421.544921875\n",
      "        vf_explained_var: 0.5067371129989624\n",
      "        vf_loss: 421.60638427734375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9156031608581543\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013800572603940964\n",
      "        model: {}\n",
      "        policy_loss: -0.10850977897644043\n",
      "        total_loss: 430.6711120605469\n",
      "        vf_explained_var: 0.4867499768733978\n",
      "        vf_loss: 430.7324523925781\n",
      "    num_steps_sampled: 264600\n",
      "    num_steps_trained: 264600\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.42222222222223\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 132.0\n",
      "    agent-1: 132.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 37.111111111111114\n",
      "    agent-1: 37.111111111111114\n",
      "  policy_reward_min:\n",
      "    agent-0: -86.0\n",
      "    agent-1: -86.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13633088420441286\n",
      "    mean_inference_ms: 1.133651899514654\n",
      "    mean_processing_ms: 0.28377250957875017\n",
      "  time_since_restore: 385.15847873687744\n",
      "  time_this_iter_s: 6.084312438964844\n",
      "  time_total_s: 385.15847873687744\n",
      "  timers:\n",
      "    learn_throughput: 1088.865\n",
      "    learn_time_ms: 3857.229\n",
      "    load_throughput: 1317582.408\n",
      "    load_time_ms: 3.188\n",
      "    sample_throughput: 1880.203\n",
      "    sample_time_ms: 2233.802\n",
      "    update_time_ms: 3.079\n",
      "  timestamp: 1607961362\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 264600\n",
      "  training_iteration: 63\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         385.158</td><td style=\"text-align: right;\">264600</td><td style=\"text-align: right;\"> 74.2222</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-56-09\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 266.0\n",
      "  episode_reward_mean: 85.70833333333333\n",
      "  episode_reward_min: -200.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 9267\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5314584970474243\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015828857198357582\n",
      "        model: {}\n",
      "        policy_loss: -0.09549996256828308\n",
      "        total_loss: 502.0425720214844\n",
      "        vf_explained_var: 0.4945429563522339\n",
      "        vf_loss: 502.10198974609375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9045132994651794\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013713310472667217\n",
      "        model: {}\n",
      "        policy_loss: -0.09946045279502869\n",
      "        total_loss: 493.38616943359375\n",
      "        vf_explained_var: 0.5028237700462341\n",
      "        vf_loss: 493.4387512207031\n",
      "    num_steps_sampled: 268800\n",
      "    num_steps_trained: 268800\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.51111111111112\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 133.0\n",
      "    agent-1: 133.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 42.854166666666664\n",
      "    agent-1: 42.854166666666664\n",
      "  policy_reward_min:\n",
      "    agent-0: -100.0\n",
      "    agent-1: -100.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13634425489696622\n",
      "    mean_inference_ms: 1.133891975007828\n",
      "    mean_processing_ms: 0.2837376134688111\n",
      "  time_since_restore: 391.33846950531006\n",
      "  time_this_iter_s: 6.179990768432617\n",
      "  time_total_s: 391.33846950531006\n",
      "  timers:\n",
      "    learn_throughput: 1088.589\n",
      "    learn_time_ms: 3858.207\n",
      "    load_throughput: 1322548.146\n",
      "    load_time_ms: 3.176\n",
      "    sample_throughput: 1873.403\n",
      "    sample_time_ms: 2241.91\n",
      "    update_time_ms: 3.024\n",
      "  timestamp: 1607961369\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268800\n",
      "  training_iteration: 64\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         391.338</td><td style=\"text-align: right;\">268800</td><td style=\"text-align: right;\"> 85.7083</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-56-15\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 276.0\n",
      "  episode_reward_mean: 83.0\n",
      "  episode_reward_min: -212.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 9411\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5411520004272461\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01693541184067726\n",
      "        model: {}\n",
      "        policy_loss: -0.10093264281749725\n",
      "        total_loss: 404.34722900390625\n",
      "        vf_explained_var: 0.5040217041969299\n",
      "        vf_loss: 404.40960693359375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9009733200073242\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013490431010723114\n",
      "        model: {}\n",
      "        policy_loss: -0.10148513317108154\n",
      "        total_loss: 403.5574951171875\n",
      "        vf_explained_var: 0.4985330104827881\n",
      "        vf_loss: 403.6129150390625\n",
      "    num_steps_sampled: 273000\n",
      "    num_steps_trained: 273000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.83333333333333\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 138.0\n",
      "    agent-1: 138.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 41.5\n",
      "    agent-1: 41.5\n",
      "  policy_reward_min:\n",
      "    agent-0: -106.0\n",
      "    agent-1: -106.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13632449880260086\n",
      "    mean_inference_ms: 1.1339151257086657\n",
      "    mean_processing_ms: 0.2837139809541602\n",
      "  time_since_restore: 397.44753980636597\n",
      "  time_this_iter_s: 6.109070301055908\n",
      "  time_total_s: 397.44753980636597\n",
      "  timers:\n",
      "    learn_throughput: 1088.984\n",
      "    learn_time_ms: 3856.806\n",
      "    load_throughput: 1292847.158\n",
      "    load_time_ms: 3.249\n",
      "    sample_throughput: 1873.442\n",
      "    sample_time_ms: 2241.863\n",
      "    update_time_ms: 2.997\n",
      "  timestamp: 1607961375\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 273000\n",
      "  training_iteration: 65\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         397.448</td><td style=\"text-align: right;\">273000</td><td style=\"text-align: right;\">      83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-56-21\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 264.0\n",
      "  episode_reward_mean: 83.17006802721089\n",
      "  episode_reward_min: -226.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 9558\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5347801446914673\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016009248793125153\n",
      "        model: {}\n",
      "        policy_loss: -0.09942074865102768\n",
      "        total_loss: 502.0951843261719\n",
      "        vf_explained_var: 0.49748754501342773\n",
      "        vf_loss: 502.15814208984375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9011729955673218\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012962596490979195\n",
      "        model: {}\n",
      "        policy_loss: -0.1025637537240982\n",
      "        total_loss: 497.7671813964844\n",
      "        vf_explained_var: 0.4847068190574646\n",
      "        vf_loss: 497.825439453125\n",
      "    num_steps_sampled: 277200\n",
      "    num_steps_trained: 277200\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.0625\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 132.0\n",
      "    agent-1: 132.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 41.585034013605444\n",
      "    agent-1: 41.585034013605444\n",
      "  policy_reward_min:\n",
      "    agent-0: -113.0\n",
      "    agent-1: -113.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13633531366827994\n",
      "    mean_inference_ms: 1.133800660278947\n",
      "    mean_processing_ms: 0.2836847919571684\n",
      "  time_since_restore: 403.5246202945709\n",
      "  time_this_iter_s: 6.077080488204956\n",
      "  time_total_s: 403.5246202945709\n",
      "  timers:\n",
      "    learn_throughput: 1088.947\n",
      "    learn_time_ms: 3856.936\n",
      "    load_throughput: 1300444.169\n",
      "    load_time_ms: 3.23\n",
      "    sample_throughput: 1874.467\n",
      "    sample_time_ms: 2240.637\n",
      "    update_time_ms: 3.207\n",
      "  timestamp: 1607961381\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 277200\n",
      "  training_iteration: 66\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         403.525</td><td style=\"text-align: right;\">277200</td><td style=\"text-align: right;\"> 83.1701</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-56-27\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 238.0\n",
      "  episode_reward_mean: 93.22222222222223\n",
      "  episode_reward_min: -152.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 9702\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5307188630104065\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015996651723980904\n",
      "        model: {}\n",
      "        policy_loss: -0.09871159493923187\n",
      "        total_loss: 434.27777099609375\n",
      "        vf_explained_var: 0.5443931818008423\n",
      "        vf_loss: 434.34002685546875\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8956869840621948\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013283474370837212\n",
      "        model: {}\n",
      "        policy_loss: -0.09935468435287476\n",
      "        total_loss: 446.25103759765625\n",
      "        vf_explained_var: 0.5209540128707886\n",
      "        vf_loss: 446.30499267578125\n",
      "    num_steps_sampled: 281400\n",
      "    num_steps_trained: 281400\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.4\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 119.0\n",
      "    agent-1: 119.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 46.611111111111114\n",
      "    agent-1: 46.611111111111114\n",
      "  policy_reward_min:\n",
      "    agent-0: -76.0\n",
      "    agent-1: -76.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13635939643641803\n",
      "    mean_inference_ms: 1.1338901666857921\n",
      "    mean_processing_ms: 0.283642925895882\n",
      "  time_since_restore: 409.60333037376404\n",
      "  time_this_iter_s: 6.078710079193115\n",
      "  time_total_s: 409.60333037376404\n",
      "  timers:\n",
      "    learn_throughput: 1089.668\n",
      "    learn_time_ms: 3854.385\n",
      "    load_throughput: 1292932.557\n",
      "    load_time_ms: 3.248\n",
      "    sample_throughput: 1874.32\n",
      "    sample_time_ms: 2240.812\n",
      "    update_time_ms: 3.233\n",
      "  timestamp: 1607961387\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 281400\n",
      "  training_iteration: 67\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         409.603</td><td style=\"text-align: right;\">281400</td><td style=\"text-align: right;\"> 93.2222</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-56-33\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 262.0\n",
      "  episode_reward_mean: 79.95833333333333\n",
      "  episode_reward_min: -134.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 9846\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5253662467002869\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01870378851890564\n",
      "        model: {}\n",
      "        policy_loss: -0.09679049253463745\n",
      "        total_loss: 445.9999084472656\n",
      "        vf_explained_var: 0.4882870316505432\n",
      "        vf_loss: 446.0541076660156\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8996002674102783\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013256847858428955\n",
      "        model: {}\n",
      "        policy_loss: -0.11101339757442474\n",
      "        total_loss: 457.9522399902344\n",
      "        vf_explained_var: 0.45581477880477905\n",
      "        vf_loss: 458.0179443359375\n",
      "    num_steps_sampled: 285600\n",
      "    num_steps_trained: 285600\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.69999999999999\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 131.0\n",
      "    agent-1: 131.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 39.979166666666664\n",
      "    agent-1: 39.979166666666664\n",
      "  policy_reward_min:\n",
      "    agent-0: -67.0\n",
      "    agent-1: -67.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13633657026769563\n",
      "    mean_inference_ms: 1.1336555361682212\n",
      "    mean_processing_ms: 0.28360848949347406\n",
      "  time_since_restore: 415.6552107334137\n",
      "  time_this_iter_s: 6.051880359649658\n",
      "  time_total_s: 415.6552107334137\n",
      "  timers:\n",
      "    learn_throughput: 1090.76\n",
      "    learn_time_ms: 3850.527\n",
      "    load_throughput: 1292429.81\n",
      "    load_time_ms: 3.25\n",
      "    sample_throughput: 1871.561\n",
      "    sample_time_ms: 2244.116\n",
      "    update_time_ms: 3.319\n",
      "  timestamp: 1607961393\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 285600\n",
      "  training_iteration: 68\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         415.655</td><td style=\"text-align: right;\">285600</td><td style=\"text-align: right;\"> 79.9583</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-56-39\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 274.0\n",
      "  episode_reward_mean: 96.51700680272108\n",
      "  episode_reward_min: -212.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 9993\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5247189998626709\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016640573740005493\n",
      "        model: {}\n",
      "        policy_loss: -0.09709703922271729\n",
      "        total_loss: 442.719482421875\n",
      "        vf_explained_var: 0.5183127522468567\n",
      "        vf_loss: 442.7786865234375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9000675678253174\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0139146214351058\n",
      "        model: {}\n",
      "        policy_loss: -0.10775239765644073\n",
      "        total_loss: 448.81549072265625\n",
      "        vf_explained_var: 0.49195176362991333\n",
      "        vf_loss: 448.8757019042969\n",
      "    num_steps_sampled: 289800\n",
      "    num_steps_trained: 289800\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.48888888888888\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 137.0\n",
      "    agent-1: 137.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 48.25850340136054\n",
      "    agent-1: 48.25850340136054\n",
      "  policy_reward_min:\n",
      "    agent-0: -106.0\n",
      "    agent-1: -106.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13632514267211818\n",
      "    mean_inference_ms: 1.1334928784713139\n",
      "    mean_processing_ms: 0.283598678125843\n",
      "  time_since_restore: 421.82129645347595\n",
      "  time_this_iter_s: 6.166085720062256\n",
      "  time_total_s: 421.82129645347595\n",
      "  timers:\n",
      "    learn_throughput: 1087.171\n",
      "    learn_time_ms: 3863.238\n",
      "    load_throughput: 1282288.31\n",
      "    load_time_ms: 3.275\n",
      "    sample_throughput: 1869.524\n",
      "    sample_time_ms: 2246.561\n",
      "    update_time_ms: 3.348\n",
      "  timestamp: 1607961399\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 289800\n",
      "  training_iteration: 69\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         421.821</td><td style=\"text-align: right;\">289800</td><td style=\"text-align: right;\">  96.517</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-56-46\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 262.0\n",
      "  episode_reward_mean: 86.47222222222223\n",
      "  episode_reward_min: -172.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 10137\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5169556736946106\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015662511810660362\n",
      "        model: {}\n",
      "        policy_loss: -0.09607099741697311\n",
      "        total_loss: 448.3619689941406\n",
      "        vf_explained_var: 0.502495288848877\n",
      "        vf_loss: 448.42236328125\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8915492296218872\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012712186202406883\n",
      "        model: {}\n",
      "        policy_loss: -0.10351862013339996\n",
      "        total_loss: 441.208984375\n",
      "        vf_explained_var: 0.4752338230609894\n",
      "        vf_loss: 441.26910400390625\n",
      "    num_steps_sampled: 294000\n",
      "    num_steps_trained: 294000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.37777777777777\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 131.0\n",
      "    agent-1: 131.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 43.236111111111114\n",
      "    agent-1: 43.236111111111114\n",
      "  policy_reward_min:\n",
      "    agent-0: -86.0\n",
      "    agent-1: -86.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13632780970291813\n",
      "    mean_inference_ms: 1.1334439564517074\n",
      "    mean_processing_ms: 0.2835855140721976\n",
      "  time_since_restore: 427.9295349121094\n",
      "  time_this_iter_s: 6.108238458633423\n",
      "  time_total_s: 427.9295349121094\n",
      "  timers:\n",
      "    learn_throughput: 1091.323\n",
      "    learn_time_ms: 3848.54\n",
      "    load_throughput: 1266277.795\n",
      "    load_time_ms: 3.317\n",
      "    sample_throughput: 1868.705\n",
      "    sample_time_ms: 2247.546\n",
      "    update_time_ms: 3.347\n",
      "  timestamp: 1607961406\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 294000\n",
      "  training_iteration: 70\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">          427.93</td><td style=\"text-align: right;\">294000</td><td style=\"text-align: right;\"> 86.4722</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-56-52\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 218.0\n",
      "  episode_reward_mean: 95.58333333333333\n",
      "  episode_reward_min: -108.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 10281\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5188999176025391\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017138933762907982\n",
      "        model: {}\n",
      "        policy_loss: -0.09707257151603699\n",
      "        total_loss: 366.5807189941406\n",
      "        vf_explained_var: 0.5180519819259644\n",
      "        vf_loss: 366.63873291015625\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8805052042007446\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012910143472254276\n",
      "        model: {}\n",
      "        policy_loss: -0.10689139366149902\n",
      "        total_loss: 357.51971435546875\n",
      "        vf_explained_var: 0.5020853281021118\n",
      "        vf_loss: 357.58251953125\n",
      "    num_steps_sampled: 298200\n",
      "    num_steps_trained: 298200\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.5875\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 109.0\n",
      "    agent-1: 109.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 47.791666666666664\n",
      "    agent-1: 47.791666666666664\n",
      "  policy_reward_min:\n",
      "    agent-0: -54.0\n",
      "    agent-1: -54.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13632782715982572\n",
      "    mean_inference_ms: 1.133414466412662\n",
      "    mean_processing_ms: 0.28356033310586526\n",
      "  time_since_restore: 433.9316806793213\n",
      "  time_this_iter_s: 6.002145767211914\n",
      "  time_total_s: 433.9316806793213\n",
      "  timers:\n",
      "    learn_throughput: 1095.655\n",
      "    learn_time_ms: 3833.325\n",
      "    load_throughput: 1268667.084\n",
      "    load_time_ms: 3.311\n",
      "    sample_throughput: 1867.349\n",
      "    sample_time_ms: 2249.178\n",
      "    update_time_ms: 3.311\n",
      "  timestamp: 1607961412\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 298200\n",
      "  training_iteration: 71\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         433.932</td><td style=\"text-align: right;\">298200</td><td style=\"text-align: right;\"> 95.5833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-56-58\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 266.0\n",
      "  episode_reward_mean: 95.65277777777777\n",
      "  episode_reward_min: -142.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 10425\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.511955738067627\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016131136566400528\n",
      "        model: {}\n",
      "        policy_loss: -0.09698043018579483\n",
      "        total_loss: 416.7928466796875\n",
      "        vf_explained_var: 0.5124643445014954\n",
      "        vf_loss: 416.8531188964844\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8794317245483398\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01272224448621273\n",
      "        model: {}\n",
      "        policy_loss: -0.1057363972067833\n",
      "        total_loss: 429.6176452636719\n",
      "        vf_explained_var: 0.49306511878967285\n",
      "        vf_loss: 429.679931640625\n",
      "    num_steps_sampled: 302400\n",
      "    num_steps_trained: 302400\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.86666666666666\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 133.0\n",
      "    agent-1: 133.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 47.826388888888886\n",
      "    agent-1: 47.826388888888886\n",
      "  policy_reward_min:\n",
      "    agent-0: -71.0\n",
      "    agent-1: -71.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13631791406498014\n",
      "    mean_inference_ms: 1.133494752689072\n",
      "    mean_processing_ms: 0.2835607670443253\n",
      "  time_since_restore: 440.07135486602783\n",
      "  time_this_iter_s: 6.139674186706543\n",
      "  time_total_s: 440.07135486602783\n",
      "  timers:\n",
      "    learn_throughput: 1095.221\n",
      "    learn_time_ms: 3834.841\n",
      "    load_throughput: 1248472.853\n",
      "    load_time_ms: 3.364\n",
      "    sample_throughput: 1870.012\n",
      "    sample_time_ms: 2245.974\n",
      "    update_time_ms: 3.386\n",
      "  timestamp: 1607961418\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 302400\n",
      "  training_iteration: 72\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         440.071</td><td style=\"text-align: right;\">302400</td><td style=\"text-align: right;\"> 95.6528</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-57-04\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 272.0\n",
      "  episode_reward_mean: 95.63265306122449\n",
      "  episode_reward_min: -86.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 10572\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.515598714351654\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016067268326878548\n",
      "        model: {}\n",
      "        policy_loss: -0.09701640903949738\n",
      "        total_loss: 380.4105224609375\n",
      "        vf_explained_var: 0.5114991664886475\n",
      "        vf_loss: 380.470947265625\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8872538805007935\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013575518503785133\n",
      "        model: {}\n",
      "        policy_loss: -0.10443882644176483\n",
      "        total_loss: 373.61810302734375\n",
      "        vf_explained_var: 0.5114281177520752\n",
      "        vf_loss: 373.6761169433594\n",
      "    num_steps_sampled: 306600\n",
      "    num_steps_trained: 306600\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.16666666666667\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 136.0\n",
      "    agent-1: 136.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 47.816326530612244\n",
      "    agent-1: 47.816326530612244\n",
      "  policy_reward_min:\n",
      "    agent-0: -43.0\n",
      "    agent-1: -43.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1363349081484292\n",
      "    mean_inference_ms: 1.1335580787068078\n",
      "    mean_processing_ms: 0.2835721386674437\n",
      "  time_since_restore: 446.1857748031616\n",
      "  time_this_iter_s: 6.114419937133789\n",
      "  time_total_s: 446.1857748031616\n",
      "  timers:\n",
      "    learn_throughput: 1094.951\n",
      "    learn_time_ms: 3835.787\n",
      "    load_throughput: 1240263.09\n",
      "    load_time_ms: 3.386\n",
      "    sample_throughput: 1868.244\n",
      "    sample_time_ms: 2248.1\n",
      "    update_time_ms: 3.363\n",
      "  timestamp: 1607961424\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 306600\n",
      "  training_iteration: 73\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         446.186</td><td style=\"text-align: right;\">306600</td><td style=\"text-align: right;\"> 95.6327</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-57-10\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 330.0\n",
      "  episode_reward_mean: 92.72222222222223\n",
      "  episode_reward_min: -120.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 10716\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.507352352142334\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015262839384377003\n",
      "        model: {}\n",
      "        policy_loss: -0.09273746609687805\n",
      "        total_loss: 397.423095703125\n",
      "        vf_explained_var: 0.5096707344055176\n",
      "        vf_loss: 397.4810791015625\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8934574127197266\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012843082658946514\n",
      "        model: {}\n",
      "        policy_loss: -0.09628456830978394\n",
      "        total_loss: 402.921875\n",
      "        vf_explained_var: 0.4950123429298401\n",
      "        vf_loss: 402.9742431640625\n",
      "    num_steps_sampled: 310800\n",
      "    num_steps_trained: 310800\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.84444444444445\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 165.0\n",
      "    agent-1: 165.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 46.361111111111114\n",
      "    agent-1: 46.361111111111114\n",
      "  policy_reward_min:\n",
      "    agent-0: -60.0\n",
      "    agent-1: -60.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13632097582187852\n",
      "    mean_inference_ms: 1.1335582580776533\n",
      "    mean_processing_ms: 0.2835585995891798\n",
      "  time_since_restore: 452.3133523464203\n",
      "  time_this_iter_s: 6.127577543258667\n",
      "  time_total_s: 452.3133523464203\n",
      "  timers:\n",
      "    learn_throughput: 1095.283\n",
      "    learn_time_ms: 3834.625\n",
      "    load_throughput: 1247995.239\n",
      "    load_time_ms: 3.365\n",
      "    sample_throughput: 1871.771\n",
      "    sample_time_ms: 2243.865\n",
      "    update_time_ms: 3.378\n",
      "  timestamp: 1607961430\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 310800\n",
      "  training_iteration: 74\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         452.313</td><td style=\"text-align: right;\">310800</td><td style=\"text-align: right;\"> 92.7222</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-57-16\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 246.0\n",
      "  episode_reward_mean: 93.09722222222223\n",
      "  episode_reward_min: -116.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 10860\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5077576637268066\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016296522691845894\n",
      "        model: {}\n",
      "        policy_loss: -0.09428450465202332\n",
      "        total_loss: 344.68878173828125\n",
      "        vf_explained_var: 0.5109023451805115\n",
      "        vf_loss: 344.7459411621094\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8804935812950134\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013582741841673851\n",
      "        model: {}\n",
      "        policy_loss: -0.10026322305202484\n",
      "        total_loss: 341.72216796875\n",
      "        vf_explained_var: 0.5001657009124756\n",
      "        vf_loss: 341.77606201171875\n",
      "    num_steps_sampled: 315000\n",
      "    num_steps_trained: 315000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.025\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 123.0\n",
      "    agent-1: 123.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 46.548611111111114\n",
      "    agent-1: 46.548611111111114\n",
      "  policy_reward_min:\n",
      "    agent-0: -58.0\n",
      "    agent-1: -58.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1363326583878629\n",
      "    mean_inference_ms: 1.133654009600563\n",
      "    mean_processing_ms: 0.2835667904626979\n",
      "  time_since_restore: 458.38442182540894\n",
      "  time_this_iter_s: 6.0710694789886475\n",
      "  time_total_s: 458.38442182540894\n",
      "  timers:\n",
      "    learn_throughput: 1096.529\n",
      "    learn_time_ms: 3830.268\n",
      "    load_throughput: 1263770.548\n",
      "    load_time_ms: 3.323\n",
      "    sample_throughput: 1871.55\n",
      "    sample_time_ms: 2244.129\n",
      "    update_time_ms: 3.382\n",
      "  timestamp: 1607961436\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 315000\n",
      "  training_iteration: 75\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         458.384</td><td style=\"text-align: right;\">315000</td><td style=\"text-align: right;\"> 93.0972</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-57-22\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 248.0\n",
      "  episode_reward_mean: 98.95833333333333\n",
      "  episode_reward_min: -98.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 11004\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5096443891525269\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015539071522653103\n",
      "        model: {}\n",
      "        policy_loss: -0.09436358511447906\n",
      "        total_loss: 311.5224609375\n",
      "        vf_explained_var: 0.5675249099731445\n",
      "        vf_loss: 311.5814208984375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8831490278244019\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013090156018733978\n",
      "        model: {}\n",
      "        policy_loss: -0.1009540855884552\n",
      "        total_loss: 312.12042236328125\n",
      "        vf_explained_var: 0.5600283741950989\n",
      "        vf_loss: 312.1766357421875\n",
      "    num_steps_sampled: 319200\n",
      "    num_steps_trained: 319200\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.55555555555556\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 124.0\n",
      "    agent-1: 124.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 49.479166666666664\n",
      "    agent-1: 49.479166666666664\n",
      "  policy_reward_min:\n",
      "    agent-0: -49.0\n",
      "    agent-1: -49.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13632623445643258\n",
      "    mean_inference_ms: 1.1333314356575919\n",
      "    mean_processing_ms: 0.2835192264340302\n",
      "  time_since_restore: 464.4925558567047\n",
      "  time_this_iter_s: 6.108134031295776\n",
      "  time_total_s: 464.4925558567047\n",
      "  timers:\n",
      "    learn_throughput: 1094.91\n",
      "    learn_time_ms: 3835.93\n",
      "    load_throughput: 1265331.868\n",
      "    load_time_ms: 3.319\n",
      "    sample_throughput: 1873.588\n",
      "    sample_time_ms: 2241.688\n",
      "    update_time_ms: 3.185\n",
      "  timestamp: 1607961442\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 319200\n",
      "  training_iteration: 76\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         464.493</td><td style=\"text-align: right;\">319200</td><td style=\"text-align: right;\"> 98.9583</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-57-28\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 244.0\n",
      "  episode_reward_mean: 103.42857142857143\n",
      "  episode_reward_min: -122.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 11151\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5082709789276123\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01522121112793684\n",
      "        model: {}\n",
      "        policy_loss: -0.0945451557636261\n",
      "        total_loss: 392.87353515625\n",
      "        vf_explained_var: 0.540500283241272\n",
      "        vf_loss: 392.93341064453125\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8812575936317444\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013012757524847984\n",
      "        model: {}\n",
      "        policy_loss: -0.10123401135206223\n",
      "        total_loss: 401.90081787109375\n",
      "        vf_explained_var: 0.5157922506332397\n",
      "        vf_loss: 401.95758056640625\n",
      "    num_steps_sampled: 323400\n",
      "    num_steps_trained: 323400\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.51111111111112\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 122.0\n",
      "    agent-1: 122.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 51.714285714285715\n",
      "    agent-1: 51.714285714285715\n",
      "  policy_reward_min:\n",
      "    agent-0: -61.0\n",
      "    agent-1: -61.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1362994399540657\n",
      "    mean_inference_ms: 1.133177787138214\n",
      "    mean_processing_ms: 0.28350486802788966\n",
      "  time_since_restore: 470.5045473575592\n",
      "  time_this_iter_s: 6.011991500854492\n",
      "  time_total_s: 470.5045473575592\n",
      "  timers:\n",
      "    learn_throughput: 1095.738\n",
      "    learn_time_ms: 3833.033\n",
      "    load_throughput: 1240044.826\n",
      "    load_time_ms: 3.387\n",
      "    sample_throughput: 1876.447\n",
      "    sample_time_ms: 2238.273\n",
      "    update_time_ms: 3.179\n",
      "  timestamp: 1607961448\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 323400\n",
      "  training_iteration: 77\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         470.505</td><td style=\"text-align: right;\">323400</td><td style=\"text-align: right;\"> 103.429</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-57-35\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 278.0\n",
      "  episode_reward_mean: 107.84722222222223\n",
      "  episode_reward_min: -80.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 11295\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5083361268043518\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01664988324046135\n",
      "        model: {}\n",
      "        policy_loss: -0.09813236445188522\n",
      "        total_loss: 343.31158447265625\n",
      "        vf_explained_var: 0.550983190536499\n",
      "        vf_loss: 343.37176513671875\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.86954665184021\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012912655249238014\n",
      "        model: {}\n",
      "        policy_loss: -0.09832674264907837\n",
      "        total_loss: 341.92694091796875\n",
      "        vf_explained_var: 0.5389573574066162\n",
      "        vf_loss: 341.98114013671875\n",
      "    num_steps_sampled: 327600\n",
      "    num_steps_trained: 327600\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.71111111111112\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 139.0\n",
      "    agent-1: 139.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 53.923611111111114\n",
      "    agent-1: 53.923611111111114\n",
      "  policy_reward_min:\n",
      "    agent-0: -40.0\n",
      "    agent-1: -40.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13631042384543265\n",
      "    mean_inference_ms: 1.1331635571680942\n",
      "    mean_processing_ms: 0.28352282601633516\n",
      "  time_since_restore: 476.6222574710846\n",
      "  time_this_iter_s: 6.117710113525391\n",
      "  time_total_s: 476.6222574710846\n",
      "  timers:\n",
      "    learn_throughput: 1094.705\n",
      "    learn_time_ms: 3836.65\n",
      "    load_throughput: 1236858.214\n",
      "    load_time_ms: 3.396\n",
      "    sample_throughput: 1873.814\n",
      "    sample_time_ms: 2241.418\n",
      "    update_time_ms: 3.095\n",
      "  timestamp: 1607961455\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 327600\n",
      "  training_iteration: 78\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         476.622</td><td style=\"text-align: right;\">327600</td><td style=\"text-align: right;\"> 107.847</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-57-41\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 268.0\n",
      "  episode_reward_mean: 95.61111111111111\n",
      "  episode_reward_min: -144.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 11439\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5114538073539734\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017067063599824905\n",
      "        model: {}\n",
      "        policy_loss: -0.0887044370174408\n",
      "        total_loss: 372.24176025390625\n",
      "        vf_explained_var: 0.5309808254241943\n",
      "        vf_loss: 372.2915954589844\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8767319917678833\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013334553688764572\n",
      "        model: {}\n",
      "        policy_loss: -0.10356462001800537\n",
      "        total_loss: 365.39764404296875\n",
      "        vf_explained_var: 0.519511878490448\n",
      "        vf_loss: 365.45562744140625\n",
      "    num_steps_sampled: 331800\n",
      "    num_steps_trained: 331800\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.6375\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 134.0\n",
      "    agent-1: 134.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 47.80555555555556\n",
      "    agent-1: 47.80555555555556\n",
      "  policy_reward_min:\n",
      "    agent-0: -72.0\n",
      "    agent-1: -72.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13633231253169015\n",
      "    mean_inference_ms: 1.1330506447523065\n",
      "    mean_processing_ms: 0.2835216167768173\n",
      "  time_since_restore: 482.70614981651306\n",
      "  time_this_iter_s: 6.083892345428467\n",
      "  time_total_s: 482.70614981651306\n",
      "  timers:\n",
      "    learn_throughput: 1097.301\n",
      "    learn_time_ms: 3827.573\n",
      "    load_throughput: 1246538.126\n",
      "    load_time_ms: 3.369\n",
      "    sample_throughput: 1873.151\n",
      "    sample_time_ms: 2242.212\n",
      "    update_time_ms: 3.043\n",
      "  timestamp: 1607961461\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 331800\n",
      "  training_iteration: 79\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         482.706</td><td style=\"text-align: right;\">331800</td><td style=\"text-align: right;\"> 95.6111</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-57-47\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 268.0\n",
      "  episode_reward_mean: 95.80952380952381\n",
      "  episode_reward_min: -106.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 11586\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5124011039733887\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01587008871138096\n",
      "        model: {}\n",
      "        policy_loss: -0.09587044268846512\n",
      "        total_loss: 391.56707763671875\n",
      "        vf_explained_var: 0.5127736330032349\n",
      "        vf_loss: 391.6268005371094\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8679534792900085\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01305566169321537\n",
      "        model: {}\n",
      "        policy_loss: -0.10436452925205231\n",
      "        total_loss: 376.1503601074219\n",
      "        vf_explained_var: 0.511811375617981\n",
      "        vf_loss: 376.2101135253906\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.38888888888889\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 134.0\n",
      "    agent-1: 134.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 47.904761904761905\n",
      "    agent-1: 47.904761904761905\n",
      "  policy_reward_min:\n",
      "    agent-0: -53.0\n",
      "    agent-1: -53.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1363236745561869\n",
      "    mean_inference_ms: 1.1328708921055404\n",
      "    mean_processing_ms: 0.28352180795303444\n",
      "  time_since_restore: 488.78977060317993\n",
      "  time_this_iter_s: 6.08362078666687\n",
      "  time_total_s: 488.78977060317993\n",
      "  timers:\n",
      "    learn_throughput: 1097.254\n",
      "    learn_time_ms: 3827.738\n",
      "    load_throughput: 1255367.986\n",
      "    load_time_ms: 3.346\n",
      "    sample_throughput: 1875.213\n",
      "    sample_time_ms: 2239.745\n",
      "    update_time_ms: 3.06\n",
      "  timestamp: 1607961467\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 80\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">          488.79</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\"> 95.8095</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-57-53\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 252.0\n",
      "  episode_reward_mean: 109.23611111111111\n",
      "  episode_reward_min: -120.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 11730\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5123047828674316\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01702931709587574\n",
      "        model: {}\n",
      "        policy_loss: -0.09549297392368317\n",
      "        total_loss: 348.4210510253906\n",
      "        vf_explained_var: 0.5330506563186646\n",
      "        vf_loss: 348.477783203125\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8739560842514038\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01308886893093586\n",
      "        model: {}\n",
      "        policy_loss: -0.09959467500448227\n",
      "        total_loss: 339.68670654296875\n",
      "        vf_explained_var: 0.530346691608429\n",
      "        vf_loss: 339.7415771484375\n",
      "    num_steps_sampled: 340200\n",
      "    num_steps_trained: 340200\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.71111111111112\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 126.0\n",
      "    agent-1: 126.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 54.61805555555556\n",
      "    agent-1: 54.61805555555556\n",
      "  policy_reward_min:\n",
      "    agent-0: -60.0\n",
      "    agent-1: -60.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13633373449762326\n",
      "    mean_inference_ms: 1.1330378119241342\n",
      "    mean_processing_ms: 0.2835250079034538\n",
      "  time_since_restore: 494.95524978637695\n",
      "  time_this_iter_s: 6.1654791831970215\n",
      "  time_total_s: 494.95524978637695\n",
      "  timers:\n",
      "    learn_throughput: 1093.326\n",
      "    learn_time_ms: 3841.49\n",
      "    load_throughput: 1250733.198\n",
      "    load_time_ms: 3.358\n",
      "    sample_throughput: 1873.132\n",
      "    sample_time_ms: 2242.234\n",
      "    update_time_ms: 3.035\n",
      "  timestamp: 1607961473\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340200\n",
      "  training_iteration: 81\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         494.955</td><td style=\"text-align: right;\">340200</td><td style=\"text-align: right;\"> 109.236</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-57-59\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 266.0\n",
      "  episode_reward_mean: 117.34722222222223\n",
      "  episode_reward_min: -60.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 11874\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5165066719055176\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015558376908302307\n",
      "        model: {}\n",
      "        policy_loss: -0.09913378208875656\n",
      "        total_loss: 326.42205810546875\n",
      "        vf_explained_var: 0.5888373851776123\n",
      "        vf_loss: 326.4857177734375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8628180623054504\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013419710099697113\n",
      "        model: {}\n",
      "        policy_loss: -0.09539321064949036\n",
      "        total_loss: 339.673828125\n",
      "        vf_explained_var: 0.5726651549339294\n",
      "        vf_loss: 339.723388671875\n",
      "    num_steps_sampled: 344400\n",
      "    num_steps_trained: 344400\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.93333333333334\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 133.0\n",
      "    agent-1: 133.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 58.673611111111114\n",
      "    agent-1: 58.673611111111114\n",
      "  policy_reward_min:\n",
      "    agent-0: -30.0\n",
      "    agent-1: -30.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13633075738069278\n",
      "    mean_inference_ms: 1.1331711714046684\n",
      "    mean_processing_ms: 0.2835442063064968\n",
      "  time_since_restore: 501.09919261932373\n",
      "  time_this_iter_s: 6.143942832946777\n",
      "  time_total_s: 501.09919261932373\n",
      "  timers:\n",
      "    learn_throughput: 1094.014\n",
      "    learn_time_ms: 3839.073\n",
      "    load_throughput: 1273767.475\n",
      "    load_time_ms: 3.297\n",
      "    sample_throughput: 1870.765\n",
      "    sample_time_ms: 2245.071\n",
      "    update_time_ms: 2.977\n",
      "  timestamp: 1607961479\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 344400\n",
      "  training_iteration: 82\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         501.099</td><td style=\"text-align: right;\">344400</td><td style=\"text-align: right;\"> 117.347</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-58-06\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 266.0\n",
      "  episode_reward_mean: 98.34722222222223\n",
      "  episode_reward_min: -122.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 12018\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5221406817436218\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01631518080830574\n",
      "        model: {}\n",
      "        policy_loss: -0.0998256504535675\n",
      "        total_loss: 396.43463134765625\n",
      "        vf_explained_var: 0.4986363649368286\n",
      "        vf_loss: 396.497314453125\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8606290817260742\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013147035613656044\n",
      "        model: {}\n",
      "        policy_loss: -0.10132291913032532\n",
      "        total_loss: 378.34820556640625\n",
      "        vf_explained_var: 0.5129656791687012\n",
      "        vf_loss: 378.40460205078125\n",
      "    num_steps_sampled: 348600\n",
      "    num_steps_trained: 348600\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.72222222222223\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 133.0\n",
      "    agent-1: 133.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 49.173611111111114\n",
      "    agent-1: 49.173611111111114\n",
      "  policy_reward_min:\n",
      "    agent-0: -61.0\n",
      "    agent-1: -61.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13633225853471057\n",
      "    mean_inference_ms: 1.133270617370932\n",
      "    mean_processing_ms: 0.2835744156403048\n",
      "  time_since_restore: 507.26443433761597\n",
      "  time_this_iter_s: 6.165241718292236\n",
      "  time_total_s: 507.26443433761597\n",
      "  timers:\n",
      "    learn_throughput: 1092.36\n",
      "    learn_time_ms: 3844.888\n",
      "    load_throughput: 1285057.104\n",
      "    load_time_ms: 3.268\n",
      "    sample_throughput: 1871.457\n",
      "    sample_time_ms: 2244.24\n",
      "    update_time_ms: 3.03\n",
      "  timestamp: 1607961486\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 348600\n",
      "  training_iteration: 83\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         507.264</td><td style=\"text-align: right;\">348600</td><td style=\"text-align: right;\"> 98.3472</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-58-12\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 268.0\n",
      "  episode_reward_mean: 102.82993197278911\n",
      "  episode_reward_min: -108.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 12165\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5170713663101196\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015721052885055542\n",
      "        model: {}\n",
      "        policy_loss: -0.09820258617401123\n",
      "        total_loss: 388.8718566894531\n",
      "        vf_explained_var: 0.5483179688453674\n",
      "        vf_loss: 388.9342041015625\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8592936396598816\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012470570392906666\n",
      "        model: {}\n",
      "        policy_loss: -0.10338837653398514\n",
      "        total_loss: 390.96612548828125\n",
      "        vf_explained_var: 0.5462685227394104\n",
      "        vf_loss: 391.0268859863281\n",
      "    num_steps_sampled: 352800\n",
      "    num_steps_trained: 352800\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.87777777777778\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 134.0\n",
      "    agent-1: 134.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 51.414965986394556\n",
      "    agent-1: 51.414965986394556\n",
      "  policy_reward_min:\n",
      "    agent-0: -54.0\n",
      "    agent-1: -54.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1363306261539785\n",
      "    mean_inference_ms: 1.133509049985531\n",
      "    mean_processing_ms: 0.2836041881880368\n",
      "  time_since_restore: 513.4648418426514\n",
      "  time_this_iter_s: 6.2004075050354\n",
      "  time_total_s: 513.4648418426514\n",
      "  timers:\n",
      "    learn_throughput: 1092.08\n",
      "    learn_time_ms: 3845.873\n",
      "    load_throughput: 1213437.355\n",
      "    load_time_ms: 3.461\n",
      "    sample_throughput: 1866.533\n",
      "    sample_time_ms: 2250.161\n",
      "    update_time_ms: 3.04\n",
      "  timestamp: 1607961492\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 352800\n",
      "  training_iteration: 84\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         513.465</td><td style=\"text-align: right;\">352800</td><td style=\"text-align: right;\">  102.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-58-18\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 274.0\n",
      "  episode_reward_mean: 101.81944444444444\n",
      "  episode_reward_min: -120.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 12309\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5164703130722046\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017333928495645523\n",
      "        model: {}\n",
      "        policy_loss: -0.1005735844373703\n",
      "        total_loss: 418.8202209472656\n",
      "        vf_explained_var: 0.521514356136322\n",
      "        vf_loss: 418.88128662109375\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8588950634002686\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012773189693689346\n",
      "        model: {}\n",
      "        policy_loss: -0.09248347580432892\n",
      "        total_loss: 417.668701171875\n",
      "        vf_explained_var: 0.4985651969909668\n",
      "        vf_loss: 417.717529296875\n",
      "    num_steps_sampled: 357000\n",
      "    num_steps_trained: 357000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.4875\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 137.0\n",
      "    agent-1: 137.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 50.90972222222222\n",
      "    agent-1: 50.90972222222222\n",
      "  policy_reward_min:\n",
      "    agent-0: -60.0\n",
      "    agent-1: -60.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1363293614238198\n",
      "    mean_inference_ms: 1.1335378820439224\n",
      "    mean_processing_ms: 0.28357030220715707\n",
      "  time_since_restore: 519.5243544578552\n",
      "  time_this_iter_s: 6.059512615203857\n",
      "  time_total_s: 519.5243544578552\n",
      "  timers:\n",
      "    learn_throughput: 1091.606\n",
      "    learn_time_ms: 3847.541\n",
      "    load_throughput: 1228131.792\n",
      "    load_time_ms: 3.42\n",
      "    sample_throughput: 1868.677\n",
      "    sample_time_ms: 2247.579\n",
      "    update_time_ms: 3.058\n",
      "  timestamp: 1607961498\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 357000\n",
      "  training_iteration: 85\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         519.524</td><td style=\"text-align: right;\">357000</td><td style=\"text-align: right;\"> 101.819</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-58-24\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 238.0\n",
      "  episode_reward_mean: 100.44444444444444\n",
      "  episode_reward_min: -156.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 12453\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.513475775718689\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016772331669926643\n",
      "        model: {}\n",
      "        policy_loss: -0.09601166844367981\n",
      "        total_loss: 350.76416015625\n",
      "        vf_explained_var: 0.49959760904312134\n",
      "        vf_loss: 350.82196044921875\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8523032665252686\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01258938293904066\n",
      "        model: {}\n",
      "        policy_loss: -0.09506255388259888\n",
      "        total_loss: 359.3417053222656\n",
      "        vf_explained_var: 0.49211594462394714\n",
      "        vf_loss: 359.3937683105469\n",
      "    num_steps_sampled: 361200\n",
      "    num_steps_trained: 361200\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.54444444444444\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 119.0\n",
      "    agent-1: 119.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 50.22222222222222\n",
      "    agent-1: 50.22222222222222\n",
      "  policy_reward_min:\n",
      "    agent-0: -78.0\n",
      "    agent-1: -78.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13632719672985671\n",
      "    mean_inference_ms: 1.1335261919831625\n",
      "    mean_processing_ms: 0.2835726994280519\n",
      "  time_since_restore: 525.741090297699\n",
      "  time_this_iter_s: 6.21673583984375\n",
      "  time_total_s: 525.741090297699\n",
      "  timers:\n",
      "    learn_throughput: 1089.767\n",
      "    learn_time_ms: 3854.036\n",
      "    load_throughput: 1225304.259\n",
      "    load_time_ms: 3.428\n",
      "    sample_throughput: 1865.109\n",
      "    sample_time_ms: 2251.879\n",
      "    update_time_ms: 3.054\n",
      "  timestamp: 1607961504\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 361200\n",
      "  training_iteration: 86\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         525.741</td><td style=\"text-align: right;\">361200</td><td style=\"text-align: right;\"> 100.444</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-58-30\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 308.0\n",
      "  episode_reward_mean: 108.29931972789116\n",
      "  episode_reward_min: -174.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 12600\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5023906230926514\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015443977899849415\n",
      "        model: {}\n",
      "        policy_loss: -0.09317349642515182\n",
      "        total_loss: 364.2420654296875\n",
      "        vf_explained_var: 0.5381510257720947\n",
      "        vf_loss: 364.300048828125\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8402940034866333\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012433970347046852\n",
      "        model: {}\n",
      "        policy_loss: -0.09178362786769867\n",
      "        total_loss: 357.53955078125\n",
      "        vf_explained_var: 0.5358051061630249\n",
      "        vf_loss: 357.5888671875\n",
      "    num_steps_sampled: 365400\n",
      "    num_steps_trained: 365400\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.08888888888889\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 154.0\n",
      "    agent-1: 154.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 54.14965986394558\n",
      "    agent-1: 54.14965986394558\n",
      "  policy_reward_min:\n",
      "    agent-0: -87.0\n",
      "    agent-1: -87.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13630922650599134\n",
      "    mean_inference_ms: 1.1333386509807948\n",
      "    mean_processing_ms: 0.2835775588132493\n",
      "  time_since_restore: 531.8372695446014\n",
      "  time_this_iter_s: 6.096179246902466\n",
      "  time_total_s: 531.8372695446014\n",
      "  timers:\n",
      "    learn_throughput: 1087.385\n",
      "    learn_time_ms: 3862.476\n",
      "    load_throughput: 1253376.175\n",
      "    load_time_ms: 3.351\n",
      "    sample_throughput: 1865.204\n",
      "    sample_time_ms: 2251.765\n",
      "    update_time_ms: 3.128\n",
      "  timestamp: 1607961510\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 365400\n",
      "  training_iteration: 87\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         531.837</td><td style=\"text-align: right;\">365400</td><td style=\"text-align: right;\"> 108.299</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-58-37\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 262.0\n",
      "  episode_reward_mean: 101.61111111111111\n",
      "  episode_reward_min: -70.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 12744\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.4973517060279846\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01535695418715477\n",
      "        model: {}\n",
      "        policy_loss: -0.09234030544757843\n",
      "        total_loss: 342.1539306640625\n",
      "        vf_explained_var: 0.5161451697349548\n",
      "        vf_loss: 342.2112731933594\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8380731344223022\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012834908440709114\n",
      "        model: {}\n",
      "        policy_loss: -0.09884551912546158\n",
      "        total_loss: 353.8974304199219\n",
      "        vf_explained_var: 0.4869212508201599\n",
      "        vf_loss: 353.9524230957031\n",
      "    num_steps_sampled: 369600\n",
      "    num_steps_trained: 369600\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.61111111111111\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 131.0\n",
      "    agent-1: 131.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 50.80555555555556\n",
      "    agent-1: 50.80555555555556\n",
      "  policy_reward_min:\n",
      "    agent-0: -35.0\n",
      "    agent-1: -35.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13631260420239905\n",
      "    mean_inference_ms: 1.1334781236104874\n",
      "    mean_processing_ms: 0.28358425586013564\n",
      "  time_since_restore: 537.9434380531311\n",
      "  time_this_iter_s: 6.106168508529663\n",
      "  time_total_s: 537.9434380531311\n",
      "  timers:\n",
      "    learn_throughput: 1089.116\n",
      "    learn_time_ms: 3856.34\n",
      "    load_throughput: 1240691.11\n",
      "    load_time_ms: 3.385\n",
      "    sample_throughput: 1861.091\n",
      "    sample_time_ms: 2256.741\n",
      "    update_time_ms: 3.105\n",
      "  timestamp: 1607961517\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 369600\n",
      "  training_iteration: 88\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         537.943</td><td style=\"text-align: right;\">369600</td><td style=\"text-align: right;\"> 101.611</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-58-43\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 306.0\n",
      "  episode_reward_mean: 108.90277777777777\n",
      "  episode_reward_min: -100.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 12888\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.49312809109687805\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016006335616111755\n",
      "        model: {}\n",
      "        policy_loss: -0.09593620896339417\n",
      "        total_loss: 337.8396911621094\n",
      "        vf_explained_var: 0.5382876396179199\n",
      "        vf_loss: 337.899169921875\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8386071920394897\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013087254948914051\n",
      "        model: {}\n",
      "        policy_loss: -0.09974555671215057\n",
      "        total_loss: 349.493896484375\n",
      "        vf_explained_var: 0.5170024633407593\n",
      "        vf_loss: 349.54888916015625\n",
      "    num_steps_sampled: 373800\n",
      "    num_steps_trained: 373800\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.18888888888888\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 153.0\n",
      "    agent-1: 153.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 54.451388888888886\n",
      "    agent-1: 54.451388888888886\n",
      "  policy_reward_min:\n",
      "    agent-0: -50.0\n",
      "    agent-1: -50.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13632799393793246\n",
      "    mean_inference_ms: 1.1335806920950056\n",
      "    mean_processing_ms: 0.28358929907389385\n",
      "  time_since_restore: 544.0738565921783\n",
      "  time_this_iter_s: 6.130418539047241\n",
      "  time_total_s: 544.0738565921783\n",
      "  timers:\n",
      "    learn_throughput: 1089.018\n",
      "    learn_time_ms: 3856.687\n",
      "    load_throughput: 1246361.738\n",
      "    load_time_ms: 3.37\n",
      "    sample_throughput: 1857.393\n",
      "    sample_time_ms: 2261.234\n",
      "    update_time_ms: 3.115\n",
      "  timestamp: 1607961523\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 373800\n",
      "  training_iteration: 89\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         544.074</td><td style=\"text-align: right;\">373800</td><td style=\"text-align: right;\"> 108.903</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-58-49\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 232.0\n",
      "  episode_reward_mean: 109.63888888888889\n",
      "  episode_reward_min: -72.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 13032\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.49436721205711365\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015600384213030338\n",
      "        model: {}\n",
      "        policy_loss: -0.09268850088119507\n",
      "        total_loss: 331.7653503417969\n",
      "        vf_explained_var: 0.5391423106193542\n",
      "        vf_loss: 331.822509765625\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8419187664985657\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01233410369604826\n",
      "        model: {}\n",
      "        policy_loss: -0.09955645352602005\n",
      "        total_loss: 335.3961486816406\n",
      "        vf_explained_var: 0.52091383934021\n",
      "        vf_loss: 335.45355224609375\n",
      "    num_steps_sampled: 378000\n",
      "    num_steps_trained: 378000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.88888888888889\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 116.0\n",
      "    agent-1: 116.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 54.81944444444444\n",
      "    agent-1: 54.81944444444444\n",
      "  policy_reward_min:\n",
      "    agent-0: -36.0\n",
      "    agent-1: -36.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13632787447023162\n",
      "    mean_inference_ms: 1.1334499126385713\n",
      "    mean_processing_ms: 0.2835527854497177\n",
      "  time_since_restore: 550.1753072738647\n",
      "  time_this_iter_s: 6.101450681686401\n",
      "  time_total_s: 550.1753072738647\n",
      "  timers:\n",
      "    learn_throughput: 1088.93\n",
      "    learn_time_ms: 3856.996\n",
      "    load_throughput: 1244240.173\n",
      "    load_time_ms: 3.376\n",
      "    sample_throughput: 1856.146\n",
      "    sample_time_ms: 2262.753\n",
      "    update_time_ms: 3.094\n",
      "  timestamp: 1607961529\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 378000\n",
      "  training_iteration: 90\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         550.175</td><td style=\"text-align: right;\">378000</td><td style=\"text-align: right;\"> 109.639</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-58-55\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 280.0\n",
      "  episode_reward_mean: 120.28571428571429\n",
      "  episode_reward_min: -108.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 13179\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.49751681089401245\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014841175638139248\n",
      "        model: {}\n",
      "        policy_loss: -0.08987541496753693\n",
      "        total_loss: 362.3948059082031\n",
      "        vf_explained_var: 0.5687013268470764\n",
      "        vf_loss: 362.45086669921875\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8317027688026428\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013415465131402016\n",
      "        model: {}\n",
      "        policy_loss: -0.09516541659832001\n",
      "        total_loss: 363.9534606933594\n",
      "        vf_explained_var: 0.5510201454162598\n",
      "        vf_loss: 364.0028076171875\n",
      "    num_steps_sampled: 382200\n",
      "    num_steps_trained: 382200\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.78750000000001\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 140.0\n",
      "    agent-1: 140.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 60.142857142857146\n",
      "    agent-1: 60.142857142857146\n",
      "  policy_reward_min:\n",
      "    agent-0: -54.0\n",
      "    agent-1: -54.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13631888593163582\n",
      "    mean_inference_ms: 1.1332601344925108\n",
      "    mean_processing_ms: 0.28350764244672194\n",
      "  time_since_restore: 556.2859973907471\n",
      "  time_this_iter_s: 6.110690116882324\n",
      "  time_total_s: 556.2859973907471\n",
      "  timers:\n",
      "    learn_throughput: 1089.2\n",
      "    learn_time_ms: 3856.042\n",
      "    load_throughput: 1198388.876\n",
      "    load_time_ms: 3.505\n",
      "    sample_throughput: 1859.819\n",
      "    sample_time_ms: 2258.284\n",
      "    update_time_ms: 3.093\n",
      "  timestamp: 1607961535\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 382200\n",
      "  training_iteration: 91\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         556.286</td><td style=\"text-align: right;\">382200</td><td style=\"text-align: right;\"> 120.286</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-59-01\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 298.0\n",
      "  episode_reward_mean: 107.95833333333333\n",
      "  episode_reward_min: -152.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 13323\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5007704496383667\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015564337372779846\n",
      "        model: {}\n",
      "        policy_loss: -0.09259136021137238\n",
      "        total_loss: 380.6728210449219\n",
      "        vf_explained_var: 0.5363483428955078\n",
      "        vf_loss: 380.7299499511719\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8395875692367554\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0127696692943573\n",
      "        model: {}\n",
      "        policy_loss: -0.09615884721279144\n",
      "        total_loss: 364.53375244140625\n",
      "        vf_explained_var: 0.5378682613372803\n",
      "        vf_loss: 364.5863037109375\n",
      "    num_steps_sampled: 386400\n",
      "    num_steps_trained: 386400\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.45555555555556\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 149.0\n",
      "    agent-1: 149.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 53.979166666666664\n",
      "    agent-1: 53.979166666666664\n",
      "  policy_reward_min:\n",
      "    agent-0: -76.0\n",
      "    agent-1: -76.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13629162722150256\n",
      "    mean_inference_ms: 1.1329967293197933\n",
      "    mean_processing_ms: 0.28343708837011306\n",
      "  time_since_restore: 562.2838232517242\n",
      "  time_this_iter_s: 5.997825860977173\n",
      "  time_total_s: 562.2838232517242\n",
      "  timers:\n",
      "    learn_throughput: 1091.163\n",
      "    learn_time_ms: 3849.103\n",
      "    load_throughput: 1196248.62\n",
      "    load_time_ms: 3.511\n",
      "    sample_throughput: 1866.139\n",
      "    sample_time_ms: 2250.636\n",
      "    update_time_ms: 3.067\n",
      "  timestamp: 1607961541\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 386400\n",
      "  training_iteration: 92\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         562.284</td><td style=\"text-align: right;\">386400</td><td style=\"text-align: right;\"> 107.958</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-59-07\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 284.0\n",
      "  episode_reward_mean: 116.81944444444444\n",
      "  episode_reward_min: -52.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 13467\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.4977831244468689\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016716305166482925\n",
      "        model: {}\n",
      "        policy_loss: -0.0892954170703888\n",
      "        total_loss: 371.6092834472656\n",
      "        vf_explained_var: 0.5454676151275635\n",
      "        vf_loss: 371.66046142578125\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.823032557964325\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012377884238958359\n",
      "        model: {}\n",
      "        policy_loss: -0.09081535041332245\n",
      "        total_loss: 369.55560302734375\n",
      "        vf_explained_var: 0.5384255051612854\n",
      "        vf_loss: 369.6041259765625\n",
      "    num_steps_sampled: 390600\n",
      "    num_steps_trained: 390600\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.47777777777777\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 142.0\n",
      "    agent-1: 142.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 58.40972222222222\n",
      "    agent-1: 58.40972222222222\n",
      "  policy_reward_min:\n",
      "    agent-0: -26.0\n",
      "    agent-1: -26.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13626483141772733\n",
      "    mean_inference_ms: 1.1328724111622828\n",
      "    mean_processing_ms: 0.2833902885439805\n",
      "  time_since_restore: 568.3547677993774\n",
      "  time_this_iter_s: 6.070944547653198\n",
      "  time_total_s: 568.3547677993774\n",
      "  timers:\n",
      "    learn_throughput: 1093.556\n",
      "    learn_time_ms: 3840.681\n",
      "    load_throughput: 1189295.095\n",
      "    load_time_ms: 3.532\n",
      "    sample_throughput: 1866.904\n",
      "    sample_time_ms: 2249.713\n",
      "    update_time_ms: 3.062\n",
      "  timestamp: 1607961547\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 390600\n",
      "  training_iteration: 93\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         568.355</td><td style=\"text-align: right;\">390600</td><td style=\"text-align: right;\"> 116.819</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-59-13\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 322.0\n",
      "  episode_reward_mean: 116.70833333333333\n",
      "  episode_reward_min: -86.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 13611\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.4976077675819397\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015236737206578255\n",
      "        model: {}\n",
      "        policy_loss: -0.09225109964609146\n",
      "        total_loss: 343.8898010253906\n",
      "        vf_explained_var: 0.5543994903564453\n",
      "        vf_loss: 343.94732666015625\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8318509459495544\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012767288833856583\n",
      "        model: {}\n",
      "        policy_loss: -0.09715966880321503\n",
      "        total_loss: 343.8381042480469\n",
      "        vf_explained_var: 0.541077196598053\n",
      "        vf_loss: 343.8916320800781\n",
      "    num_steps_sampled: 394800\n",
      "    num_steps_trained: 394800\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.55000000000001\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 161.0\n",
      "    agent-1: 161.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 58.354166666666664\n",
      "    agent-1: 58.354166666666664\n",
      "  policy_reward_min:\n",
      "    agent-0: -43.0\n",
      "    agent-1: -43.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13626525792564112\n",
      "    mean_inference_ms: 1.1326859258908941\n",
      "    mean_processing_ms: 0.2833600322681174\n",
      "  time_since_restore: 574.4049050807953\n",
      "  time_this_iter_s: 6.050137281417847\n",
      "  time_total_s: 574.4049050807953\n",
      "  timers:\n",
      "    learn_throughput: 1096.044\n",
      "    learn_time_ms: 3831.964\n",
      "    load_throughput: 1262602.084\n",
      "    load_time_ms: 3.326\n",
      "    sample_throughput: 1871.943\n",
      "    sample_time_ms: 2243.658\n",
      "    update_time_ms: 3.051\n",
      "  timestamp: 1607961553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 394800\n",
      "  training_iteration: 94\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         574.405</td><td style=\"text-align: right;\">394800</td><td style=\"text-align: right;\"> 116.708</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-59-19\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 288.0\n",
      "  episode_reward_mean: 104.57142857142857\n",
      "  episode_reward_min: -76.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 13758\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.4948298931121826\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015367463231086731\n",
      "        model: {}\n",
      "        policy_loss: -0.09324313700199127\n",
      "        total_loss: 385.225830078125\n",
      "        vf_explained_var: 0.5111556649208069\n",
      "        vf_loss: 385.2840576171875\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8294680118560791\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012366796843707561\n",
      "        model: {}\n",
      "        policy_loss: -0.09307480603456497\n",
      "        total_loss: 382.246337890625\n",
      "        vf_explained_var: 0.5087248682975769\n",
      "        vf_loss: 382.29718017578125\n",
      "    num_steps_sampled: 399000\n",
      "    num_steps_trained: 399000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.15555555555557\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 144.0\n",
      "    agent-1: 144.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 52.285714285714285\n",
      "    agent-1: 52.285714285714285\n",
      "  policy_reward_min:\n",
      "    agent-0: -38.0\n",
      "    agent-1: -38.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13625583045457296\n",
      "    mean_inference_ms: 1.132495113431151\n",
      "    mean_processing_ms: 0.28334906265172705\n",
      "  time_since_restore: 580.4409425258636\n",
      "  time_this_iter_s: 6.036037445068359\n",
      "  time_total_s: 580.4409425258636\n",
      "  timers:\n",
      "    learn_throughput: 1096.463\n",
      "    learn_time_ms: 3830.499\n",
      "    load_throughput: 1250528.988\n",
      "    load_time_ms: 3.359\n",
      "    sample_throughput: 1872.685\n",
      "    sample_time_ms: 2242.769\n",
      "    update_time_ms: 3.032\n",
      "  timestamp: 1607961559\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 399000\n",
      "  training_iteration: 95\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         580.441</td><td style=\"text-align: right;\">399000</td><td style=\"text-align: right;\"> 104.571</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-59-26\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 262.0\n",
      "  episode_reward_mean: 107.27777777777777\n",
      "  episode_reward_min: -124.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 13902\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.49655503034591675\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015360120683908463\n",
      "        model: {}\n",
      "        policy_loss: -0.09434573352336884\n",
      "        total_loss: 364.1514587402344\n",
      "        vf_explained_var: 0.528185248374939\n",
      "        vf_loss: 364.2108154296875\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8309159278869629\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0125370342284441\n",
      "        model: {}\n",
      "        policy_loss: -0.09458383917808533\n",
      "        total_loss: 351.96875\n",
      "        vf_explained_var: 0.5317700505256653\n",
      "        vf_loss: 352.02044677734375\n",
      "    num_steps_sampled: 403200\n",
      "    num_steps_trained: 403200\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.0\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 131.0\n",
      "    agent-1: 131.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 53.638888888888886\n",
      "    agent-1: 53.638888888888886\n",
      "  policy_reward_min:\n",
      "    agent-0: -62.0\n",
      "    agent-1: -62.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1362508946745774\n",
      "    mean_inference_ms: 1.1323351128747778\n",
      "    mean_processing_ms: 0.2833216962525937\n",
      "  time_since_restore: 586.4717419147491\n",
      "  time_this_iter_s: 6.030799388885498\n",
      "  time_total_s: 586.4717419147491\n",
      "  timers:\n",
      "    learn_throughput: 1101.068\n",
      "    learn_time_ms: 3814.478\n",
      "    load_throughput: 1250733.198\n",
      "    load_time_ms: 3.358\n",
      "    sample_throughput: 1874.688\n",
      "    sample_time_ms: 2240.373\n",
      "    update_time_ms: 3.01\n",
      "  timestamp: 1607961566\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 403200\n",
      "  training_iteration: 96\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         586.472</td><td style=\"text-align: right;\">403200</td><td style=\"text-align: right;\"> 107.278</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-59-32\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 268.0\n",
      "  episode_reward_mean: 109.06944444444444\n",
      "  episode_reward_min: -62.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 14046\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.49773263931274414\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015846826136112213\n",
      "        model: {}\n",
      "        policy_loss: -0.09420359134674072\n",
      "        total_loss: 306.1822509765625\n",
      "        vf_explained_var: 0.5459572076797485\n",
      "        vf_loss: 306.2403869628906\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.827944278717041\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012307969853281975\n",
      "        model: {}\n",
      "        policy_loss: -0.0960918664932251\n",
      "        total_loss: 314.0053405761719\n",
      "        vf_explained_var: 0.5295212268829346\n",
      "        vf_loss: 314.05938720703125\n",
      "    num_steps_sampled: 407400\n",
      "    num_steps_trained: 407400\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.73333333333333\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 134.0\n",
      "    agent-1: 134.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 54.53472222222222\n",
      "    agent-1: 54.53472222222222\n",
      "  policy_reward_min:\n",
      "    agent-0: -31.0\n",
      "    agent-1: -31.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1362589863187996\n",
      "    mean_inference_ms: 1.1321007098624871\n",
      "    mean_processing_ms: 0.2833021098129428\n",
      "  time_since_restore: 592.6142721176147\n",
      "  time_this_iter_s: 6.142530202865601\n",
      "  time_total_s: 592.6142721176147\n",
      "  timers:\n",
      "    learn_throughput: 1099.73\n",
      "    learn_time_ms: 3819.121\n",
      "    load_throughput: 1230499.49\n",
      "    load_time_ms: 3.413\n",
      "    sample_throughput: 1874.559\n",
      "    sample_time_ms: 2240.526\n",
      "    update_time_ms: 2.986\n",
      "  timestamp: 1607961572\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 407400\n",
      "  training_iteration: 97\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         592.614</td><td style=\"text-align: right;\">407400</td><td style=\"text-align: right;\"> 109.069</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-59-38\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 268.0\n",
      "  episode_reward_mean: 109.74149659863946\n",
      "  episode_reward_min: -174.0\n",
      "  episodes_this_iter: 147\n",
      "  episodes_total: 14193\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5001276731491089\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015698552131652832\n",
      "        model: {}\n",
      "        policy_loss: -0.09040378034114838\n",
      "        total_loss: 394.39691162109375\n",
      "        vf_explained_var: 0.5500261783599854\n",
      "        vf_loss: 394.4515686035156\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8136769533157349\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011880487203598022\n",
      "        model: {}\n",
      "        policy_loss: -0.09544976055622101\n",
      "        total_loss: 392.23675537109375\n",
      "        vf_explained_var: 0.5424336194992065\n",
      "        vf_loss: 392.29156494140625\n",
      "    num_steps_sampled: 411600\n",
      "    num_steps_trained: 411600\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.25555555555556\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 134.0\n",
      "    agent-1: 134.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 54.87074829931973\n",
      "    agent-1: 54.87074829931973\n",
      "  policy_reward_min:\n",
      "    agent-0: -87.0\n",
      "    agent-1: -87.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1362608562530864\n",
      "    mean_inference_ms: 1.1318989933142825\n",
      "    mean_processing_ms: 0.28330539081094236\n",
      "  time_since_restore: 598.828332901001\n",
      "  time_this_iter_s: 6.2140607833862305\n",
      "  time_total_s: 598.828332901001\n",
      "  timers:\n",
      "    learn_throughput: 1097.878\n",
      "    learn_time_ms: 3825.563\n",
      "    load_throughput: 1247094.077\n",
      "    load_time_ms: 3.368\n",
      "    sample_throughput: 1870.773\n",
      "    sample_time_ms: 2245.062\n",
      "    update_time_ms: 2.994\n",
      "  timestamp: 1607961578\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 411600\n",
      "  training_iteration: 98\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         598.828</td><td style=\"text-align: right;\">411600</td><td style=\"text-align: right;\"> 109.741</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-59-44\n",
      "  done: false\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 272.0\n",
      "  episode_reward_mean: 107.59722222222223\n",
      "  episode_reward_min: -124.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 14337\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.4878270626068115\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015068535692989826\n",
      "        model: {}\n",
      "        policy_loss: -0.08671523630619049\n",
      "        total_loss: 384.852294921875\n",
      "        vf_explained_var: 0.5421512126922607\n",
      "        vf_loss: 384.9046936035156\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8122903108596802\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01227217260748148\n",
      "        model: {}\n",
      "        policy_loss: -0.08933187276124954\n",
      "        total_loss: 399.6097412109375\n",
      "        vf_explained_var: 0.5172815322875977\n",
      "        vf_loss: 399.65716552734375\n",
      "    num_steps_sampled: 415800\n",
      "    num_steps_trained: 415800\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.875\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 136.0\n",
      "    agent-1: 136.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 53.798611111111114\n",
      "    agent-1: 53.798611111111114\n",
      "  policy_reward_min:\n",
      "    agent-0: -62.0\n",
      "    agent-1: -62.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13625590689077585\n",
      "    mean_inference_ms: 1.131873653010389\n",
      "    mean_processing_ms: 0.2832933687512785\n",
      "  time_since_restore: 604.9054446220398\n",
      "  time_this_iter_s: 6.077111721038818\n",
      "  time_total_s: 604.9054446220398\n",
      "  timers:\n",
      "    learn_throughput: 1098.844\n",
      "    learn_time_ms: 3822.2\n",
      "    load_throughput: 1248455.157\n",
      "    load_time_ms: 3.364\n",
      "    sample_throughput: 1872.342\n",
      "    sample_time_ms: 2243.18\n",
      "    update_time_ms: 2.999\n",
      "  timestamp: 1607961584\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 415800\n",
      "  training_iteration: 99\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         604.905</td><td style=\"text-align: right;\">415800</td><td style=\"text-align: right;\"> 107.597</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SyncSched_f11d9_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-14_15-59-50\n",
      "  done: true\n",
      "  episode_len_mean: 29.0\n",
      "  episode_reward_max: 308.0\n",
      "  episode_reward_mean: 113.23611111111111\n",
      "  episode_reward_min: -92.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 14481\n",
      "  experiment_id: 36c0c02ef718476eb7a1bfb3ed10989f\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.49306729435920715\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015900682657957077\n",
      "        model: {}\n",
      "        policy_loss: -0.08796179294586182\n",
      "        total_loss: 334.07666015625\n",
      "        vf_explained_var: 0.5522558689117432\n",
      "        vf_loss: 334.1283874511719\n",
      "      agent-1:\n",
      "        cur_kl_coeff: 3.417187452316284\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.808088481426239\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012648173607885838\n",
      "        model: {}\n",
      "        policy_loss: -0.09487334638834\n",
      "        total_loss: 316.0809326171875\n",
      "        vf_explained_var: 0.5640047192573547\n",
      "        vf_loss: 316.13262939453125\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.67777777777778\n",
      "    ram_util_percent: 5.2\n",
      "  pid: 2283\n",
      "  policy_reward_max:\n",
      "    agent-0: 154.0\n",
      "    agent-1: 154.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 56.61805555555556\n",
      "    agent-1: 56.61805555555556\n",
      "  policy_reward_min:\n",
      "    agent-0: -46.0\n",
      "    agent-1: -46.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.13624315753683486\n",
      "    mean_inference_ms: 1.131790033854531\n",
      "    mean_processing_ms: 0.2832351520385099\n",
      "  time_since_restore: 611.0274229049683\n",
      "  time_this_iter_s: 6.121978282928467\n",
      "  time_total_s: 611.0274229049683\n",
      "  timers:\n",
      "    learn_throughput: 1098.043\n",
      "    learn_time_ms: 3824.986\n",
      "    load_throughput: 1257007.257\n",
      "    load_time_ms: 3.341\n",
      "    sample_throughput: 1873.004\n",
      "    sample_time_ms: 2242.387\n",
      "    update_time_ms: 2.989\n",
      "  timestamp: 1607961590\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 100\n",
      "  trial_id: f11d9_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>RUNNING </td><td>172.16.2.4:2283</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         611.027</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\"> 113.236</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.9/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/32.37 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/MultiStageSchedSync<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SyncSched_f11d9_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         611.027</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\"> 113.236</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config={\n",
    "    \"log_level\": \"WARN\",\n",
    "    \"num_workers\": 3,\n",
    "    \"num_cpus_for_driver\": 1,\n",
    "    \"num_cpus_per_worker\": 1,\n",
    "    \"lr\": 5e-3,\n",
    "    \"model\":{\"fcnet_hiddens\": [128, 128]},\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policy_graphs,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "    },\n",
    "    \"env\": env_name\n",
    "}\n",
    "\n",
    "exp_name = 'MultiStageSchedSync'\n",
    "\n",
    "exp_dict = {\n",
    "        'name': exp_name,\n",
    "        'run_or_experiment': 'PPO',\n",
    "        \"stop\": {\n",
    "            \"training_iteration\": 100\n",
    "        },\n",
    "        'checkpoint_freq': 20,\n",
    "        \"config\": config,\n",
    "}\n",
    "ray.init(ignore_reinit_error=True)\n",
    "results = tune.run(**exp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>num_healthy_workers</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>done</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>...</th>\n",
       "      <th>info/learner/agent-1/entropy_coeff</th>\n",
       "      <th>config/env</th>\n",
       "      <th>config/log_level</th>\n",
       "      <th>config/lr</th>\n",
       "      <th>config/model</th>\n",
       "      <th>config/multiagent</th>\n",
       "      <th>config/num_cpus_for_driver</th>\n",
       "      <th>config/num_cpus_per_worker</th>\n",
       "      <th>config/num_workers</th>\n",
       "      <th>logdir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>308.0</td>\n",
       "      <td>-92.0</td>\n",
       "      <td>113.236111</td>\n",
       "      <td>29.0</td>\n",
       "      <td>144</td>\n",
       "      <td>3</td>\n",
       "      <td>420000</td>\n",
       "      <td>True</td>\n",
       "      <td>14481</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SyncSched</td>\n",
       "      <td>WARN</td>\n",
       "      <td>0.005</td>\n",
       "      <td>{'fcnet_hiddens': [128, 128]}</td>\n",
       "      <td>{'policies': {'agent-0': [None, 'Box(13,)', 'D...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>/home/cdhubbs/ray_results/MultiStageSchedSync/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0               308.0               -92.0           113.236111   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  num_healthy_workers  timesteps_total  \\\n",
       "0              29.0                 144                    3           420000   \n",
       "\n",
       "   done  episodes_total  training_iteration  ...  \\\n",
       "0  True           14481                 100  ...   \n",
       "\n",
       "  info/learner/agent-1/entropy_coeff config/env  config/log_level  config/lr  \\\n",
       "0                                0.0  SyncSched              WARN      0.005   \n",
       "\n",
       "                    config/model  \\\n",
       "0  {'fcnet_hiddens': [128, 128]}   \n",
       "\n",
       "                                   config/multiagent  \\\n",
       "0  {'policies': {'agent-0': [None, 'Box(13,)', 'D...   \n",
       "\n",
       "  config/num_cpus_for_driver config/num_cpus_per_worker  config/num_workers  \\\n",
       "0                          1                          1                   3   \n",
       "\n",
       "                                              logdir  \n",
       "0  /home/cdhubbs/ray_results/MultiStageSchedSync/...  \n",
       "\n",
       "[1 rows x 74 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = results.dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cdhubbs/ray_results/MultiStageSchedSync/PPO_SyncSched_0_2020-12-14_15-49-25q5fqh5c0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = df['logdir'][0]\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_len_mean</th>\n",
       "      <th>episodes_this_iter</th>\n",
       "      <th>num_healthy_workers</th>\n",
       "      <th>timesteps_total</th>\n",
       "      <th>done</th>\n",
       "      <th>episodes_total</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>...</th>\n",
       "      <th>info/learner/agent-0/entropy_coeff</th>\n",
       "      <th>info/learner/agent-1/cur_kl_coeff</th>\n",
       "      <th>info/learner/agent-1/cur_lr</th>\n",
       "      <th>info/learner/agent-1/total_loss</th>\n",
       "      <th>info/learner/agent-1/policy_loss</th>\n",
       "      <th>info/learner/agent-1/vf_loss</th>\n",
       "      <th>info/learner/agent-1/vf_explained_var</th>\n",
       "      <th>info/learner/agent-1/kl</th>\n",
       "      <th>info/learner/agent-1/entropy</th>\n",
       "      <th>info/learner/agent-1/entropy_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120.0</td>\n",
       "      <td>-384.0</td>\n",
       "      <td>-103.388889</td>\n",
       "      <td>29.0</td>\n",
       "      <td>144</td>\n",
       "      <td>3</td>\n",
       "      <td>4200</td>\n",
       "      <td>False</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2431.98200</td>\n",
       "      <td>-0.121600</td>\n",
       "      <td>2432.0903</td>\n",
       "      <td>-5.401671e-08</td>\n",
       "      <td>0.064609</td>\n",
       "      <td>1.324227</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>146.0</td>\n",
       "      <td>-384.0</td>\n",
       "      <td>-77.500000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>144</td>\n",
       "      <td>3</td>\n",
       "      <td>8400</td>\n",
       "      <td>False</td>\n",
       "      <td>288</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1699.42040</td>\n",
       "      <td>-0.177049</td>\n",
       "      <td>1699.5771</td>\n",
       "      <td>-6.891787e-08</td>\n",
       "      <td>0.068268</td>\n",
       "      <td>1.266015</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146.0</td>\n",
       "      <td>-274.0</td>\n",
       "      <td>-60.555556</td>\n",
       "      <td>29.0</td>\n",
       "      <td>144</td>\n",
       "      <td>3</td>\n",
       "      <td>12600</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1211.03170</td>\n",
       "      <td>-0.193702</td>\n",
       "      <td>1211.1957</td>\n",
       "      <td>2.104789e-07</td>\n",
       "      <td>0.066146</td>\n",
       "      <td>1.213030</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138.0</td>\n",
       "      <td>-324.0</td>\n",
       "      <td>-67.537415</td>\n",
       "      <td>29.0</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "      <td>16800</td>\n",
       "      <td>False</td>\n",
       "      <td>579</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6750</td>\n",
       "      <td>0.005</td>\n",
       "      <td>937.58680</td>\n",
       "      <td>-0.184393</td>\n",
       "      <td>937.7363</td>\n",
       "      <td>1.649917e-01</td>\n",
       "      <td>0.051693</td>\n",
       "      <td>1.189658</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>144.0</td>\n",
       "      <td>-282.0</td>\n",
       "      <td>-45.486111</td>\n",
       "      <td>29.0</td>\n",
       "      <td>144</td>\n",
       "      <td>3</td>\n",
       "      <td>21000</td>\n",
       "      <td>False</td>\n",
       "      <td>723</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0125</td>\n",
       "      <td>0.005</td>\n",
       "      <td>749.57635</td>\n",
       "      <td>-0.167860</td>\n",
       "      <td>749.7023</td>\n",
       "      <td>2.915760e-01</td>\n",
       "      <td>0.041357</td>\n",
       "      <td>1.153130</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
       "0               120.0              -384.0          -103.388889   \n",
       "1               146.0              -384.0           -77.500000   \n",
       "2               146.0              -274.0           -60.555556   \n",
       "3               138.0              -324.0           -67.537415   \n",
       "4               144.0              -282.0           -45.486111   \n",
       "\n",
       "   episode_len_mean  episodes_this_iter  num_healthy_workers  timesteps_total  \\\n",
       "0              29.0                 144                    3             4200   \n",
       "1              29.0                 144                    3             8400   \n",
       "2              29.0                 144                    3            12600   \n",
       "3              29.0                 147                    3            16800   \n",
       "4              29.0                 144                    3            21000   \n",
       "\n",
       "    done  episodes_total  training_iteration  ...  \\\n",
       "0  False             144                   1  ...   \n",
       "1  False             288                   2  ...   \n",
       "2  False             432                   3  ...   \n",
       "3  False             579                   4  ...   \n",
       "4  False             723                   5  ...   \n",
       "\n",
       "  info/learner/agent-0/entropy_coeff info/learner/agent-1/cur_kl_coeff  \\\n",
       "0                                0.0                            0.2000   \n",
       "1                                0.0                            0.3000   \n",
       "2                                0.0                            0.4500   \n",
       "3                                0.0                            0.6750   \n",
       "4                                0.0                            1.0125   \n",
       "\n",
       "   info/learner/agent-1/cur_lr  info/learner/agent-1/total_loss  \\\n",
       "0                        0.005                       2431.98200   \n",
       "1                        0.005                       1699.42040   \n",
       "2                        0.005                       1211.03170   \n",
       "3                        0.005                        937.58680   \n",
       "4                        0.005                        749.57635   \n",
       "\n",
       "   info/learner/agent-1/policy_loss  info/learner/agent-1/vf_loss  \\\n",
       "0                         -0.121600                     2432.0903   \n",
       "1                         -0.177049                     1699.5771   \n",
       "2                         -0.193702                     1211.1957   \n",
       "3                         -0.184393                      937.7363   \n",
       "4                         -0.167860                      749.7023   \n",
       "\n",
       "  info/learner/agent-1/vf_explained_var info/learner/agent-1/kl  \\\n",
       "0                         -5.401671e-08                0.064609   \n",
       "1                         -6.891787e-08                0.068268   \n",
       "2                          2.104789e-07                0.066146   \n",
       "3                          1.649917e-01                0.051693   \n",
       "4                          2.915760e-01                0.041357   \n",
       "\n",
       "   info/learner/agent-1/entropy  info/learner/agent-1/entropy_coeff  \n",
       "0                      1.324227                                 0.0  \n",
       "1                      1.266015                                 0.0  \n",
       "2                      1.213030                                 0.0  \n",
       "3                      1.189658                                 0.0  \n",
       "4                      1.153130                                 0.0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(path + '/progress.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.plot(y='episode_reward_mean', kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphadow",
   "language": "python",
   "name": "alphadow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
