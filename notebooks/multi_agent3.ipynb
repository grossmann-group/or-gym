{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent AlphaDow\n",
    "\n",
    "This is a notebook to integrate AlphaDow with a multi-agent framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from alphaDow2.envs.facility import MultiStageGenEnv\n",
    "from alphaDow2 import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentSyncEnv(MultiStageGenEnv, MultiAgentEnv):\n",
    "    '''\n",
    "    This is a child class of the MultiStageGenEnv class and inherits Ray's\n",
    "    MultiAgentEnv class to implement a synchronous multi-agent approach for\n",
    "    the multi-stage scheduling problem. \n",
    "    '''\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if 'env_config' in kwargs:\n",
    "            self.env_config = kwargs['env_config']\n",
    "        else:\n",
    "            self.env_config = {}\n",
    "        super().__init__(env_config=self.env_config)\n",
    "        \n",
    "        self.agent_ids = ['_'.join([stage, train]) \n",
    "            for stage, train, _ in self.stage_train_list]\n",
    "        self.num_agents = len(self.agent_ids)\n",
    "        \n",
    "        self.state = self.reset()\n",
    "        \n",
    "    def step(self, action):\n",
    "        state, reward, done, info = self._Step(action)\n",
    "        states = {i: state for i in self.agent_ids}\n",
    "        rewards = {i: reward for i in self.agent_ids}\n",
    "        dones = {i: done for i in self.agent_ids}\n",
    "        dones['__all__'] = done\n",
    "        infos = {i: info for i in self.agent_ids}\n",
    "        \n",
    "        return states, rewards, dones, infos\n",
    "    \n",
    "    def reset(self):\n",
    "        state = self._Reset()\n",
    "        return {i: state for i in self.agent_ids}\n",
    "    \n",
    "env = MultiAgentSyncEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test env\n",
    "env.reset()\n",
    "while True:\n",
    "    actions = env.action_space.sample()\n",
    "    s, r, d, _ = env.step(actions)\n",
    "    if d['__all__']:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_gen(agent_id):\n",
    "    return (None, env.observation_space, env.action_space[agent_id], {})\n",
    "\n",
    "policy_graphs = {f'agent-{i}': policy_gen(i) for i in env.agent_ids}\n",
    "\n",
    "def policy_mapping_fn(agent_id):\n",
    "    return f'agent-{agent_id}'\n",
    "\n",
    "def env_creator(*args, **kwargs):\n",
    "    return MultiAgentSyncEnv()\n",
    "\n",
    "env_name = \"ADSyncSched\"\n",
    "tune.register_env(env_name, env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-15 09:25:33,802\tINFO resource_spec.py:212 -- Starting Ray with 32.28 GiB memory available for workers and up to 16.16 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-12-15 09:25:34,053\tWARNING services.py:923 -- Redis failed to start, retrying now.\n",
      "2020-12-15 09:25:34,302\tINFO services.py:1165 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 2.2/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=29234)\u001b[0m 2020-12-15 09:25:37,801\tINFO trainer.py:585 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=29234)\u001b[0m 2020-12-15 09:25:37,801\tINFO trainer.py:612 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=29234)\u001b[0m 2020-12-15 09:25:48,177\tINFO trainable.py:181 -- _setup took 10.848 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=29234)\u001b[0m 2020-12-15 09:25:48,178\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-26-02\n",
      "  done: false\n",
      "  episode_len_mean: 772.0\n",
      "  episode_reward_max: 283605.0\n",
      "  episode_reward_mean: 268735.0\n",
      "  episode_reward_min: 247905.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 3\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.3611476421356201\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0259897131472826\n",
      "        model: {}\n",
      "        policy_loss: 0.008747939020395279\n",
      "        total_loss: 56117120.0\n",
      "        vf_explained_var: -2.2351741790771484e-08\n",
      "        vf_loss: 56117116.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.360254168510437\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.027363497763872147\n",
      "        model: {}\n",
      "        policy_loss: 0.004874399397522211\n",
      "        total_loss: 56083304.0\n",
      "        vf_explained_var: 7.227063179016113e-07\n",
      "        vf_loss: 56083304.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.3583917617797852\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.028146374970674515\n",
      "        model: {}\n",
      "        policy_loss: 0.011416349560022354\n",
      "        total_loss: 55984440.0\n",
      "        vf_explained_var: -9.313225746154785e-09\n",
      "        vf_loss: 55984440.0\n",
      "    num_steps_sampled: 4200\n",
      "    num_steps_trained: 4200\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.60952380952382\n",
      "    ram_util_percent: 6.038095238095237\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 94535.0\n",
      "    agent-stage0_train1: 94535.0\n",
      "    agent-stage1_train0: 94535.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 89578.33333333333\n",
      "    agent-stage0_train1: 89578.33333333333\n",
      "    agent-stage1_train0: 89578.33333333333\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: 82635.0\n",
      "    agent-stage0_train1: 82635.0\n",
      "    agent-stage1_train0: 82635.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.2463112707907267\n",
      "    mean_inference_ms: 1.5888888581208096\n",
      "    mean_processing_ms: 0.35626951240795496\n",
      "  time_since_restore: 14.210137367248535\n",
      "  time_this_iter_s: 14.210137367248535\n",
      "  time_total_s: 14.210137367248535\n",
      "  timers:\n",
      "    learn_throughput: 576.59\n",
      "    learn_time_ms: 7284.2\n",
      "    load_throughput: 16526.625\n",
      "    load_time_ms: 254.135\n",
      "    sample_throughput: 690.84\n",
      "    sample_time_ms: 6079.559\n",
      "    update_time_ms: 3.981\n",
      "  timestamp: 1608024362\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4200\n",
      "  training_iteration: 1\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.3/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         14.2101</td><td style=\"text-align: right;\">4200</td><td style=\"text-align: right;\">  268735</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-26-15\n",
      "  done: false\n",
      "  episode_len_mean: 716.2\n",
      "  episode_reward_max: 321942.0\n",
      "  episode_reward_mean: 287402.7\n",
      "  episode_reward_min: 247905.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 10\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.3299652338027954\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02793634682893753\n",
      "        model: {}\n",
      "        policy_loss: 0.004365582950413227\n",
      "        total_loss: 81841824.0\n",
      "        vf_explained_var: 9.313225746154785e-09\n",
      "        vf_loss: 81841824.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.338819980621338\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020550210028886795\n",
      "        model: {}\n",
      "        policy_loss: -0.0001912123989313841\n",
      "        total_loss: 81939824.0\n",
      "        vf_explained_var: 1.695007085800171e-07\n",
      "        vf_loss: 81939824.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.2974299192428589\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020711716264486313\n",
      "        model: {}\n",
      "        policy_loss: -0.0033148322254419327\n",
      "        total_loss: 81829376.0\n",
      "        vf_explained_var: 1.1175870895385742e-08\n",
      "        vf_loss: 81829376.0\n",
      "    num_steps_sampled: 8400\n",
      "    num_steps_trained: 8400\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.65555555555554\n",
      "    ram_util_percent: 6.099999999999999\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 107314.0\n",
      "    agent-stage0_train1: 107314.0\n",
      "    agent-stage1_train0: 107314.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 95800.9\n",
      "    agent-stage0_train1: 95800.9\n",
      "    agent-stage1_train0: 95800.9\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: 82635.0\n",
      "    agent-stage0_train1: 82635.0\n",
      "    agent-stage1_train0: 82635.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.2944873408644346\n",
      "    mean_inference_ms: 1.5604238189345256\n",
      "    mean_processing_ms: 0.3590929307504969\n",
      "  time_since_restore: 27.06960153579712\n",
      "  time_this_iter_s: 12.859464168548584\n",
      "  time_total_s: 27.06960153579712\n",
      "  timers:\n",
      "    learn_throughput: 600.45\n",
      "    learn_time_ms: 6994.75\n",
      "    load_throughput: 31562.535\n",
      "    load_time_ms: 133.069\n",
      "    sample_throughput: 688.101\n",
      "    sample_time_ms: 6103.755\n",
      "    update_time_ms: 4.064\n",
      "  timestamp: 1608024375\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8400\n",
      "  training_iteration: 2\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         27.0696</td><td style=\"text-align: right;\">8400</td><td style=\"text-align: right;\">  287403</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-26-27\n",
      "  done: false\n",
      "  episode_len_mean: 703.5\n",
      "  episode_reward_max: 321942.0\n",
      "  episode_reward_mean: 285660.1875\n",
      "  episode_reward_min: 247905.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 16\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.343700647354126\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.025928158313035965\n",
      "        model: {}\n",
      "        policy_loss: 0.0005663437768816948\n",
      "        total_loss: 70989056.0\n",
      "        vf_explained_var: -2.60770320892334e-08\n",
      "        vf_loss: 70989056.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.3011822700500488\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.027135558426380157\n",
      "        model: {}\n",
      "        policy_loss: -0.0020003095269203186\n",
      "        total_loss: 71223744.0\n",
      "        vf_explained_var: -2.421438694000244e-08\n",
      "        vf_loss: 71223744.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1757149696350098\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.027187153697013855\n",
      "        model: {}\n",
      "        policy_loss: 0.010908231139183044\n",
      "        total_loss: 70522944.0\n",
      "        vf_explained_var: 3.725290298461914e-09\n",
      "        vf_loss: 70522944.0\n",
      "    num_steps_sampled: 12600\n",
      "    num_steps_trained: 12600\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.8722222222222\n",
      "    ram_util_percent: 6.099999999999999\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 107314.0\n",
      "    agent-stage0_train1: 107314.0\n",
      "    agent-stage1_train0: 107314.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 95220.0625\n",
      "    agent-stage0_train1: 95220.0625\n",
      "    agent-stage1_train0: 95220.0625\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: 82635.0\n",
      "    agent-stage0_train1: 82635.0\n",
      "    agent-stage1_train0: 82635.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.306614183420717\n",
      "    mean_inference_ms: 1.5507479236812283\n",
      "    mean_processing_ms: 0.3593415267123028\n",
      "  time_since_restore: 39.628145694732666\n",
      "  time_this_iter_s: 12.558544158935547\n",
      "  time_total_s: 39.628145694732666\n",
      "  timers:\n",
      "    learn_throughput: 615.479\n",
      "    learn_time_ms: 6823.95\n",
      "    load_throughput: 45217.582\n",
      "    load_time_ms: 92.884\n",
      "    sample_throughput: 690.19\n",
      "    sample_time_ms: 6085.279\n",
      "    update_time_ms: 4.071\n",
      "  timestamp: 1608024387\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12600\n",
      "  training_iteration: 3\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         39.6281</td><td style=\"text-align: right;\">12600</td><td style=\"text-align: right;\">  285660</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-26-41\n",
      "  done: false\n",
      "  episode_len_mean: 711.6666666666666\n",
      "  episode_reward_max: 321942.0\n",
      "  episode_reward_mean: 285839.14285714284\n",
      "  episode_reward_min: 247905.0\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 21\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.3647277355194092\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015276411548256874\n",
      "        model: {}\n",
      "        policy_loss: -0.003055206034332514\n",
      "        total_loss: 55613568.0\n",
      "        vf_explained_var: -5.587935447692871e-09\n",
      "        vf_loss: 55613568.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.2722229957580566\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.024950552731752396\n",
      "        model: {}\n",
      "        policy_loss: 0.0015107751823961735\n",
      "        total_loss: 55782496.0\n",
      "        vf_explained_var: 1.862645149230957e-09\n",
      "        vf_loss: 55782496.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0697777271270752\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01708054542541504\n",
      "        model: {}\n",
      "        policy_loss: 7.758475840091705e-05\n",
      "        total_loss: 55559176.0\n",
      "        vf_explained_var: -1.30385160446167e-08\n",
      "        vf_loss: 55559176.0\n",
      "    num_steps_sampled: 16800\n",
      "    num_steps_trained: 16800\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.07368421052632\n",
      "    ram_util_percent: 6.210526315789474\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 107314.0\n",
      "    agent-stage0_train1: 107314.0\n",
      "    agent-stage1_train0: 107314.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 95279.71428571429\n",
      "    agent-stage0_train1: 95279.71428571429\n",
      "    agent-stage1_train0: 95279.71428571429\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: 82635.0\n",
      "    agent-stage0_train1: 82635.0\n",
      "    agent-stage1_train0: 82635.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.30629547126127\n",
      "    mean_inference_ms: 1.54605870128409\n",
      "    mean_processing_ms: 0.35921980734367953\n",
      "  time_since_restore: 53.06192922592163\n",
      "  time_this_iter_s: 13.433783531188965\n",
      "  time_total_s: 53.06192922592163\n",
      "  timers:\n",
      "    learn_throughput: 602.128\n",
      "    learn_time_ms: 6975.262\n",
      "    load_throughput: 58005.34\n",
      "    load_time_ms: 72.407\n",
      "    sample_throughput: 693.298\n",
      "    sample_time_ms: 6058.002\n",
      "    update_time_ms: 4.072\n",
      "  timestamp: 1608024401\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16800\n",
      "  training_iteration: 4\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         53.0619</td><td style=\"text-align: right;\">16800</td><td style=\"text-align: right;\">  285839</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-26-54\n",
      "  done: false\n",
      "  episode_len_mean: 719.8518518518518\n",
      "  episode_reward_max: 359739.0\n",
      "  episode_reward_mean: 292720.44444444444\n",
      "  episode_reward_min: 247905.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 27\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.3495979309082031\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022386644035577774\n",
      "        model: {}\n",
      "        policy_loss: 0.00010419241152703762\n",
      "        total_loss: 92081536.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 92081536.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.2454273700714111\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006880201864987612\n",
      "        model: {}\n",
      "        policy_loss: -1.0174000635743141e-05\n",
      "        total_loss: 92293784.0\n",
      "        vf_explained_var: -1.30385160446167e-08\n",
      "        vf_loss: 92293784.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.042198657989502\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01096670888364315\n",
      "        model: {}\n",
      "        policy_loss: -0.00325492350384593\n",
      "        total_loss: 92175424.0\n",
      "        vf_explained_var: -7.450580596923828e-09\n",
      "        vf_loss: 92175424.0\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.72631578947369\n",
      "    ram_util_percent: 6.15263157894737\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 119913.0\n",
      "    agent-stage0_train1: 119913.0\n",
      "    agent-stage1_train0: 119913.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 97573.48148148147\n",
      "    agent-stage0_train1: 97573.48148148147\n",
      "    agent-stage1_train0: 97573.48148148147\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: 82635.0\n",
      "    agent-stage0_train1: 82635.0\n",
      "    agent-stage1_train0: 82635.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.3074173601941284\n",
      "    mean_inference_ms: 1.5425695111081639\n",
      "    mean_processing_ms: 0.359655035422949\n",
      "  time_since_restore: 66.22197079658508\n",
      "  time_this_iter_s: 13.160041570663452\n",
      "  time_total_s: 66.22197079658508\n",
      "  timers:\n",
      "    learn_throughput: 601.787\n",
      "    learn_time_ms: 6979.211\n",
      "    load_throughput: 69642.525\n",
      "    load_time_ms: 60.308\n",
      "    sample_throughput: 691.543\n",
      "    sample_time_ms: 6073.379\n",
      "    update_time_ms: 4.115\n",
      "  timestamp: 1608024414\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 5\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          66.222</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">  292720</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-27-07\n",
      "  done: false\n",
      "  episode_len_mean: 714.2424242424242\n",
      "  episode_reward_max: 380844.0\n",
      "  episode_reward_mean: 304512.0\n",
      "  episode_reward_min: 247905.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 33\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.283511757850647\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012594873085618019\n",
      "        model: {}\n",
      "        policy_loss: -0.0016389070078730583\n",
      "        total_loss: 114133584.0\n",
      "        vf_explained_var: -7.450580596923828e-09\n",
      "        vf_loss: 114133584.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.2405028343200684\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012942202389240265\n",
      "        model: {}\n",
      "        policy_loss: 0.003915526904165745\n",
      "        total_loss: 114260640.0\n",
      "        vf_explained_var: -7.450580596923828e-09\n",
      "        vf_loss: 114260640.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0317518711090088\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014840939082205296\n",
      "        model: {}\n",
      "        policy_loss: 0.00496521033346653\n",
      "        total_loss: 114064064.0\n",
      "        vf_explained_var: 1.4901161193847656e-08\n",
      "        vf_loss: 114064064.0\n",
      "    num_steps_sampled: 25200\n",
      "    num_steps_trained: 25200\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.83888888888889\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 126948.0\n",
      "    agent-stage0_train1: 126948.0\n",
      "    agent-stage1_train0: 126948.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 101504.0\n",
      "    agent-stage0_train1: 101504.0\n",
      "    agent-stage1_train0: 101504.0\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: 82635.0\n",
      "    agent-stage0_train1: 82635.0\n",
      "    agent-stage1_train0: 82635.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.3073808163343505\n",
      "    mean_inference_ms: 1.539674463584876\n",
      "    mean_processing_ms: 0.35970913613378197\n",
      "  time_since_restore: 78.99102139472961\n",
      "  time_this_iter_s: 12.769050598144531\n",
      "  time_total_s: 78.99102139472961\n",
      "  timers:\n",
      "    learn_throughput: 604.625\n",
      "    learn_time_ms: 6946.452\n",
      "    load_throughput: 80604.762\n",
      "    load_time_ms: 52.106\n",
      "    sample_throughput: 693.703\n",
      "    sample_time_ms: 6054.463\n",
      "    update_time_ms: 4.103\n",
      "  timestamp: 1608024427\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25200\n",
      "  training_iteration: 6\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">          78.991</td><td style=\"text-align: right;\">25200</td><td style=\"text-align: right;\">  304512</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-27-20\n",
      "  done: false\n",
      "  episode_len_mean: 711.3076923076923\n",
      "  episode_reward_max: 384402.0\n",
      "  episode_reward_mean: 309110.1538461539\n",
      "  episode_reward_min: 247905.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 39\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.3127107620239258\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009356336668133736\n",
      "        model: {}\n",
      "        policy_loss: 0.005688743200153112\n",
      "        total_loss: 90479368.0\n",
      "        vf_explained_var: 1.1175870895385742e-08\n",
      "        vf_loss: 90479368.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.2214874029159546\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008932145312428474\n",
      "        model: {}\n",
      "        policy_loss: -0.0002636548597365618\n",
      "        total_loss: 90953088.0\n",
      "        vf_explained_var: 2.2351741790771484e-08\n",
      "        vf_loss: 90953088.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0032572746276855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010733859613537788\n",
      "        model: {}\n",
      "        policy_loss: 0.007313713431358337\n",
      "        total_loss: 90465648.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 90465648.0\n",
      "    num_steps_sampled: 29400\n",
      "    num_steps_trained: 29400\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.77368421052633\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 128134.0\n",
      "    agent-stage0_train1: 128134.0\n",
      "    agent-stage1_train0: 128134.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 103036.71794871795\n",
      "    agent-stage0_train1: 103036.71794871795\n",
      "    agent-stage1_train0: 103036.71794871795\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: 82635.0\n",
      "    agent-stage0_train1: 82635.0\n",
      "    agent-stage1_train0: 82635.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.3077268569555054\n",
      "    mean_inference_ms: 1.5372394031742629\n",
      "    mean_processing_ms: 0.3598852340602633\n",
      "  time_since_restore: 91.70747542381287\n",
      "  time_this_iter_s: 12.716454029083252\n",
      "  time_total_s: 91.70747542381287\n",
      "  timers:\n",
      "    learn_throughput: 608.314\n",
      "    learn_time_ms: 6904.332\n",
      "    load_throughput: 90548.676\n",
      "    load_time_ms: 46.384\n",
      "    sample_throughput: 694.023\n",
      "    sample_time_ms: 6051.677\n",
      "    update_time_ms: 4.232\n",
      "  timestamp: 1608024440\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29400\n",
      "  training_iteration: 7\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         91.7075</td><td style=\"text-align: right;\">29400</td><td style=\"text-align: right;\">  309110</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-27-33\n",
      "  done: false\n",
      "  episode_len_mean: 710.2666666666667\n",
      "  episode_reward_max: 384402.0\n",
      "  episode_reward_mean: 314078.0\n",
      "  episode_reward_min: 247905.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 45\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.352473497390747\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014137305319309235\n",
      "        model: {}\n",
      "        policy_loss: -0.004786821082234383\n",
      "        total_loss: 93559952.0\n",
      "        vf_explained_var: 3.725290298461914e-09\n",
      "        vf_loss: 93559952.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.2359251976013184\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007976200431585312\n",
      "        model: {}\n",
      "        policy_loss: 0.00012710271403193474\n",
      "        total_loss: 93919256.0\n",
      "        vf_explained_var: 2.60770320892334e-08\n",
      "        vf_loss: 93919256.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8375976085662842\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005864663980901241\n",
      "        model: {}\n",
      "        policy_loss: -0.0046371398493647575\n",
      "        total_loss: 93794416.0\n",
      "        vf_explained_var: -1.1175870895385742e-08\n",
      "        vf_loss: 93794416.0\n",
      "    num_steps_sampled: 33600\n",
      "    num_steps_trained: 33600\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.8111111111111\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 128134.0\n",
      "    agent-stage0_train1: 128134.0\n",
      "    agent-stage1_train0: 128134.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 104692.66666666667\n",
      "    agent-stage0_train1: 104692.66666666667\n",
      "    agent-stage1_train0: 104692.66666666667\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: 82635.0\n",
      "    agent-stage0_train1: 82635.0\n",
      "    agent-stage1_train0: 82635.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.30732366347577\n",
      "    mean_inference_ms: 1.5354145295623318\n",
      "    mean_processing_ms: 0.360010136472669\n",
      "  time_since_restore: 104.59957695007324\n",
      "  time_this_iter_s: 12.892101526260376\n",
      "  time_total_s: 104.59957695007324\n",
      "  timers:\n",
      "    learn_throughput: 608.741\n",
      "    learn_time_ms: 6899.484\n",
      "    load_throughput: 99890.783\n",
      "    load_time_ms: 42.046\n",
      "    sample_throughput: 694.815\n",
      "    sample_time_ms: 6044.779\n",
      "    update_time_ms: 4.222\n",
      "  timestamp: 1608024453\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33600\n",
      "  training_iteration: 8\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">           104.6</td><td style=\"text-align: right;\">33600</td><td style=\"text-align: right;\">  314078</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-27-46\n",
      "  done: false\n",
      "  episode_len_mean: 712.2941176470588\n",
      "  episode_reward_max: 388584.0\n",
      "  episode_reward_mean: 318983.4705882353\n",
      "  episode_reward_min: 247905.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 51\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.3374602794647217\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006471266504377127\n",
      "        model: {}\n",
      "        policy_loss: -0.004233758430927992\n",
      "        total_loss: 90685808.0\n",
      "        vf_explained_var: 9.313225746154785e-09\n",
      "        vf_loss: 90685808.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.206902265548706\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015423308126628399\n",
      "        model: {}\n",
      "        policy_loss: 0.0027836356312036514\n",
      "        total_loss: 90841176.0\n",
      "        vf_explained_var: 1.4901161193847656e-08\n",
      "        vf_loss: 90841176.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.7665879726409912\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010652015917003155\n",
      "        model: {}\n",
      "        policy_loss: 0.0041988627053797245\n",
      "        total_loss: 90512368.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 90512368.0\n",
      "    num_steps_sampled: 37800\n",
      "    num_steps_trained: 37800\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.35789473684211\n",
      "    ram_util_percent: 6.231578947368421\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 129528.0\n",
      "    agent-stage0_train1: 129528.0\n",
      "    agent-stage1_train0: 129528.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 106327.82352941176\n",
      "    agent-stage0_train1: 106327.82352941176\n",
      "    agent-stage1_train0: 106327.82352941176\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: 82635.0\n",
      "    agent-stage0_train1: 82635.0\n",
      "    agent-stage1_train0: 82635.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.3072218516295835\n",
      "    mean_inference_ms: 1.5347118134101017\n",
      "    mean_processing_ms: 0.3602468838116994\n",
      "  time_since_restore: 117.84491634368896\n",
      "  time_this_iter_s: 13.245339393615723\n",
      "  time_total_s: 117.84491634368896\n",
      "  timers:\n",
      "    learn_throughput: 608.558\n",
      "    learn_time_ms: 6901.562\n",
      "    load_throughput: 108359.134\n",
      "    load_time_ms: 38.76\n",
      "    sample_throughput: 691.685\n",
      "    sample_time_ms: 6072.126\n",
      "    update_time_ms: 4.375\n",
      "  timestamp: 1608024466\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37800\n",
      "  training_iteration: 9\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         117.845</td><td style=\"text-align: right;\">37800</td><td style=\"text-align: right;\">  318983</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-27-59\n",
      "  done: false\n",
      "  episode_len_mean: 708.3508771929825\n",
      "  episode_reward_max: 388584.0\n",
      "  episode_reward_mean: 321048.0\n",
      "  episode_reward_min: 247905.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 57\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.3112157583236694\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014471384696662426\n",
      "        model: {}\n",
      "        policy_loss: -0.009466606192290783\n",
      "        total_loss: 111349392.0\n",
      "        vf_explained_var: -5.587935447692871e-09\n",
      "        vf_loss: 111349392.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.170749545097351\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008995383977890015\n",
      "        model: {}\n",
      "        policy_loss: -0.0009631216526031494\n",
      "        total_loss: 110870512.0\n",
      "        vf_explained_var: -1.4901161193847656e-08\n",
      "        vf_loss: 110870512.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.885370671749115\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011073799803853035\n",
      "        model: {}\n",
      "        policy_loss: 0.0039589982479810715\n",
      "        total_loss: 110639608.0\n",
      "        vf_explained_var: 3.725290298461914e-09\n",
      "        vf_loss: 110639608.0\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.98947368421052\n",
      "    ram_util_percent: 6.273684210526316\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 129528.0\n",
      "    agent-stage0_train1: 129528.0\n",
      "    agent-stage1_train0: 129528.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 107016.0\n",
      "    agent-stage0_train1: 107016.0\n",
      "    agent-stage1_train0: 107016.0\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: 82635.0\n",
      "    agent-stage0_train1: 82635.0\n",
      "    agent-stage1_train0: 82635.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.3085017471594766\n",
      "    mean_inference_ms: 1.5348039792985126\n",
      "    mean_processing_ms: 0.36052817751943844\n",
      "  time_since_restore: 131.2150399684906\n",
      "  time_this_iter_s: 13.370123624801636\n",
      "  time_total_s: 131.2150399684906\n",
      "  timers:\n",
      "    learn_throughput: 608.061\n",
      "    learn_time_ms: 6907.201\n",
      "    load_throughput: 116755.469\n",
      "    load_time_ms: 35.973\n",
      "    sample_throughput: 688.154\n",
      "    sample_time_ms: 6103.285\n",
      "    update_time_ms: 4.365\n",
      "  timestamp: 1608024479\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 10\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         131.215</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\">  321048</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-28-12\n",
      "  done: false\n",
      "  episode_len_mean: 692.939393939394\n",
      "  episode_reward_max: 388584.0\n",
      "  episode_reward_mean: 310533.13636363635\n",
      "  episode_reward_min: 1023.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 66\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.2624142169952393\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010860410518944263\n",
      "        model: {}\n",
      "        policy_loss: -0.0016686003655195236\n",
      "        total_loss: 82423688.0\n",
      "        vf_explained_var: -9.313225746154785e-09\n",
      "        vf_loss: 82423688.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1166658401489258\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00969550758600235\n",
      "        model: {}\n",
      "        policy_loss: 0.0007492201402783394\n",
      "        total_loss: 82877952.0\n",
      "        vf_explained_var: 1.862645149230957e-09\n",
      "        vf_loss: 82877952.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9212633967399597\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006846476346254349\n",
      "        model: {}\n",
      "        policy_loss: -0.0015511661767959595\n",
      "        total_loss: 82851728.0\n",
      "        vf_explained_var: -1.862645149230957e-09\n",
      "        vf_loss: 82851728.0\n",
      "    num_steps_sampled: 46200\n",
      "    num_steps_trained: 46200\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.35789473684211\n",
      "    ram_util_percent: 6.242105263157895\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 129528.0\n",
      "    agent-stage0_train1: 129528.0\n",
      "    agent-stage1_train0: 129528.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 103511.04545454546\n",
      "    agent-stage0_train1: 103511.04545454546\n",
      "    agent-stage1_train0: 103511.04545454546\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: 341.0\n",
      "    agent-stage0_train1: 341.0\n",
      "    agent-stage1_train0: 341.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.3122785972618014\n",
      "    mean_inference_ms: 1.535120838127489\n",
      "    mean_processing_ms: 0.36095266296473943\n",
      "  time_since_restore: 144.22553038597107\n",
      "  time_this_iter_s: 13.010490417480469\n",
      "  time_total_s: 144.22553038597107\n",
      "  timers:\n",
      "    learn_throughput: 614.466\n",
      "    learn_time_ms: 6835.206\n",
      "    load_throughput: 357891.036\n",
      "    load_time_ms: 11.735\n",
      "    sample_throughput: 684.369\n",
      "    sample_time_ms: 6137.041\n",
      "    update_time_ms: 4.383\n",
      "  timestamp: 1608024492\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46200\n",
      "  training_iteration: 11\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         144.226</td><td style=\"text-align: right;\">46200</td><td style=\"text-align: right;\">  310533</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-28-25\n",
      "  done: false\n",
      "  episode_len_mean: 679.0958904109589\n",
      "  episode_reward_max: 388932.0\n",
      "  episode_reward_mean: 316077.7397260274\n",
      "  episode_reward_min: 1023.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 73\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.252145528793335\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0147372642531991\n",
      "        model: {}\n",
      "        policy_loss: 0.0031618699431419373\n",
      "        total_loss: 183244032.0\n",
      "        vf_explained_var: -1.30385160446167e-08\n",
      "        vf_loss: 183244032.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1008751392364502\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008633537217974663\n",
      "        model: {}\n",
      "        policy_loss: 0.00011232681572437286\n",
      "        total_loss: 183918176.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 183918176.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.880074143409729\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008391337469220161\n",
      "        model: {}\n",
      "        policy_loss: 0.0028316397219896317\n",
      "        total_loss: 183596304.0\n",
      "        vf_explained_var: -1.1175870895385742e-08\n",
      "        vf_loss: 183596304.0\n",
      "    num_steps_sampled: 50400\n",
      "    num_steps_trained: 50400\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.16666666666667\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 129644.0\n",
      "    agent-stage0_train1: 129644.0\n",
      "    agent-stage1_train0: 129644.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 105359.24657534246\n",
      "    agent-stage0_train1: 105359.24657534246\n",
      "    agent-stage1_train0: 105359.24657534246\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: 341.0\n",
      "    agent-stage0_train1: 341.0\n",
      "    agent-stage1_train0: 341.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.3157680980997273\n",
      "    mean_inference_ms: 1.5351872156646498\n",
      "    mean_processing_ms: 0.36114083546468684\n",
      "  time_since_restore: 157.11602759361267\n",
      "  time_this_iter_s: 12.890497207641602\n",
      "  time_total_s: 157.11602759361267\n",
      "  timers:\n",
      "    learn_throughput: 615.413\n",
      "    learn_time_ms: 6824.682\n",
      "    load_throughput: 357690.47\n",
      "    load_time_ms: 11.742\n",
      "    sample_throughput: 682.887\n",
      "    sample_time_ms: 6150.362\n",
      "    update_time_ms: 4.37\n",
      "  timestamp: 1608024505\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50400\n",
      "  training_iteration: 12\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         157.116</td><td style=\"text-align: right;\">50400</td><td style=\"text-align: right;\">  316078</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-28-39\n",
      "  done: false\n",
      "  episode_len_mean: 665.5185185185185\n",
      "  episode_reward_max: 390237.0\n",
      "  episode_reward_mean: 321489.0\n",
      "  episode_reward_min: 1023.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 81\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.2889777421951294\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0068427459336817265\n",
      "        model: {}\n",
      "        policy_loss: -0.003912094049155712\n",
      "        total_loss: 181769888.0\n",
      "        vf_explained_var: -1.30385160446167e-08\n",
      "        vf_loss: 181769888.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1405597925186157\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012117834761738777\n",
      "        model: {}\n",
      "        policy_loss: 0.0018992843106389046\n",
      "        total_loss: 181720192.0\n",
      "        vf_explained_var: -1.862645149230957e-08\n",
      "        vf_loss: 181720192.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8588824272155762\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008295845240354538\n",
      "        model: {}\n",
      "        policy_loss: -0.0012489799410104752\n",
      "        total_loss: 181635696.0\n",
      "        vf_explained_var: -3.725290298461914e-09\n",
      "        vf_loss: 181635696.0\n",
      "    num_steps_sampled: 54600\n",
      "    num_steps_trained: 54600\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.415\n",
      "    ram_util_percent: 6.235000000000001\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 130079.0\n",
      "    agent-stage0_train1: 130079.0\n",
      "    agent-stage1_train0: 130079.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 107163.0\n",
      "    agent-stage0_train1: 107163.0\n",
      "    agent-stage1_train0: 107163.0\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: 341.0\n",
      "    agent-stage0_train1: 341.0\n",
      "    agent-stage1_train0: 341.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.320126570728137\n",
      "    mean_inference_ms: 1.5352264891788359\n",
      "    mean_processing_ms: 0.3613587444280087\n",
      "  time_since_restore: 170.7176787853241\n",
      "  time_this_iter_s: 13.601651191711426\n",
      "  time_total_s: 170.7176787853241\n",
      "  timers:\n",
      "    learn_throughput: 610.284\n",
      "    learn_time_ms: 6882.046\n",
      "    load_throughput: 354346.271\n",
      "    load_time_ms: 11.853\n",
      "    sample_throughput: 677.755\n",
      "    sample_time_ms: 6196.932\n",
      "    update_time_ms: 4.371\n",
      "  timestamp: 1608024519\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 54600\n",
      "  training_iteration: 13\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         170.718</td><td style=\"text-align: right;\">54600</td><td style=\"text-align: right;\">  321489</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-28-53\n",
      "  done: false\n",
      "  episode_len_mean: 656.625\n",
      "  episode_reward_max: 390237.0\n",
      "  episode_reward_mean: 325049.25\n",
      "  episode_reward_min: 1023.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 88\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.3060574531555176\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012340176850557327\n",
      "        model: {}\n",
      "        policy_loss: -0.008046197704970837\n",
      "        total_loss: 160853152.0\n",
      "        vf_explained_var: -1.4901161193847656e-08\n",
      "        vf_loss: 160853152.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1109907627105713\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007502542808651924\n",
      "        model: {}\n",
      "        policy_loss: 0.0007018819451332092\n",
      "        total_loss: 161019280.0\n",
      "        vf_explained_var: 9.313225746154785e-09\n",
      "        vf_loss: 161019280.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8902325630187988\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007504175417125225\n",
      "        model: {}\n",
      "        policy_loss: -0.001637561246752739\n",
      "        total_loss: 160763792.0\n",
      "        vf_explained_var: -1.4901161193847656e-08\n",
      "        vf_loss: 160763792.0\n",
      "    num_steps_sampled: 58800\n",
      "    num_steps_trained: 58800\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.85789473684211\n",
      "    ram_util_percent: 6.236842105263158\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 130079.0\n",
      "    agent-stage0_train1: 130079.0\n",
      "    agent-stage1_train0: 130079.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 108349.75\n",
      "    agent-stage0_train1: 108349.75\n",
      "    agent-stage1_train0: 108349.75\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: 341.0\n",
      "    agent-stage0_train1: 341.0\n",
      "    agent-stage1_train0: 341.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.3241328512356216\n",
      "    mean_inference_ms: 1.5352668325563834\n",
      "    mean_processing_ms: 0.36148216111124737\n",
      "  time_since_restore: 184.24545526504517\n",
      "  time_this_iter_s: 13.52777647972107\n",
      "  time_total_s: 184.24545526504517\n",
      "  timers:\n",
      "    learn_throughput: 612.065\n",
      "    learn_time_ms: 6862.022\n",
      "    load_throughput: 347350.259\n",
      "    load_time_ms: 12.092\n",
      "    sample_throughput: 674.573\n",
      "    sample_time_ms: 6226.163\n",
      "    update_time_ms: 4.347\n",
      "  timestamp: 1608024533\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 58800\n",
      "  training_iteration: 14\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         184.245</td><td style=\"text-align: right;\">58800</td><td style=\"text-align: right;\">  325049</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-29-06\n",
      "  done: false\n",
      "  episode_len_mean: 646.6354166666666\n",
      "  episode_reward_max: 404694.0\n",
      "  episode_reward_mean: 323156.90625\n",
      "  episode_reward_min: 1023.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 96\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.2544571161270142\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011585218831896782\n",
      "        model: {}\n",
      "        policy_loss: 0.0007476822938770056\n",
      "        total_loss: 101374960.0\n",
      "        vf_explained_var: -1.1175870895385742e-08\n",
      "        vf_loss: 101374960.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0656346082687378\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01233847625553608\n",
      "        model: {}\n",
      "        policy_loss: 0.0010338285937905312\n",
      "        total_loss: 101594552.0\n",
      "        vf_explained_var: -1.4901161193847656e-08\n",
      "        vf_loss: 101594552.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9905423521995544\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009912695735692978\n",
      "        model: {}\n",
      "        policy_loss: 0.0009817699901759624\n",
      "        total_loss: 101494480.0\n",
      "        vf_explained_var: -1.862645149230957e-08\n",
      "        vf_loss: 101494480.0\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 63000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.58421052631579\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 134898.0\n",
      "    agent-stage0_train1: 134898.0\n",
      "    agent-stage1_train0: 134898.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 107718.96875\n",
      "    agent-stage0_train1: 107718.96875\n",
      "    agent-stage1_train0: 107718.96875\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: 341.0\n",
      "    agent-stage0_train1: 341.0\n",
      "    agent-stage1_train0: 341.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.32851500275751\n",
      "    mean_inference_ms: 1.5349465922578318\n",
      "    mean_processing_ms: 0.3615764153600382\n",
      "  time_since_restore: 197.27126264572144\n",
      "  time_this_iter_s: 13.02580738067627\n",
      "  time_total_s: 197.27126264572144\n",
      "  timers:\n",
      "    learn_throughput: 615.071\n",
      "    learn_time_ms: 6828.485\n",
      "    load_throughput: 343333.745\n",
      "    load_time_ms: 12.233\n",
      "    sample_throughput: 672.416\n",
      "    sample_time_ms: 6246.131\n",
      "    update_time_ms: 4.362\n",
      "  timestamp: 1608024546\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 15\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         197.271</td><td style=\"text-align: right;\">63000</td><td style=\"text-align: right;\">  323157</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-29-19\n",
      "  done: false\n",
      "  episode_len_mean: 629.1\n",
      "  episode_reward_max: 404694.0\n",
      "  episode_reward_mean: 296424.48\n",
      "  episode_reward_min: -355500.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 105\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.214457631111145\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009953013621270657\n",
      "        model: {}\n",
      "        policy_loss: -0.005117571912705898\n",
      "        total_loss: 127827256.0\n",
      "        vf_explained_var: -2.0489096641540527e-08\n",
      "        vf_loss: 127827256.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0001482963562012\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011444389820098877\n",
      "        model: {}\n",
      "        policy_loss: 0.004445694386959076\n",
      "        total_loss: 127772160.0\n",
      "        vf_explained_var: -9.313225746154785e-09\n",
      "        vf_loss: 127772160.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0207085609436035\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011818808503448963\n",
      "        model: {}\n",
      "        policy_loss: 0.0031181182712316513\n",
      "        total_loss: 127801504.0\n",
      "        vf_explained_var: -7.450580596923828e-09\n",
      "        vf_loss: 127801504.0\n",
      "    num_steps_sampled: 67200\n",
      "    num_steps_trained: 67200\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.77368421052631\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 134898.0\n",
      "    agent-stage0_train1: 134898.0\n",
      "    agent-stage1_train0: 134898.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 98808.16\n",
      "    agent-stage0_train1: 98808.16\n",
      "    agent-stage1_train0: 98808.16\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -118500.0\n",
      "    agent-stage0_train1: -118500.0\n",
      "    agent-stage1_train0: -118500.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.3366942579603793\n",
      "    mean_inference_ms: 1.5328716305678514\n",
      "    mean_processing_ms: 0.3618752066662311\n",
      "  time_since_restore: 210.33329105377197\n",
      "  time_this_iter_s: 13.062028408050537\n",
      "  time_total_s: 210.33329105377197\n",
      "  timers:\n",
      "    learn_throughput: 617.03\n",
      "    learn_time_ms: 6806.796\n",
      "    load_throughput: 342169.371\n",
      "    load_time_ms: 12.275\n",
      "    sample_throughput: 666.983\n",
      "    sample_time_ms: 6297.017\n",
      "    update_time_ms: 4.373\n",
      "  timestamp: 1608024559\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 67200\n",
      "  training_iteration: 16\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         210.333</td><td style=\"text-align: right;\">67200</td><td style=\"text-align: right;\">  296424</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-29-32\n",
      "  done: false\n",
      "  episode_len_mean: 615.5\n",
      "  episode_reward_max: 404694.0\n",
      "  episode_reward_mean: 292027.5\n",
      "  episode_reward_min: -355500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 113\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.23794424533844\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009181461296975613\n",
      "        model: {}\n",
      "        policy_loss: -0.0025042779743671417\n",
      "        total_loss: 154905920.0\n",
      "        vf_explained_var: 1.30385160446167e-08\n",
      "        vf_loss: 154905920.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0683614015579224\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007349672261625528\n",
      "        model: {}\n",
      "        policy_loss: -3.928644582629204e-05\n",
      "        total_loss: 156109728.0\n",
      "        vf_explained_var: -3.725290298461914e-09\n",
      "        vf_loss: 156109728.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.066695213317871\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010888330638408661\n",
      "        model: {}\n",
      "        policy_loss: 0.0028270529583096504\n",
      "        total_loss: 155467008.0\n",
      "        vf_explained_var: -1.6763806343078613e-08\n",
      "        vf_loss: 155467008.0\n",
      "    num_steps_sampled: 71400\n",
      "    num_steps_trained: 71400\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.66315789473684\n",
      "    ram_util_percent: 6.247368421052632\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 134898.0\n",
      "    agent-stage0_train1: 134898.0\n",
      "    agent-stage1_train0: 134898.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 97342.5\n",
      "    agent-stage0_train1: 97342.5\n",
      "    agent-stage1_train0: 97342.5\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -118500.0\n",
      "    agent-stage0_train1: -118500.0\n",
      "    agent-stage1_train0: -118500.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.343868034495604\n",
      "    mean_inference_ms: 1.5321518730483092\n",
      "    mean_processing_ms: 0.3620570916407355\n",
      "  time_since_restore: 223.720463514328\n",
      "  time_this_iter_s: 13.38717246055603\n",
      "  time_total_s: 223.720463514328\n",
      "  timers:\n",
      "    learn_throughput: 617.05\n",
      "    learn_time_ms: 6806.576\n",
      "    load_throughput: 341135.697\n",
      "    load_time_ms: 12.312\n",
      "    sample_throughput: 659.916\n",
      "    sample_time_ms: 6364.448\n",
      "    update_time_ms: 4.291\n",
      "  timestamp: 1608024572\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71400\n",
      "  training_iteration: 17\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">          223.72</td><td style=\"text-align: right;\">71400</td><td style=\"text-align: right;\">  292028</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-29-45\n",
      "  done: false\n",
      "  episode_len_mean: 598.43\n",
      "  episode_reward_max: 404694.0\n",
      "  episode_reward_mean: 295675.89\n",
      "  episode_reward_min: -355500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 121\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.2208409309387207\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010607204400002956\n",
      "        model: {}\n",
      "        policy_loss: 0.004823475144803524\n",
      "        total_loss: 177024112.0\n",
      "        vf_explained_var: 5.587935447692871e-09\n",
      "        vf_loss: 177024112.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.009676218032837\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01150763500481844\n",
      "        model: {}\n",
      "        policy_loss: 0.002660229802131653\n",
      "        total_loss: 177193952.0\n",
      "        vf_explained_var: 1.30385160446167e-08\n",
      "        vf_loss: 177193952.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0336143970489502\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008128291927278042\n",
      "        model: {}\n",
      "        policy_loss: 0.0031879907473921776\n",
      "        total_loss: 177212192.0\n",
      "        vf_explained_var: 1.4901161193847656e-08\n",
      "        vf_loss: 177212192.0\n",
      "    num_steps_sampled: 75600\n",
      "    num_steps_trained: 75600\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.07777777777778\n",
      "    ram_util_percent: 6.155555555555556\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 134898.0\n",
      "    agent-stage0_train1: 134898.0\n",
      "    agent-stage1_train0: 134898.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 98558.63\n",
      "    agent-stage0_train1: 98558.63\n",
      "    agent-stage1_train0: 98558.63\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -118500.0\n",
      "    agent-stage0_train1: -118500.0\n",
      "    agent-stage1_train0: -118500.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.352242370596193\n",
      "    mean_inference_ms: 1.5319879768833433\n",
      "    mean_processing_ms: 0.3623370278994593\n",
      "  time_since_restore: 236.7436821460724\n",
      "  time_this_iter_s: 13.023218631744385\n",
      "  time_total_s: 236.7436821460724\n",
      "  timers:\n",
      "    learn_throughput: 619.954\n",
      "    learn_time_ms: 6774.693\n",
      "    load_throughput: 340141.084\n",
      "    load_time_ms: 12.348\n",
      "    sample_throughput: 655.272\n",
      "    sample_time_ms: 6409.555\n",
      "    update_time_ms: 4.262\n",
      "  timestamp: 1608024585\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75600\n",
      "  training_iteration: 18\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         236.744</td><td style=\"text-align: right;\">75600</td><td style=\"text-align: right;\">  295676</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-29-59\n",
      "  done: false\n",
      "  episode_len_mean: 582.35\n",
      "  episode_reward_max: 404694.0\n",
      "  episode_reward_mean: 295959.81\n",
      "  episode_reward_min: -355500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 129\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.2582893371582031\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008823813870549202\n",
      "        model: {}\n",
      "        policy_loss: -0.004412564914673567\n",
      "        total_loss: 160302160.0\n",
      "        vf_explained_var: 1.862645149230957e-08\n",
      "        vf_loss: 160302160.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.008171558380127\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00897236354649067\n",
      "        model: {}\n",
      "        policy_loss: -0.002706449944525957\n",
      "        total_loss: 160878288.0\n",
      "        vf_explained_var: 1.862645149230957e-08\n",
      "        vf_loss: 160878288.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.043445348739624\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021808262914419174\n",
      "        model: {}\n",
      "        policy_loss: 0.0046572391875088215\n",
      "        total_loss: 159406864.0\n",
      "        vf_explained_var: -5.587935447692871e-09\n",
      "        vf_loss: 159406864.0\n",
      "    num_steps_sampled: 79800\n",
      "    num_steps_trained: 79800\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.89473684210526\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 134898.0\n",
      "    agent-stage0_train1: 134898.0\n",
      "    agent-stage1_train0: 134898.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 98653.27\n",
      "    agent-stage0_train1: 98653.27\n",
      "    agent-stage1_train0: 98653.27\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -118500.0\n",
      "    agent-stage0_train1: -118500.0\n",
      "    agent-stage1_train0: -118500.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.3607407894701278\n",
      "    mean_inference_ms: 1.5318394968528029\n",
      "    mean_processing_ms: 0.3624669381564955\n",
      "  time_since_restore: 249.92504501342773\n",
      "  time_this_iter_s: 13.181362867355347\n",
      "  time_total_s: 249.92504501342773\n",
      "  timers:\n",
      "    learn_throughput: 620.512\n",
      "    learn_time_ms: 6768.605\n",
      "    load_throughput: 336543.052\n",
      "    load_time_ms: 12.48\n",
      "    sample_throughput: 655.247\n",
      "    sample_time_ms: 6409.794\n",
      "    update_time_ms: 4.099\n",
      "  timestamp: 1608024599\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79800\n",
      "  training_iteration: 19\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         249.925</td><td style=\"text-align: right;\">79800</td><td style=\"text-align: right;\">  295960</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-30-12\n",
      "  done: false\n",
      "  episode_len_mean: 565.36\n",
      "  episode_reward_max: 404694.0\n",
      "  episode_reward_mean: 277764.63\n",
      "  episode_reward_min: -355500.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 138\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1813830137252808\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013538362458348274\n",
      "        model: {}\n",
      "        policy_loss: 0.0014542192220687866\n",
      "        total_loss: 133301768.0\n",
      "        vf_explained_var: -1.862645149230957e-09\n",
      "        vf_loss: 133301768.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9839160442352295\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010278625413775444\n",
      "        model: {}\n",
      "        policy_loss: 0.004541456699371338\n",
      "        total_loss: 133852232.0\n",
      "        vf_explained_var: -1.4901161193847656e-08\n",
      "        vf_loss: 133852232.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0065743923187256\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007657754234969616\n",
      "        model: {}\n",
      "        policy_loss: -0.0017144584562629461\n",
      "        total_loss: 133175936.0\n",
      "        vf_explained_var: 5.587935447692871e-09\n",
      "        vf_loss: 133175936.0\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.73157894736842\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 134898.0\n",
      "    agent-stage0_train1: 134898.0\n",
      "    agent-stage1_train0: 134898.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 92588.21\n",
      "    agent-stage0_train1: 92588.21\n",
      "    agent-stage1_train0: 92588.21\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -118500.0\n",
      "    agent-stage0_train1: -118500.0\n",
      "    agent-stage1_train0: -118500.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.3716246399031404\n",
      "    mean_inference_ms: 1.5322544122261297\n",
      "    mean_processing_ms: 0.36265193487289676\n",
      "  time_since_restore: 263.0106272697449\n",
      "  time_this_iter_s: 13.085582256317139\n",
      "  time_total_s: 263.0106272697449\n",
      "  timers:\n",
      "    learn_throughput: 623.734\n",
      "    learn_time_ms: 6733.644\n",
      "    load_throughput: 331017.265\n",
      "    load_time_ms: 12.688\n",
      "    sample_throughput: 654.585\n",
      "    sample_time_ms: 6416.279\n",
      "    update_time_ms: 4.053\n",
      "  timestamp: 1608024612\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 20\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         263.011</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  277765</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-30-25\n",
      "  done: false\n",
      "  episode_len_mean: 551.8\n",
      "  episode_reward_max: 404694.0\n",
      "  episode_reward_mean: 265318.71\n",
      "  episode_reward_min: -355500.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 145\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1111998558044434\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017209092155098915\n",
      "        model: {}\n",
      "        policy_loss: -0.00019862595945596695\n",
      "        total_loss: 96150752.0\n",
      "        vf_explained_var: -5.587935447692871e-09\n",
      "        vf_loss: 96150752.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0285282135009766\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008115935139358044\n",
      "        model: {}\n",
      "        policy_loss: 0.001299404539167881\n",
      "        total_loss: 95823504.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 95823504.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0179364681243896\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01554282009601593\n",
      "        model: {}\n",
      "        policy_loss: -0.003772866679355502\n",
      "        total_loss: 95822312.0\n",
      "        vf_explained_var: -1.862645149230957e-09\n",
      "        vf_loss: 95822312.0\n",
      "    num_steps_sampled: 88200\n",
      "    num_steps_trained: 88200\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.33157894736841\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 134898.0\n",
      "    agent-stage0_train1: 134898.0\n",
      "    agent-stage1_train0: 134898.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 88439.57\n",
      "    agent-stage0_train1: 88439.57\n",
      "    agent-stage1_train0: 88439.57\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -118500.0\n",
      "    agent-stage0_train1: -118500.0\n",
      "    agent-stage1_train0: -118500.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.3810359368755103\n",
      "    mean_inference_ms: 1.5326477772727594\n",
      "    mean_processing_ms: 0.36274972584085213\n",
      "  time_since_restore: 276.2702419757843\n",
      "  time_this_iter_s: 13.259614706039429\n",
      "  time_total_s: 276.2702419757843\n",
      "  timers:\n",
      "    learn_throughput: 621.683\n",
      "    learn_time_ms: 6755.853\n",
      "    load_throughput: 322133.739\n",
      "    load_time_ms: 13.038\n",
      "    sample_throughput: 654.383\n",
      "    sample_time_ms: 6418.26\n",
      "    update_time_ms: 4.062\n",
      "  timestamp: 1608024625\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88200\n",
      "  training_iteration: 21\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">          276.27</td><td style=\"text-align: right;\">88200</td><td style=\"text-align: right;\">  265319</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-30-39\n",
      "  done: false\n",
      "  episode_len_mean: 530.65\n",
      "  episode_reward_max: 404694.0\n",
      "  episode_reward_mean: 241406.91\n",
      "  episode_reward_min: -355500.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 154\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.085221290588379\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009380446746945381\n",
      "        model: {}\n",
      "        policy_loss: 0.005392966791987419\n",
      "        total_loss: 150267584.0\n",
      "        vf_explained_var: 3.725290298461914e-09\n",
      "        vf_loss: 150267584.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.068947196006775\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009473268873989582\n",
      "        model: {}\n",
      "        policy_loss: 0.0026138070970773697\n",
      "        total_loss: 150889840.0\n",
      "        vf_explained_var: -3.725290298461914e-09\n",
      "        vf_loss: 150889840.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8120548725128174\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007899653166532516\n",
      "        model: {}\n",
      "        policy_loss: -0.004440086893737316\n",
      "        total_loss: 150084768.0\n",
      "        vf_explained_var: 1.30385160446167e-08\n",
      "        vf_loss: 150084768.0\n",
      "    num_steps_sampled: 92400\n",
      "    num_steps_trained: 92400\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.68499999999999\n",
      "    ram_util_percent: 6.200000000000001\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 134898.0\n",
      "    agent-stage0_train1: 134898.0\n",
      "    agent-stage1_train0: 134898.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 80468.97\n",
      "    agent-stage0_train1: 80468.97\n",
      "    agent-stage1_train0: 80468.97\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -118500.0\n",
      "    agent-stage0_train1: -118500.0\n",
      "    agent-stage1_train0: -118500.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.392740260154179\n",
      "    mean_inference_ms: 1.532091005280641\n",
      "    mean_processing_ms: 0.3627286901829136\n",
      "  time_since_restore: 289.6248893737793\n",
      "  time_this_iter_s: 13.354647397994995\n",
      "  time_total_s: 289.6248893737793\n",
      "  timers:\n",
      "    learn_throughput: 619.551\n",
      "    learn_time_ms: 6779.104\n",
      "    load_throughput: 314004.788\n",
      "    load_time_ms: 13.376\n",
      "    sample_throughput: 652.084\n",
      "    sample_time_ms: 6440.893\n",
      "    update_time_ms: 4.087\n",
      "  timestamp: 1608024639\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92400\n",
      "  training_iteration: 22\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         289.625</td><td style=\"text-align: right;\">92400</td><td style=\"text-align: right;\">  241407</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-30-52\n",
      "  done: false\n",
      "  episode_len_mean: 521.06\n",
      "  episode_reward_max: 404694.0\n",
      "  episode_reward_mean: 225365.07\n",
      "  episode_reward_min: -355500.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 162\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1118688583374023\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007748397067189217\n",
      "        model: {}\n",
      "        policy_loss: -0.0025833945255726576\n",
      "        total_loss: 128848560.0\n",
      "        vf_explained_var: 1.1175870895385742e-08\n",
      "        vf_loss: 128848560.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0906096696853638\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00913973804563284\n",
      "        model: {}\n",
      "        policy_loss: 0.003974486608058214\n",
      "        total_loss: 129564592.0\n",
      "        vf_explained_var: -1.1175870895385742e-08\n",
      "        vf_loss: 129564592.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8701091408729553\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008027885109186172\n",
      "        model: {}\n",
      "        policy_loss: -0.0021765343844890594\n",
      "        total_loss: 128630384.0\n",
      "        vf_explained_var: -7.450580596923828e-09\n",
      "        vf_loss: 128630384.0\n",
      "    num_steps_sampled: 96600\n",
      "    num_steps_trained: 96600\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.77894736842104\n",
      "    ram_util_percent: 6.231578947368422\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 134898.0\n",
      "    agent-stage0_train1: 134898.0\n",
      "    agent-stage1_train0: 134898.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 75121.69\n",
      "    agent-stage0_train1: 75121.69\n",
      "    agent-stage1_train0: 75121.69\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -118500.0\n",
      "    agent-stage0_train1: -118500.0\n",
      "    agent-stage1_train0: -118500.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.402288899140792\n",
      "    mean_inference_ms: 1.5312694697851554\n",
      "    mean_processing_ms: 0.36264736673057385\n",
      "  time_since_restore: 303.3929443359375\n",
      "  time_this_iter_s: 13.768054962158203\n",
      "  time_total_s: 303.3929443359375\n",
      "  timers:\n",
      "    learn_throughput: 618.185\n",
      "    learn_time_ms: 6794.088\n",
      "    load_throughput: 312946.039\n",
      "    load_time_ms: 13.421\n",
      "    sample_throughput: 651.917\n",
      "    sample_time_ms: 6442.541\n",
      "    update_time_ms: 4.134\n",
      "  timestamp: 1608024652\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96600\n",
      "  training_iteration: 23\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.5/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         303.393</td><td style=\"text-align: right;\">96600</td><td style=\"text-align: right;\">  225365</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-31-07\n",
      "  done: false\n",
      "  episode_len_mean: 514.84\n",
      "  episode_reward_max: 404694.0\n",
      "  episode_reward_mean: 171155.76\n",
      "  episode_reward_min: -1045410.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 171\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0180665254592896\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009847468696534634\n",
      "        model: {}\n",
      "        policy_loss: -0.0007204324938356876\n",
      "        total_loss: 675624768.0\n",
      "        vf_explained_var: -1.862645149230957e-09\n",
      "        vf_loss: 675624768.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9426760077476501\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005714052356779575\n",
      "        model: {}\n",
      "        policy_loss: 0.0018521621823310852\n",
      "        total_loss: 676868160.0\n",
      "        vf_explained_var: 1.1175870895385742e-08\n",
      "        vf_loss: 676868160.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8412421345710754\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011276096105575562\n",
      "        model: {}\n",
      "        policy_loss: -0.004564151633530855\n",
      "        total_loss: 673238656.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 673238656.0\n",
      "    num_steps_sampled: 100800\n",
      "    num_steps_trained: 100800\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.92857142857144\n",
      "    ram_util_percent: 6.404761904761905\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 134898.0\n",
      "    agent-stage0_train1: 134898.0\n",
      "    agent-stage1_train0: 134898.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 57051.92\n",
      "    agent-stage0_train1: 57051.92\n",
      "    agent-stage1_train0: 57051.92\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -348470.0\n",
      "    agent-stage0_train1: -348470.0\n",
      "    agent-stage1_train0: -348470.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.4133818699075613\n",
      "    mean_inference_ms: 1.5313275059782858\n",
      "    mean_processing_ms: 0.36264975130710597\n",
      "  time_since_restore: 317.74321842193604\n",
      "  time_this_iter_s: 14.350274085998535\n",
      "  time_total_s: 317.74321842193604\n",
      "  timers:\n",
      "    learn_throughput: 619.468\n",
      "    learn_time_ms: 6780.014\n",
      "    load_throughput: 309747.923\n",
      "    load_time_ms: 13.559\n",
      "    sample_throughput: 642.315\n",
      "    sample_time_ms: 6538.844\n",
      "    update_time_ms: 4.156\n",
      "  timestamp: 1608024667\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100800\n",
      "  training_iteration: 24\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         317.743</td><td style=\"text-align: right;\">100800</td><td style=\"text-align: right;\">  171156</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-31-21\n",
      "  done: false\n",
      "  episode_len_mean: 507.94\n",
      "  episode_reward_max: 404694.0\n",
      "  episode_reward_mean: 94912.77\n",
      "  episode_reward_min: -1045410.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 180\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8884502053260803\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005336248315870762\n",
      "        model: {}\n",
      "        policy_loss: 0.0006147054955363274\n",
      "        total_loss: 944541440.0\n",
      "        vf_explained_var: -2.2351741790771484e-08\n",
      "        vf_loss: 944541440.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0202422142028809\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010226164013147354\n",
      "        model: {}\n",
      "        policy_loss: -0.0007772082462906837\n",
      "        total_loss: 941539392.0\n",
      "        vf_explained_var: -1.30385160446167e-08\n",
      "        vf_loss: 941539392.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.7031506299972534\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007289163302630186\n",
      "        model: {}\n",
      "        policy_loss: -0.0033202050253748894\n",
      "        total_loss: 942742848.0\n",
      "        vf_explained_var: -5.587935447692871e-09\n",
      "        vf_loss: 942742848.0\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.34736842105262\n",
      "    ram_util_percent: 6.21578947368421\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 134898.0\n",
      "    agent-stage0_train1: 134898.0\n",
      "    agent-stage1_train0: 134898.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: 31637.59\n",
      "    agent-stage0_train1: 31637.59\n",
      "    agent-stage1_train0: 31637.59\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -348470.0\n",
      "    agent-stage0_train1: -348470.0\n",
      "    agent-stage1_train0: -348470.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.4242528254961253\n",
      "    mean_inference_ms: 1.5316717494063885\n",
      "    mean_processing_ms: 0.3626622557748203\n",
      "  time_since_restore: 331.38816380500793\n",
      "  time_this_iter_s: 13.6449453830719\n",
      "  time_total_s: 331.38816380500793\n",
      "  timers:\n",
      "    learn_throughput: 617.26\n",
      "    learn_time_ms: 6804.263\n",
      "    load_throughput: 306712.738\n",
      "    load_time_ms: 13.694\n",
      "    sample_throughput: 638.655\n",
      "    sample_time_ms: 6576.32\n",
      "    update_time_ms: 4.14\n",
      "  timestamp: 1608024681\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 25\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.5/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         331.388</td><td style=\"text-align: right;\">105000</td><td style=\"text-align: right;\"> 94912.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-31-35\n",
      "  done: false\n",
      "  episode_len_mean: 501.27\n",
      "  episode_reward_max: 388446.0\n",
      "  episode_reward_mean: -15529.32\n",
      "  episode_reward_min: -1264212.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 189\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.7069255709648132\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0035540577955543995\n",
      "        model: {}\n",
      "        policy_loss: 0.0008265934884548187\n",
      "        total_loss: 2249603584.0\n",
      "        vf_explained_var: -1.4901161193847656e-08\n",
      "        vf_loss: 2249603584.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.972669780254364\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008058829233050346\n",
      "        model: {}\n",
      "        policy_loss: 0.003132096491754055\n",
      "        total_loss: 2257656832.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 2257656832.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8397887349128723\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009098690003156662\n",
      "        model: {}\n",
      "        policy_loss: -0.0011384936515241861\n",
      "        total_loss: 2253817856.0\n",
      "        vf_explained_var: -1.6763806343078613e-08\n",
      "        vf_loss: 2253817856.0\n",
      "    num_steps_sampled: 109200\n",
      "    num_steps_trained: 109200\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.69047619047618\n",
      "    ram_util_percent: 6.31904761904762\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 129482.0\n",
      "    agent-stage0_train1: 129482.0\n",
      "    agent-stage1_train0: 129482.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -5176.44\n",
      "    agent-stage0_train1: -5176.44\n",
      "    agent-stage1_train0: -5176.44\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -421404.0\n",
      "    agent-stage0_train1: -421404.0\n",
      "    agent-stage1_train0: -421404.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.4348731832580466\n",
      "    mean_inference_ms: 1.532791909066668\n",
      "    mean_processing_ms: 0.3627864985721685\n",
      "  time_since_restore: 345.5964238643646\n",
      "  time_this_iter_s: 14.20826005935669\n",
      "  time_total_s: 345.5964238643646\n",
      "  timers:\n",
      "    learn_throughput: 615.145\n",
      "    learn_time_ms: 6827.662\n",
      "    load_throughput: 301677.695\n",
      "    load_time_ms: 13.922\n",
      "    sample_throughput: 629.948\n",
      "    sample_time_ms: 6667.221\n",
      "    update_time_ms: 4.111\n",
      "  timestamp: 1608024695\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 109200\n",
      "  training_iteration: 26\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         345.596</td><td style=\"text-align: right;\">109200</td><td style=\"text-align: right;\">-15529.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-31-50\n",
      "  done: false\n",
      "  episode_len_mean: 500.4\n",
      "  episode_reward_max: 381519.0\n",
      "  episode_reward_mean: -109399.5\n",
      "  episode_reward_min: -1264212.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 198\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.7773417234420776\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006421894766390324\n",
      "        model: {}\n",
      "        policy_loss: 0.0012248354032635689\n",
      "        total_loss: 1708081152.0\n",
      "        vf_explained_var: -1.6763806343078613e-08\n",
      "        vf_loss: 1708081152.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0611640214920044\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008248252794146538\n",
      "        model: {}\n",
      "        policy_loss: -0.0017053838819265366\n",
      "        total_loss: 1705782528.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 1705782528.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8078432083129883\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012666098773479462\n",
      "        model: {}\n",
      "        policy_loss: -0.0028800484724342823\n",
      "        total_loss: 1699722880.0\n",
      "        vf_explained_var: -5.587935447692871e-09\n",
      "        vf_loss: 1699722880.0\n",
      "    num_steps_sampled: 113400\n",
      "    num_steps_trained: 113400\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 89.0095238095238\n",
      "    ram_util_percent: 6.438095238095237\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 127173.0\n",
      "    agent-stage0_train1: 127173.0\n",
      "    agent-stage1_train0: 127173.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -36466.5\n",
      "    agent-stage0_train1: -36466.5\n",
      "    agent-stage1_train0: -36466.5\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -421404.0\n",
      "    agent-stage0_train1: -421404.0\n",
      "    agent-stage1_train0: -421404.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.44503704895965\n",
      "    mean_inference_ms: 1.534328035385087\n",
      "    mean_processing_ms: 0.36297006541714366\n",
      "  time_since_restore: 360.5437562465668\n",
      "  time_this_iter_s: 14.947332382202148\n",
      "  time_total_s: 360.5437562465668\n",
      "  timers:\n",
      "    learn_throughput: 601.204\n",
      "    learn_time_ms: 6985.985\n",
      "    load_throughput: 297034.003\n",
      "    load_time_ms: 14.14\n",
      "    sample_throughput: 630.209\n",
      "    sample_time_ms: 6664.453\n",
      "    update_time_ms: 4.117\n",
      "  timestamp: 1608024710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 113400\n",
      "  training_iteration: 27\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         360.544</td><td style=\"text-align: right;\">113400</td><td style=\"text-align: right;\"> -109400</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-32-03\n",
      "  done: false\n",
      "  episode_len_mean: 502.38\n",
      "  episode_reward_max: 381519.0\n",
      "  episode_reward_mean: -148812.69\n",
      "  episode_reward_min: -1264212.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 204\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6744680404663086\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007300335448235273\n",
      "        model: {}\n",
      "        policy_loss: 0.005616647191345692\n",
      "        total_loss: 1053337984.0\n",
      "        vf_explained_var: 5.587935447692871e-09\n",
      "        vf_loss: 1053337984.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1417994499206543\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0072213090024888515\n",
      "        model: {}\n",
      "        policy_loss: 0.00037722010165452957\n",
      "        total_loss: 1046688832.0\n",
      "        vf_explained_var: 1.30385160446167e-08\n",
      "        vf_loss: 1046688832.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8097449541091919\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006697746459394693\n",
      "        model: {}\n",
      "        policy_loss: -0.0015979744493961334\n",
      "        total_loss: 1049124544.0\n",
      "        vf_explained_var: 2.421438694000244e-08\n",
      "        vf_loss: 1049124608.0\n",
      "    num_steps_sampled: 117600\n",
      "    num_steps_trained: 117600\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.66315789473683\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 127173.0\n",
      "    agent-stage0_train1: 127173.0\n",
      "    agent-stage1_train0: 127173.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -49604.23\n",
      "    agent-stage0_train1: -49604.23\n",
      "    agent-stage1_train0: -49604.23\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -421404.0\n",
      "    agent-stage0_train1: -421404.0\n",
      "    agent-stage1_train0: -421404.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.451180691530454\n",
      "    mean_inference_ms: 1.5352007924500064\n",
      "    mean_processing_ms: 0.36310657239503363\n",
      "  time_since_restore: 373.6831283569336\n",
      "  time_this_iter_s: 13.139372110366821\n",
      "  time_total_s: 373.6831283569336\n",
      "  timers:\n",
      "    learn_throughput: 599.653\n",
      "    learn_time_ms: 7004.054\n",
      "    load_throughput: 288576.206\n",
      "    load_time_ms: 14.554\n",
      "    sample_throughput: 630.856\n",
      "    sample_time_ms: 6657.624\n",
      "    update_time_ms: 4.159\n",
      "  timestamp: 1608024723\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 117600\n",
      "  training_iteration: 28\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         373.683</td><td style=\"text-align: right;\">117600</td><td style=\"text-align: right;\"> -148813</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-32-17\n",
      "  done: false\n",
      "  episode_len_mean: 506.56\n",
      "  episode_reward_max: 381519.0\n",
      "  episode_reward_mean: -176913.87\n",
      "  episode_reward_min: -1264212.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 213\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.838301420211792\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008049704134464264\n",
      "        model: {}\n",
      "        policy_loss: 0.0010860953480005264\n",
      "        total_loss: 165438000.0\n",
      "        vf_explained_var: 1.862645149230957e-09\n",
      "        vf_loss: 165438000.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1621077060699463\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012017675675451756\n",
      "        model: {}\n",
      "        policy_loss: 0.002322722226381302\n",
      "        total_loss: 166372800.0\n",
      "        vf_explained_var: -2.0489096641540527e-08\n",
      "        vf_loss: 166372800.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8378992080688477\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006942362058907747\n",
      "        model: {}\n",
      "        policy_loss: 0.0013923859223723412\n",
      "        total_loss: 164651520.0\n",
      "        vf_explained_var: 7.450580596923828e-09\n",
      "        vf_loss: 164651520.0\n",
      "    num_steps_sampled: 121800\n",
      "    num_steps_trained: 121800\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.89473684210526\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 127173.0\n",
      "    agent-stage0_train1: 127173.0\n",
      "    agent-stage1_train0: 127173.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -58971.29\n",
      "    agent-stage0_train1: -58971.29\n",
      "    agent-stage1_train0: -58971.29\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -421404.0\n",
      "    agent-stage0_train1: -421404.0\n",
      "    agent-stage1_train0: -421404.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.459364982955509\n",
      "    mean_inference_ms: 1.5364663116112274\n",
      "    mean_processing_ms: 0.36324991697474024\n",
      "  time_since_restore: 387.2130982875824\n",
      "  time_this_iter_s: 13.529969930648804\n",
      "  time_total_s: 387.2130982875824\n",
      "  timers:\n",
      "    learn_throughput: 597.686\n",
      "    learn_time_ms: 7027.106\n",
      "    load_throughput: 288332.012\n",
      "    load_time_ms: 14.567\n",
      "    sample_throughput: 629.771\n",
      "    sample_time_ms: 6669.095\n",
      "    update_time_ms: 4.191\n",
      "  timestamp: 1608024737\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 121800\n",
      "  training_iteration: 29\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         387.213</td><td style=\"text-align: right;\">121800</td><td style=\"text-align: right;\"> -176914</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-32-31\n",
      "  done: false\n",
      "  episode_len_mean: 513.09\n",
      "  episode_reward_max: 381519.0\n",
      "  episode_reward_mean: -184188.42\n",
      "  episode_reward_min: -1264212.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 219\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8451752066612244\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011233896017074585\n",
      "        model: {}\n",
      "        policy_loss: 0.008418424986302853\n",
      "        total_loss: 60551056.0\n",
      "        vf_explained_var: -9.313225746154785e-09\n",
      "        vf_loss: 60551056.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1578483581542969\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.024608202278614044\n",
      "        model: {}\n",
      "        policy_loss: 0.0031107570976018906\n",
      "        total_loss: 60979452.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 60979452.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8381088376045227\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006178743205964565\n",
      "        model: {}\n",
      "        policy_loss: 0.0002954998053610325\n",
      "        total_loss: 60875676.0\n",
      "        vf_explained_var: 1.1175870895385742e-08\n",
      "        vf_loss: 60875672.0\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.83\n",
      "    ram_util_percent: 6.449999999999998\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 127173.0\n",
      "    agent-stage0_train1: 127173.0\n",
      "    agent-stage1_train0: 127173.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -61396.14\n",
      "    agent-stage0_train1: -61396.14\n",
      "    agent-stage1_train0: -61396.14\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -421404.0\n",
      "    agent-stage0_train1: -421404.0\n",
      "    agent-stage1_train0: -421404.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.4639227454014048\n",
      "    mean_inference_ms: 1.5372889737342144\n",
      "    mean_processing_ms: 0.3633593800168402\n",
      "  time_since_restore: 401.1595597267151\n",
      "  time_this_iter_s: 13.94646143913269\n",
      "  time_total_s: 401.1595597267151\n",
      "  timers:\n",
      "    learn_throughput: 591.781\n",
      "    learn_time_ms: 7097.218\n",
      "    load_throughput: 285106.992\n",
      "    load_time_ms: 14.731\n",
      "    sample_throughput: 628.281\n",
      "    sample_time_ms: 6684.912\n",
      "    update_time_ms: 4.224\n",
      "  timestamp: 1608024751\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 30\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">          401.16</td><td style=\"text-align: right;\">126000</td><td style=\"text-align: right;\"> -184188</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-32-44\n",
      "  done: false\n",
      "  episode_len_mean: 520.33\n",
      "  episode_reward_max: 381519.0\n",
      "  episode_reward_mean: -194325.09\n",
      "  episode_reward_min: -1264212.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 225\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.7390443682670593\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009094415232539177\n",
      "        model: {}\n",
      "        policy_loss: 0.004232407547533512\n",
      "        total_loss: 52978720.0\n",
      "        vf_explained_var: 1.6763806343078613e-08\n",
      "        vf_loss: 52978720.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1707606315612793\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011822761967778206\n",
      "        model: {}\n",
      "        policy_loss: -5.912035703659058e-06\n",
      "        total_loss: 53086944.0\n",
      "        vf_explained_var: -1.6763806343078613e-08\n",
      "        vf_loss: 53086944.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8073002099990845\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00564244668930769\n",
      "        model: {}\n",
      "        policy_loss: 0.0028404504992067814\n",
      "        total_loss: 53124480.0\n",
      "        vf_explained_var: -1.30385160446167e-08\n",
      "        vf_loss: 53124480.0\n",
      "    num_steps_sampled: 130200\n",
      "    num_steps_trained: 130200\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.09473684210526\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 127173.0\n",
      "    agent-stage0_train1: 127173.0\n",
      "    agent-stage1_train0: 127173.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -64775.03\n",
      "    agent-stage0_train1: -64775.03\n",
      "    agent-stage1_train0: -64775.03\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -421404.0\n",
      "    agent-stage0_train1: -421404.0\n",
      "    agent-stage1_train0: -421404.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.468442596037482\n",
      "    mean_inference_ms: 1.5383113983414154\n",
      "    mean_processing_ms: 0.3634435539475093\n",
      "  time_since_restore: 414.0216917991638\n",
      "  time_this_iter_s: 12.86213207244873\n",
      "  time_total_s: 414.0216917991638\n",
      "  timers:\n",
      "    learn_throughput: 593.595\n",
      "    learn_time_ms: 7075.53\n",
      "    load_throughput: 287516.473\n",
      "    load_time_ms: 14.608\n",
      "    sample_throughput: 629.977\n",
      "    sample_time_ms: 6666.915\n",
      "    update_time_ms: 4.358\n",
      "  timestamp: 1608024764\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 130200\n",
      "  training_iteration: 31\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         414.022</td><td style=\"text-align: right;\">130200</td><td style=\"text-align: right;\"> -194325</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-32-57\n",
      "  done: false\n",
      "  episode_len_mean: 528.48\n",
      "  episode_reward_max: 365049.0\n",
      "  episode_reward_mean: -193336.86\n",
      "  episode_reward_min: -1264212.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 232\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.7507752180099487\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0034549294505268335\n",
      "        model: {}\n",
      "        policy_loss: 0.0014220895245671272\n",
      "        total_loss: 89437296.0\n",
      "        vf_explained_var: 1.862645149230957e-09\n",
      "        vf_loss: 89437296.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1543588638305664\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00879085436463356\n",
      "        model: {}\n",
      "        policy_loss: 0.0020088497549295425\n",
      "        total_loss: 89867984.0\n",
      "        vf_explained_var: -3.725290298461914e-09\n",
      "        vf_loss: 89867984.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8138747811317444\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006783574819564819\n",
      "        model: {}\n",
      "        policy_loss: 0.001463484950363636\n",
      "        total_loss: 89608480.0\n",
      "        vf_explained_var: 7.450580596923828e-09\n",
      "        vf_loss: 89608480.0\n",
      "    num_steps_sampled: 134400\n",
      "    num_steps_trained: 134400\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.31666666666666\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 121683.0\n",
      "    agent-stage0_train1: 121683.0\n",
      "    agent-stage1_train0: 121683.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -64445.62\n",
      "    agent-stage0_train1: -64445.62\n",
      "    agent-stage1_train0: -64445.62\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -421404.0\n",
      "    agent-stage0_train1: -421404.0\n",
      "    agent-stage1_train0: -421404.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.473487624425898\n",
      "    mean_inference_ms: 1.5395174913025356\n",
      "    mean_processing_ms: 0.36351259995512647\n",
      "  time_since_restore: 426.82907462120056\n",
      "  time_this_iter_s: 12.807382822036743\n",
      "  time_total_s: 426.82907462120056\n",
      "  timers:\n",
      "    learn_throughput: 596.059\n",
      "    learn_time_ms: 7046.277\n",
      "    load_throughput: 293102.03\n",
      "    load_time_ms: 14.329\n",
      "    sample_throughput: 632.345\n",
      "    sample_time_ms: 6641.947\n",
      "    update_time_ms: 4.332\n",
      "  timestamp: 1608024777\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 134400\n",
      "  training_iteration: 32\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         426.829</td><td style=\"text-align: right;\">134400</td><td style=\"text-align: right;\"> -193337</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-33-10\n",
      "  done: false\n",
      "  episode_len_mean: 534.47\n",
      "  episode_reward_max: 367467.0\n",
      "  episode_reward_mean: -182137.65\n",
      "  episode_reward_min: -1264212.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 240\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.7220034599304199\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01923440583050251\n",
      "        model: {}\n",
      "        policy_loss: 0.009240644983947277\n",
      "        total_loss: 89880736.0\n",
      "        vf_explained_var: -1.6763806343078613e-08\n",
      "        vf_loss: 89880736.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1706637144088745\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007139204535633326\n",
      "        model: {}\n",
      "        policy_loss: 0.0005904883146286011\n",
      "        total_loss: 90474536.0\n",
      "        vf_explained_var: -1.30385160446167e-08\n",
      "        vf_loss: 90474536.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.7354557514190674\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006449956446886063\n",
      "        model: {}\n",
      "        policy_loss: 0.0010240273550152779\n",
      "        total_loss: 90375840.0\n",
      "        vf_explained_var: -1.6763806343078613e-08\n",
      "        vf_loss: 90375840.0\n",
      "    num_steps_sampled: 138600\n",
      "    num_steps_trained: 138600\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.74736842105264\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 122489.0\n",
      "    agent-stage0_train1: 122489.0\n",
      "    agent-stage1_train0: 122489.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -60712.55\n",
      "    agent-stage0_train1: -60712.55\n",
      "    agent-stage1_train0: -60712.55\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -421404.0\n",
      "    agent-stage0_train1: -421404.0\n",
      "    agent-stage1_train0: -421404.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.4783980608078284\n",
      "    mean_inference_ms: 1.5406675452310912\n",
      "    mean_processing_ms: 0.36364345253335395\n",
      "  time_since_restore: 439.7716226577759\n",
      "  time_this_iter_s: 12.942548036575317\n",
      "  time_total_s: 439.7716226577759\n",
      "  timers:\n",
      "    learn_throughput: 600.803\n",
      "    learn_time_ms: 6990.643\n",
      "    load_throughput: 289586.644\n",
      "    load_time_ms: 14.503\n",
      "    sample_throughput: 634.936\n",
      "    sample_time_ms: 6614.837\n",
      "    update_time_ms: 4.318\n",
      "  timestamp: 1608024790\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 138600\n",
      "  training_iteration: 33\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         439.772</td><td style=\"text-align: right;\">138600</td><td style=\"text-align: right;\"> -182138</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-33-23\n",
      "  done: false\n",
      "  episode_len_mean: 537.68\n",
      "  episode_reward_max: 367467.0\n",
      "  episode_reward_mean: -197379.03\n",
      "  episode_reward_min: -1264212.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 247\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5936481952667236\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023666122928261757\n",
      "        model: {}\n",
      "        policy_loss: 0.01451393123716116\n",
      "        total_loss: 311310912.0\n",
      "        vf_explained_var: 2.60770320892334e-08\n",
      "        vf_loss: 311310912.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1756200790405273\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009924471378326416\n",
      "        model: {}\n",
      "        policy_loss: -0.003436119295656681\n",
      "        total_loss: 305320864.0\n",
      "        vf_explained_var: -1.30385160446167e-08\n",
      "        vf_loss: 305320864.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6944581866264343\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015281908214092255\n",
      "        model: {}\n",
      "        policy_loss: -0.002669256180524826\n",
      "        total_loss: 306231488.0\n",
      "        vf_explained_var: -1.862645149230957e-09\n",
      "        vf_loss: 306231488.0\n",
      "    num_steps_sampled: 142800\n",
      "    num_steps_trained: 142800\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.93888888888888\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 122489.0\n",
      "    agent-stage0_train1: 122489.0\n",
      "    agent-stage1_train0: 122489.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -65793.01\n",
      "    agent-stage0_train1: -65793.01\n",
      "    agent-stage1_train0: -65793.01\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -421404.0\n",
      "    agent-stage0_train1: -421404.0\n",
      "    agent-stage1_train0: -421404.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.4822963589907294\n",
      "    mean_inference_ms: 1.541685001167316\n",
      "    mean_processing_ms: 0.36372868975731476\n",
      "  time_since_restore: 452.73538541793823\n",
      "  time_this_iter_s: 12.963762760162354\n",
      "  time_total_s: 452.73538541793823\n",
      "  timers:\n",
      "    learn_throughput: 605.684\n",
      "    learn_time_ms: 6934.313\n",
      "    load_throughput: 291216.219\n",
      "    load_time_ms: 14.422\n",
      "    sample_throughput: 643.054\n",
      "    sample_time_ms: 6531.335\n",
      "    update_time_ms: 5.289\n",
      "  timestamp: 1608024803\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 142800\n",
      "  training_iteration: 34\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         452.735</td><td style=\"text-align: right;\">142800</td><td style=\"text-align: right;\"> -197379</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-33-36\n",
      "  done: false\n",
      "  episode_len_mean: 543.73\n",
      "  episode_reward_max: 367467.0\n",
      "  episode_reward_mean: -228582.0\n",
      "  episode_reward_min: -1264212.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 255\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.37968748807907104\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5228270292282104\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008963287808001041\n",
      "        model: {}\n",
      "        policy_loss: 0.0033739260397851467\n",
      "        total_loss: 299068320.0\n",
      "        vf_explained_var: -9.313225746154785e-09\n",
      "        vf_loss: 299068320.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.172200322151184\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008629901334643364\n",
      "        model: {}\n",
      "        policy_loss: -0.0006459730211645365\n",
      "        total_loss: 299691200.0\n",
      "        vf_explained_var: -9.313225746154785e-09\n",
      "        vf_loss: 299691200.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6729048490524292\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006426239386200905\n",
      "        model: {}\n",
      "        policy_loss: -0.00030049076303839684\n",
      "        total_loss: 298876352.0\n",
      "        vf_explained_var: 1.4901161193847656e-08\n",
      "        vf_loss: 298876352.0\n",
      "    num_steps_sampled: 147000\n",
      "    num_steps_trained: 147000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.13\n",
      "    ram_util_percent: 6.215\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 122489.0\n",
      "    agent-stage0_train1: 122489.0\n",
      "    agent-stage1_train0: 122489.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -76194.0\n",
      "    agent-stage0_train1: -76194.0\n",
      "    agent-stage1_train0: -76194.0\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -421404.0\n",
      "    agent-stage0_train1: -421404.0\n",
      "    agent-stage1_train0: -421404.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.4862233103399807\n",
      "    mean_inference_ms: 1.5427280735561972\n",
      "    mean_processing_ms: 0.36384366465291407\n",
      "  time_since_restore: 466.1401832103729\n",
      "  time_this_iter_s: 13.404797792434692\n",
      "  time_total_s: 466.1401832103729\n",
      "  timers:\n",
      "    learn_throughput: 603.94\n",
      "    learn_time_ms: 6954.328\n",
      "    load_throughput: 290783.107\n",
      "    load_time_ms: 14.444\n",
      "    sample_throughput: 647.404\n",
      "    sample_time_ms: 6487.448\n",
      "    update_time_ms: 5.344\n",
      "  timestamp: 1608024816\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 147000\n",
      "  training_iteration: 35\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.5/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">          466.14</td><td style=\"text-align: right;\">147000</td><td style=\"text-align: right;\"> -228582</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-33-51\n",
      "  done: false\n",
      "  episode_len_mean: 547.03\n",
      "  episode_reward_max: 367467.0\n",
      "  episode_reward_mean: -253478.1\n",
      "  episode_reward_min: -1264212.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 263\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.37968748807907104\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.4513266682624817\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00895717367529869\n",
      "        model: {}\n",
      "        policy_loss: 0.00983350072056055\n",
      "        total_loss: 317265696.0\n",
      "        vf_explained_var: -2.0489096641540527e-08\n",
      "        vf_loss: 317265696.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1654186248779297\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01223238930106163\n",
      "        model: {}\n",
      "        policy_loss: -0.0011208131909370422\n",
      "        total_loss: 314413248.0\n",
      "        vf_explained_var: -9.313225746154785e-09\n",
      "        vf_loss: 314413248.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6625018119812012\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011473813094198704\n",
      "        model: {}\n",
      "        policy_loss: 0.003608935046941042\n",
      "        total_loss: 316534016.0\n",
      "        vf_explained_var: 7.450580596923828e-09\n",
      "        vf_loss: 316534016.0\n",
      "    num_steps_sampled: 151200\n",
      "    num_steps_trained: 151200\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.5952380952381\n",
      "    ram_util_percent: 6.428571428571429\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 122489.0\n",
      "    agent-stage0_train1: 122489.0\n",
      "    agent-stage1_train0: 122489.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -84492.7\n",
      "    agent-stage0_train1: -84492.7\n",
      "    agent-stage1_train0: -84492.7\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -421404.0\n",
      "    agent-stage0_train1: -421404.0\n",
      "    agent-stage1_train0: -421404.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.490244434204818\n",
      "    mean_inference_ms: 1.544334680840318\n",
      "    mean_processing_ms: 0.36399189530432524\n",
      "  time_since_restore: 480.89853978157043\n",
      "  time_this_iter_s: 14.75835657119751\n",
      "  time_total_s: 480.89853978157043\n",
      "  timers:\n",
      "    learn_throughput: 599.529\n",
      "    learn_time_ms: 7005.504\n",
      "    load_throughput: 289669.499\n",
      "    load_time_ms: 14.499\n",
      "    sample_throughput: 647.022\n",
      "    sample_time_ms: 6491.283\n",
      "    update_time_ms: 5.336\n",
      "  timestamp: 1608024831\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 151200\n",
      "  training_iteration: 36\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         480.899</td><td style=\"text-align: right;\">151200</td><td style=\"text-align: right;\"> -253478</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-34-04\n",
      "  done: false\n",
      "  episode_len_mean: 553.84\n",
      "  episode_reward_max: 367467.0\n",
      "  episode_reward_mean: -240243.69\n",
      "  episode_reward_min: -1264212.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 270\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.37968748807907104\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5049195289611816\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009103003889322281\n",
      "        model: {}\n",
      "        policy_loss: 0.0050999755039811134\n",
      "        total_loss: 130897296.0\n",
      "        vf_explained_var: -2.60770320892334e-08\n",
      "        vf_loss: 130897296.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1983776092529297\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004244795069098473\n",
      "        model: {}\n",
      "        policy_loss: -0.00040359562262892723\n",
      "        total_loss: 130986192.0\n",
      "        vf_explained_var: 1.30385160446167e-08\n",
      "        vf_loss: 130986192.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.7340298891067505\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011175833642482758\n",
      "        model: {}\n",
      "        policy_loss: -0.005793082993477583\n",
      "        total_loss: 130380400.0\n",
      "        vf_explained_var: -1.1175870895385742e-08\n",
      "        vf_loss: 130380400.0\n",
      "    num_steps_sampled: 155400\n",
      "    num_steps_trained: 155400\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.80555555555556\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 122489.0\n",
      "    agent-stage0_train1: 122489.0\n",
      "    agent-stage1_train0: 122489.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -80081.23\n",
      "    agent-stage0_train1: -80081.23\n",
      "    agent-stage1_train0: -80081.23\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -421404.0\n",
      "    agent-stage0_train1: -421404.0\n",
      "    agent-stage1_train0: -421404.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.492343641677361\n",
      "    mean_inference_ms: 1.544920300957413\n",
      "    mean_processing_ms: 0.36403785470602706\n",
      "  time_since_restore: 493.69272351264954\n",
      "  time_this_iter_s: 12.794183731079102\n",
      "  time_total_s: 493.69272351264954\n",
      "  timers:\n",
      "    learn_throughput: 614.127\n",
      "    learn_time_ms: 6838.977\n",
      "    load_throughput: 290828.233\n",
      "    load_time_ms: 14.442\n",
      "    sample_throughput: 651.896\n",
      "    sample_time_ms: 6442.745\n",
      "    update_time_ms: 5.326\n",
      "  timestamp: 1608024844\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 155400\n",
      "  training_iteration: 37\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         493.693</td><td style=\"text-align: right;\">155400</td><td style=\"text-align: right;\"> -240244</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-34-17\n",
      "  done: false\n",
      "  episode_len_mean: 561.67\n",
      "  episode_reward_max: 367467.0\n",
      "  episode_reward_mean: -229931.73\n",
      "  episode_reward_min: -1264212.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 276\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.37968748807907104\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.3666011691093445\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0022515032906085253\n",
      "        model: {}\n",
      "        policy_loss: -0.0004920987412333488\n",
      "        total_loss: 286247040.0\n",
      "        vf_explained_var: 1.1175870895385742e-08\n",
      "        vf_loss: 286247040.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.7593749761581421\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1703400611877441\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009802810847759247\n",
      "        model: {}\n",
      "        policy_loss: 0.0017403047531843185\n",
      "        total_loss: 288182272.0\n",
      "        vf_explained_var: 7.450580596923828e-09\n",
      "        vf_loss: 288182272.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.7198498845100403\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005158711224794388\n",
      "        model: {}\n",
      "        policy_loss: 0.002895233454182744\n",
      "        total_loss: 287979552.0\n",
      "        vf_explained_var: 3.725290298461914e-09\n",
      "        vf_loss: 287979552.0\n",
      "    num_steps_sampled: 159600\n",
      "    num_steps_trained: 159600\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.67894736842105\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 122489.0\n",
      "    agent-stage0_train1: 122489.0\n",
      "    agent-stage1_train0: 122489.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -76643.91\n",
      "    agent-stage0_train1: -76643.91\n",
      "    agent-stage1_train0: -76643.91\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -421404.0\n",
      "    agent-stage0_train1: -421404.0\n",
      "    agent-stage1_train0: -421404.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.4936657966857436\n",
      "    mean_inference_ms: 1.5453977605716274\n",
      "    mean_processing_ms: 0.36402835974863573\n",
      "  time_since_restore: 506.61665868759155\n",
      "  time_this_iter_s: 12.923935174942017\n",
      "  time_total_s: 506.61665868759155\n",
      "  timers:\n",
      "    learn_throughput: 615.616\n",
      "    learn_time_ms: 6822.438\n",
      "    load_throughput: 296595.411\n",
      "    load_time_ms: 14.161\n",
      "    sample_throughput: 652.35\n",
      "    sample_time_ms: 6438.265\n",
      "    update_time_ms: 5.272\n",
      "  timestamp: 1608024857\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159600\n",
      "  training_iteration: 38\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         506.617</td><td style=\"text-align: right;\">159600</td><td style=\"text-align: right;\"> -229932</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-34-30\n",
      "  done: false\n",
      "  episode_len_mean: 567.15\n",
      "  episode_reward_max: 367467.0\n",
      "  episode_reward_mean: -223404.57\n",
      "  episode_reward_min: -1153008.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 285\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.18984374403953552\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.3154633939266205\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014938590116798878\n",
      "        model: {}\n",
      "        policy_loss: 0.003301195800304413\n",
      "        total_loss: 1030447616.0\n",
      "        vf_explained_var: 5.587935447692871e-09\n",
      "        vf_loss: 1030447616.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.7593749761581421\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1732451915740967\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011451571248471737\n",
      "        model: {}\n",
      "        policy_loss: -0.004372055176645517\n",
      "        total_loss: 1025331840.0\n",
      "        vf_explained_var: -1.4901161193847656e-08\n",
      "        vf_loss: 1025331840.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6695748567581177\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.003309519262984395\n",
      "        model: {}\n",
      "        policy_loss: 0.000826764851808548\n",
      "        total_loss: 1031006080.0\n",
      "        vf_explained_var: -1.6763806343078613e-08\n",
      "        vf_loss: 1031006080.0\n",
      "    num_steps_sampled: 163800\n",
      "    num_steps_trained: 163800\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.68888888888888\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 122489.0\n",
      "    agent-stage0_train1: 122489.0\n",
      "    agent-stage1_train0: 122489.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -74468.19\n",
      "    agent-stage0_train1: -74468.19\n",
      "    agent-stage1_train0: -74468.19\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -384336.0\n",
      "    agent-stage0_train1: -384336.0\n",
      "    agent-stage1_train0: -384336.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.4953538816859013\n",
      "    mean_inference_ms: 1.545497010780777\n",
      "    mean_processing_ms: 0.36400665386892556\n",
      "  time_since_restore: 519.5086190700531\n",
      "  time_this_iter_s: 12.891960382461548\n",
      "  time_total_s: 519.5086190700531\n",
      "  timers:\n",
      "    learn_throughput: 620.808\n",
      "    learn_time_ms: 6765.382\n",
      "    load_throughput: 297461.839\n",
      "    load_time_ms: 14.119\n",
      "    sample_throughput: 652.994\n",
      "    sample_time_ms: 6431.912\n",
      "    update_time_ms: 5.264\n",
      "  timestamp: 1608024870\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 163800\n",
      "  training_iteration: 39\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         519.509</td><td style=\"text-align: right;\">163800</td><td style=\"text-align: right;\"> -223405</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-34-43\n",
      "  done: false\n",
      "  episode_len_mean: 568.32\n",
      "  episode_reward_max: 367467.0\n",
      "  episode_reward_mean: -189224.94\n",
      "  episode_reward_min: -950982.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 294\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.18984374403953552\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.2854801416397095\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04292096942663193\n",
      "        model: {}\n",
      "        policy_loss: 0.005894351750612259\n",
      "        total_loss: 937011904.0\n",
      "        vf_explained_var: 1.30385160446167e-08\n",
      "        vf_loss: 937011904.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.7593749761581421\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.147750735282898\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014350459910929203\n",
      "        model: {}\n",
      "        policy_loss: -0.0015712641179561615\n",
      "        total_loss: 934084480.0\n",
      "        vf_explained_var: -9.313225746154785e-09\n",
      "        vf_loss: 934084480.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6435008645057678\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011438321322202682\n",
      "        model: {}\n",
      "        policy_loss: 0.004556030035018921\n",
      "        total_loss: 935132224.0\n",
      "        vf_explained_var: -7.450580596923828e-09\n",
      "        vf_loss: 935132224.0\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.19473684210526\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 122489.0\n",
      "    agent-stage0_train1: 122489.0\n",
      "    agent-stage1_train0: 122489.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -63074.98\n",
      "    agent-stage0_train1: -63074.98\n",
      "    agent-stage1_train0: -63074.98\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -316994.0\n",
      "    agent-stage0_train1: -316994.0\n",
      "    agent-stage1_train0: -316994.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.4965718056031627\n",
      "    mean_inference_ms: 1.5451949871736397\n",
      "    mean_processing_ms: 0.3639319213877215\n",
      "  time_since_restore: 532.5431928634644\n",
      "  time_this_iter_s: 13.034573793411255\n",
      "  time_total_s: 532.5431928634644\n",
      "  timers:\n",
      "    learn_throughput: 627.768\n",
      "    learn_time_ms: 6690.374\n",
      "    load_throughput: 298632.744\n",
      "    load_time_ms: 14.064\n",
      "    sample_throughput: 654.638\n",
      "    sample_time_ms: 6415.762\n",
      "    update_time_ms: 5.262\n",
      "  timestamp: 1608024883\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 40\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         532.543</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\"> -189225</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-34-56\n",
      "  done: false\n",
      "  episode_len_mean: 567.86\n",
      "  episode_reward_max: 367467.0\n",
      "  episode_reward_mean: -168381.72\n",
      "  episode_reward_min: -950982.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 300\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.2847656309604645\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.35558995604515076\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006909715943038464\n",
      "        model: {}\n",
      "        policy_loss: 0.0011941799893975258\n",
      "        total_loss: 460347520.0\n",
      "        vf_explained_var: -9.313225746154785e-09\n",
      "        vf_loss: 460347520.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.7593749761581421\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1919538974761963\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008156543597579002\n",
      "        model: {}\n",
      "        policy_loss: 0.0009899251163005829\n",
      "        total_loss: 457858304.0\n",
      "        vf_explained_var: 5.587935447692871e-09\n",
      "        vf_loss: 457858304.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6634359359741211\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012351036071777344\n",
      "        model: {}\n",
      "        policy_loss: 0.0049896035343408585\n",
      "        total_loss: 457504064.0\n",
      "        vf_explained_var: -1.1175870895385742e-08\n",
      "        vf_loss: 457504064.0\n",
      "    num_steps_sampled: 172200\n",
      "    num_steps_trained: 172200\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.10555555555555\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 122489.0\n",
      "    agent-stage0_train1: 122489.0\n",
      "    agent-stage1_train0: 122489.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -56127.24\n",
      "    agent-stage0_train1: -56127.24\n",
      "    agent-stage1_train0: -56127.24\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -316994.0\n",
      "    agent-stage0_train1: -316994.0\n",
      "    agent-stage1_train0: -316994.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.497403101293076\n",
      "    mean_inference_ms: 1.5450007157934826\n",
      "    mean_processing_ms: 0.36386565363860107\n",
      "  time_since_restore: 545.4305884838104\n",
      "  time_this_iter_s: 12.88739562034607\n",
      "  time_total_s: 545.4305884838104\n",
      "  timers:\n",
      "    learn_throughput: 628.441\n",
      "    learn_time_ms: 6683.202\n",
      "    load_throughput: 296983.927\n",
      "    load_time_ms: 14.142\n",
      "    sample_throughput: 653.588\n",
      "    sample_time_ms: 6426.066\n",
      "    update_time_ms: 5.152\n",
      "  timestamp: 1608024896\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172200\n",
      "  training_iteration: 41\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         545.431</td><td style=\"text-align: right;\">172200</td><td style=\"text-align: right;\"> -168382</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-35-09\n",
      "  done: false\n",
      "  episode_len_mean: 570.3\n",
      "  episode_reward_max: 367467.0\n",
      "  episode_reward_mean: -167873.97\n",
      "  episode_reward_min: -790236.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 309\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.2847656309604645\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.2644461989402771\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.002563083078712225\n",
      "        model: {}\n",
      "        policy_loss: -0.0006207031547091901\n",
      "        total_loss: 498126144.0\n",
      "        vf_explained_var: -7.450580596923828e-09\n",
      "        vf_loss: 498126144.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.7593749761581421\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.161379337310791\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012293214909732342\n",
      "        model: {}\n",
      "        policy_loss: 0.0032613417133688927\n",
      "        total_loss: 498450976.0\n",
      "        vf_explained_var: -3.725290298461914e-09\n",
      "        vf_loss: 498450976.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.7765203714370728\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013261908665299416\n",
      "        model: {}\n",
      "        policy_loss: -0.007495033089071512\n",
      "        total_loss: 495573632.0\n",
      "        vf_explained_var: -3.725290298461914e-09\n",
      "        vf_loss: 495573632.0\n",
      "    num_steps_sampled: 176400\n",
      "    num_steps_trained: 176400\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.3263157894737\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 122489.0\n",
      "    agent-stage0_train1: 122489.0\n",
      "    agent-stage1_train0: 122489.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -55957.99\n",
      "    agent-stage0_train1: -55957.99\n",
      "    agent-stage1_train0: -55957.99\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -263412.0\n",
      "    agent-stage0_train1: -263412.0\n",
      "    agent-stage1_train0: -263412.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.4978487175594015\n",
      "    mean_inference_ms: 1.5444862564749087\n",
      "    mean_processing_ms: 0.36376771774468397\n",
      "  time_since_restore: 558.3077256679535\n",
      "  time_this_iter_s: 12.877137184143066\n",
      "  time_total_s: 558.3077256679535\n",
      "  timers:\n",
      "    learn_throughput: 627.522\n",
      "    learn_time_ms: 6692.987\n",
      "    load_throughput: 294918.089\n",
      "    load_time_ms: 14.241\n",
      "    sample_throughput: 653.894\n",
      "    sample_time_ms: 6423.055\n",
      "    update_time_ms: 5.257\n",
      "  timestamp: 1608024909\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176400\n",
      "  training_iteration: 42\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         558.308</td><td style=\"text-align: right;\">176400</td><td style=\"text-align: right;\"> -167874</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-35-22\n",
      "  done: false\n",
      "  episode_len_mean: 568.86\n",
      "  episode_reward_max: 367467.0\n",
      "  episode_reward_mean: -198865.02\n",
      "  episode_reward_min: -790236.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 315\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.14238281548023224\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.25541844964027405\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0032194440718740225\n",
      "        model: {}\n",
      "        policy_loss: 0.0005071591585874557\n",
      "        total_loss: 414420896.0\n",
      "        vf_explained_var: 1.862645149230957e-09\n",
      "        vf_loss: 414420896.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.7593749761581421\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1995670795440674\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0274873785674572\n",
      "        model: {}\n",
      "        policy_loss: 0.0045428466983139515\n",
      "        total_loss: 417121536.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 417121536.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6983751058578491\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012920528650283813\n",
      "        model: {}\n",
      "        policy_loss: -0.0037405239418148994\n",
      "        total_loss: 414961184.0\n",
      "        vf_explained_var: -1.862645149230957e-09\n",
      "        vf_loss: 414961184.0\n",
      "    num_steps_sampled: 180600\n",
      "    num_steps_trained: 180600\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.22777777777779\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 122489.0\n",
      "    agent-stage0_train1: 122489.0\n",
      "    agent-stage1_train0: 122489.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -66288.34\n",
      "    agent-stage0_train1: -66288.34\n",
      "    agent-stage1_train0: -66288.34\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -263412.0\n",
      "    agent-stage0_train1: -263412.0\n",
      "    agent-stage1_train0: -263412.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.498463358972441\n",
      "    mean_inference_ms: 1.5442549694774266\n",
      "    mean_processing_ms: 0.36369537542488317\n",
      "  time_since_restore: 571.1283524036407\n",
      "  time_this_iter_s: 12.820626735687256\n",
      "  time_total_s: 571.1283524036407\n",
      "  timers:\n",
      "    learn_throughput: 629.301\n",
      "    learn_time_ms: 6674.071\n",
      "    load_throughput: 298811.558\n",
      "    load_time_ms: 14.056\n",
      "    sample_throughput: 653.202\n",
      "    sample_time_ms: 6429.866\n",
      "    update_time_ms: 5.477\n",
      "  timestamp: 1608024922\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180600\n",
      "  training_iteration: 43\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         571.128</td><td style=\"text-align: right;\">180600</td><td style=\"text-align: right;\"> -198865</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-35-35\n",
      "  done: false\n",
      "  episode_len_mean: 565.13\n",
      "  episode_reward_max: 367467.0\n",
      "  episode_reward_mean: -252462.21\n",
      "  episode_reward_min: -1101903.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 322\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.07119140774011612\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.27512305974960327\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0037526553496718407\n",
      "        model: {}\n",
      "        policy_loss: -0.0024413233622908592\n",
      "        total_loss: 1186223360.0\n",
      "        vf_explained_var: -5.587935447692871e-09\n",
      "        vf_loss: 1186223360.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.224684715270996\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00776016665622592\n",
      "        model: {}\n",
      "        policy_loss: 0.005173771642148495\n",
      "        total_loss: 1189539072.0\n",
      "        vf_explained_var: -1.4901161193847656e-08\n",
      "        vf_loss: 1189539072.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6843252778053284\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012212424539029598\n",
      "        model: {}\n",
      "        policy_loss: -0.00021124654449522495\n",
      "        total_loss: 1189160192.0\n",
      "        vf_explained_var: 5.587935447692871e-09\n",
      "        vf_loss: 1189160192.0\n",
      "    num_steps_sampled: 184800\n",
      "    num_steps_trained: 184800\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.06315789473685\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 122489.0\n",
      "    agent-stage0_train1: 122489.0\n",
      "    agent-stage1_train0: 122489.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -84154.07\n",
      "    agent-stage0_train1: -84154.07\n",
      "    agent-stage1_train0: -84154.07\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -367301.0\n",
      "    agent-stage0_train1: -367301.0\n",
      "    agent-stage1_train0: -367301.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.4988711193088267\n",
      "    mean_inference_ms: 1.5437316839083528\n",
      "    mean_processing_ms: 0.3636015516287219\n",
      "  time_since_restore: 583.9670166969299\n",
      "  time_this_iter_s: 12.838664293289185\n",
      "  time_total_s: 583.9670166969299\n",
      "  timers:\n",
      "    learn_throughput: 629.564\n",
      "    learn_time_ms: 6671.282\n",
      "    load_throughput: 300940.723\n",
      "    load_time_ms: 13.956\n",
      "    sample_throughput: 654.06\n",
      "    sample_time_ms: 6421.43\n",
      "    update_time_ms: 4.478\n",
      "  timestamp: 1608024935\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184800\n",
      "  training_iteration: 44\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         583.967</td><td style=\"text-align: right;\">184800</td><td style=\"text-align: right;\"> -252462</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-35-48\n",
      "  done: false\n",
      "  episode_len_mean: 561.61\n",
      "  episode_reward_max: 367467.0\n",
      "  episode_reward_mean: -317804.43\n",
      "  episode_reward_min: -1101903.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 330\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.03559570387005806\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.43674764037132263\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.024540605023503304\n",
      "        model: {}\n",
      "        policy_loss: 0.008261153474450111\n",
      "        total_loss: 583589440.0\n",
      "        vf_explained_var: 2.2351741790771484e-08\n",
      "        vf_loss: 583589440.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.238977313041687\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008289754390716553\n",
      "        model: {}\n",
      "        policy_loss: 0.006602891255170107\n",
      "        total_loss: 585291648.0\n",
      "        vf_explained_var: -9.313225746154785e-09\n",
      "        vf_loss: 585291648.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.7190282344818115\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01399002131074667\n",
      "        model: {}\n",
      "        policy_loss: 0.002527912613004446\n",
      "        total_loss: 580752384.0\n",
      "        vf_explained_var: -5.587935447692871e-09\n",
      "        vf_loss: 580752384.0\n",
      "    num_steps_sampled: 189000\n",
      "    num_steps_trained: 189000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.4388888888889\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 122489.0\n",
      "    agent-stage0_train1: 122489.0\n",
      "    agent-stage1_train0: 122489.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -105934.81\n",
      "    agent-stage0_train1: -105934.81\n",
      "    agent-stage1_train0: -105934.81\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -367301.0\n",
      "    agent-stage0_train1: -367301.0\n",
      "    agent-stage1_train0: -367301.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.499609199116673\n",
      "    mean_inference_ms: 1.5432805523662012\n",
      "    mean_processing_ms: 0.36355723427883846\n",
      "  time_since_restore: 597.0339124202728\n",
      "  time_this_iter_s: 13.066895723342896\n",
      "  time_total_s: 597.0339124202728\n",
      "  timers:\n",
      "    learn_throughput: 633.61\n",
      "    learn_time_ms: 6628.684\n",
      "    load_throughput: 300885.209\n",
      "    load_time_ms: 13.959\n",
      "    sample_throughput: 653.162\n",
      "    sample_time_ms: 6430.258\n",
      "    update_time_ms: 4.373\n",
      "  timestamp: 1608024948\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 189000\n",
      "  training_iteration: 45\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         597.034</td><td style=\"text-align: right;\">189000</td><td style=\"text-align: right;\"> -317804</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-36-01\n",
      "  done: false\n",
      "  episode_len_mean: 557.55\n",
      "  episode_reward_max: 318363.0\n",
      "  episode_reward_mean: -399525.15\n",
      "  episode_reward_min: -1189590.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 337\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.05339355394244194\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.14446097612380981\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.028607357293367386\n",
      "        model: {}\n",
      "        policy_loss: 0.010213583707809448\n",
      "        total_loss: 2423133696.0\n",
      "        vf_explained_var: -5.587935447692871e-09\n",
      "        vf_loss: 2423133696.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.2027487754821777\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009856005199253559\n",
      "        model: {}\n",
      "        policy_loss: 0.0053773787803947926\n",
      "        total_loss: 2421498368.0\n",
      "        vf_explained_var: -3.5390257835388184e-08\n",
      "        vf_loss: 2421498368.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6786453723907471\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006337325554341078\n",
      "        model: {}\n",
      "        policy_loss: -0.0034249378368258476\n",
      "        total_loss: 2411595776.0\n",
      "        vf_explained_var: 1.30385160446167e-08\n",
      "        vf_loss: 2411595776.0\n",
      "    num_steps_sampled: 193200\n",
      "    num_steps_trained: 193200\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.4736842105263\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 106121.0\n",
      "    agent-stage0_train1: 106121.0\n",
      "    agent-stage1_train0: 106121.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -133175.05\n",
      "    agent-stage0_train1: -133175.05\n",
      "    agent-stage1_train0: -133175.05\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -396530.0\n",
      "    agent-stage0_train1: -396530.0\n",
      "    agent-stage1_train0: -396530.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.500271978439819\n",
      "    mean_inference_ms: 1.5429702414045137\n",
      "    mean_processing_ms: 0.3634791411382529\n",
      "  time_since_restore: 609.9632818698883\n",
      "  time_this_iter_s: 12.929369449615479\n",
      "  time_total_s: 609.9632818698883\n",
      "  timers:\n",
      "    learn_throughput: 641.882\n",
      "    learn_time_ms: 6543.261\n",
      "    load_throughput: 302718.662\n",
      "    load_time_ms: 13.874\n",
      "    sample_throughput: 663.193\n",
      "    sample_time_ms: 6332.995\n",
      "    update_time_ms: 4.397\n",
      "  timestamp: 1608024961\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 193200\n",
      "  training_iteration: 46\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         609.963</td><td style=\"text-align: right;\">193200</td><td style=\"text-align: right;\"> -399525</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-36-14\n",
      "  done: false\n",
      "  episode_len_mean: 554.31\n",
      "  episode_reward_max: 24519.0\n",
      "  episode_reward_mean: -524816.46\n",
      "  episode_reward_min: -1792617.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 346\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.08009032905101776\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.14569607377052307\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.001749303424730897\n",
      "        model: {}\n",
      "        policy_loss: 0.0014964258298277855\n",
      "        total_loss: 4194099712.0\n",
      "        vf_explained_var: 1.862645149230957e-09\n",
      "        vf_loss: 4194099712.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1500391960144043\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010488754138350487\n",
      "        model: {}\n",
      "        policy_loss: -0.0019054543226957321\n",
      "        total_loss: 4182044160.0\n",
      "        vf_explained_var: -1.4901161193847656e-08\n",
      "        vf_loss: 4182044160.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6590400338172913\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009037713520228863\n",
      "        model: {}\n",
      "        policy_loss: 0.0011572185903787613\n",
      "        total_loss: 4184821760.0\n",
      "        vf_explained_var: 5.587935447692871e-09\n",
      "        vf_loss: 4184821760.0\n",
      "    num_steps_sampled: 197400\n",
      "    num_steps_trained: 197400\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.22105263157896\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 8173.0\n",
      "    agent-stage0_train1: 8173.0\n",
      "    agent-stage1_train0: 8173.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -174938.82\n",
      "    agent-stage0_train1: -174938.82\n",
      "    agent-stage1_train0: -174938.82\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -597539.0\n",
      "    agent-stage0_train1: -597539.0\n",
      "    agent-stage1_train0: -597539.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5015563862111083\n",
      "    mean_inference_ms: 1.5426908597154247\n",
      "    mean_processing_ms: 0.3634452165098083\n",
      "  time_since_restore: 622.922842502594\n",
      "  time_this_iter_s: 12.959560632705688\n",
      "  time_total_s: 622.922842502594\n",
      "  timers:\n",
      "    learn_throughput: 641.991\n",
      "    learn_time_ms: 6542.149\n",
      "    load_throughput: 302288.549\n",
      "    load_time_ms: 13.894\n",
      "    sample_throughput: 661.365\n",
      "    sample_time_ms: 6350.502\n",
      "    update_time_ms: 4.377\n",
      "  timestamp: 1608024974\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 197400\n",
      "  training_iteration: 47\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         622.923</td><td style=\"text-align: right;\">197400</td><td style=\"text-align: right;\"> -524816</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-36-27\n",
      "  done: false\n",
      "  episode_len_mean: 546.55\n",
      "  episode_reward_max: 24519.0\n",
      "  episode_reward_mean: -637198.56\n",
      "  episode_reward_min: -1792617.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 355\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.04004516452550888\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.15309512615203857\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013758324086666107\n",
      "        model: {}\n",
      "        policy_loss: -0.005074711050838232\n",
      "        total_loss: 6116615168.0\n",
      "        vf_explained_var: -7.450580596923828e-09\n",
      "        vf_loss: 6116615168.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1203303337097168\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010835545137524605\n",
      "        model: {}\n",
      "        policy_loss: 0.006617710925638676\n",
      "        total_loss: 6191623168.0\n",
      "        vf_explained_var: 7.450580596923828e-09\n",
      "        vf_loss: 6191623168.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.47943830490112305\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017749188467860222\n",
      "        model: {}\n",
      "        policy_loss: 0.011129021644592285\n",
      "        total_loss: 6194254848.0\n",
      "        vf_explained_var: 3.725290298461914e-09\n",
      "        vf_loss: 6194254848.0\n",
      "    num_steps_sampled: 201600\n",
      "    num_steps_trained: 201600\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.26111111111112\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 8173.0\n",
      "    agent-stage0_train1: 8173.0\n",
      "    agent-stage1_train0: 8173.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -212399.52\n",
      "    agent-stage0_train1: -212399.52\n",
      "    agent-stage1_train0: -212399.52\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -597539.0\n",
      "    agent-stage0_train1: -597539.0\n",
      "    agent-stage1_train0: -597539.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5030154843223293\n",
      "    mean_inference_ms: 1.5424871911267561\n",
      "    mean_processing_ms: 0.3634098757650359\n",
      "  time_since_restore: 636.1111288070679\n",
      "  time_this_iter_s: 13.188286304473877\n",
      "  time_total_s: 636.1111288070679\n",
      "  timers:\n",
      "    learn_throughput: 641.017\n",
      "    learn_time_ms: 6552.083\n",
      "    load_throughput: 301488.728\n",
      "    load_time_ms: 13.931\n",
      "    sample_throughput: 659.679\n",
      "    sample_time_ms: 6366.729\n",
      "    update_time_ms: 4.398\n",
      "  timestamp: 1608024987\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 201600\n",
      "  training_iteration: 48\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         636.111</td><td style=\"text-align: right;\">201600</td><td style=\"text-align: right;\"> -637199</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-36-40\n",
      "  done: false\n",
      "  episode_len_mean: 541.69\n",
      "  episode_reward_max: 24519.0\n",
      "  episode_reward_mean: -745205.13\n",
      "  episode_reward_min: -1792617.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 363\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.04004516452550888\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.12376564741134644\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0015382695710286498\n",
      "        model: {}\n",
      "        policy_loss: 0.004979060962796211\n",
      "        total_loss: 6360590336.0\n",
      "        vf_explained_var: -2.0489096641540527e-08\n",
      "        vf_loss: 6360590336.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1772334575653076\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00884045660495758\n",
      "        model: {}\n",
      "        policy_loss: 3.7628691643476486e-05\n",
      "        total_loss: 6289455616.0\n",
      "        vf_explained_var: -1.6763806343078613e-08\n",
      "        vf_loss: 6289455616.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.45103883743286133\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006481544114649296\n",
      "        model: {}\n",
      "        policy_loss: 0.005633005406707525\n",
      "        total_loss: 6337204224.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 6337204224.0\n",
      "    num_steps_sampled: 205800\n",
      "    num_steps_trained: 205800\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.42105263157895\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 8173.0\n",
      "    agent-stage0_train1: 8173.0\n",
      "    agent-stage1_train0: 8173.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -248401.71\n",
      "    agent-stage0_train1: -248401.71\n",
      "    agent-stage1_train0: -248401.71\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -597539.0\n",
      "    agent-stage0_train1: -597539.0\n",
      "    agent-stage1_train0: -597539.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.503653575835323\n",
      "    mean_inference_ms: 1.5416405283572\n",
      "    mean_processing_ms: 0.36330070924969604\n",
      "  time_since_restore: 649.0725455284119\n",
      "  time_this_iter_s: 12.961416721343994\n",
      "  time_total_s: 649.0725455284119\n",
      "  timers:\n",
      "    learn_throughput: 641.272\n",
      "    learn_time_ms: 6549.485\n",
      "    load_throughput: 300374.218\n",
      "    load_time_ms: 13.983\n",
      "    sample_throughput: 658.707\n",
      "    sample_time_ms: 6376.129\n",
      "    update_time_ms: 4.396\n",
      "  timestamp: 1608025000\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 205800\n",
      "  training_iteration: 49\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         649.073</td><td style=\"text-align: right;\">205800</td><td style=\"text-align: right;\"> -745205</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-36-53\n",
      "  done: false\n",
      "  episode_len_mean: 530.69\n",
      "  episode_reward_max: 24519.0\n",
      "  episode_reward_mean: -887211.96\n",
      "  episode_reward_min: -2075292.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 372\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.02002258226275444\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.023893853649497032\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0367027185857296\n",
      "        model: {}\n",
      "        policy_loss: 0.0022069690749049187\n",
      "        total_loss: 8650849280.0\n",
      "        vf_explained_var: -1.30385160446167e-08\n",
      "        vf_loss: 8650849280.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1764273643493652\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006573312915861607\n",
      "        model: {}\n",
      "        policy_loss: 0.0018565692007541656\n",
      "        total_loss: 8682434560.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 8682434560.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.46429678797721863\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0035411298740655184\n",
      "        model: {}\n",
      "        policy_loss: 0.0013964837417006493\n",
      "        total_loss: 8724568064.0\n",
      "        vf_explained_var: -1.862645149230957e-09\n",
      "        vf_loss: 8724568064.0\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.22105263157896\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 8173.0\n",
      "    agent-stage0_train1: 8173.0\n",
      "    agent-stage1_train0: 8173.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -295737.32\n",
      "    agent-stage0_train1: -295737.32\n",
      "    agent-stage1_train0: -295737.32\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -691764.0\n",
      "    agent-stage0_train1: -691764.0\n",
      "    agent-stage1_train0: -691764.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5046742108005695\n",
      "    mean_inference_ms: 1.5407050157632556\n",
      "    mean_processing_ms: 0.36315078276977963\n",
      "  time_since_restore: 662.0267109870911\n",
      "  time_this_iter_s: 12.9541654586792\n",
      "  time_total_s: 662.0267109870911\n",
      "  timers:\n",
      "    learn_throughput: 641.974\n",
      "    learn_time_ms: 6542.319\n",
      "    load_throughput: 300634.114\n",
      "    load_time_ms: 13.97\n",
      "    sample_throughput: 658.825\n",
      "    sample_time_ms: 6374.982\n",
      "    update_time_ms: 4.55\n",
      "  timestamp: 1608025013\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 50\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         662.027</td><td style=\"text-align: right;\">210000</td><td style=\"text-align: right;\"> -887212</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-37-06\n",
      "  done: false\n",
      "  episode_len_mean: 523.08\n",
      "  episode_reward_max: 24519.0\n",
      "  episode_reward_mean: -1014021.99\n",
      "  episode_reward_min: -2116167.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 381\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.03003387525677681\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.030868416652083397\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01129627414047718\n",
      "        model: {}\n",
      "        policy_loss: 0.006650155410170555\n",
      "        total_loss: 7092396544.0\n",
      "        vf_explained_var: -1.1175870895385742e-08\n",
      "        vf_loss: 7092396544.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1944971084594727\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012284347787499428\n",
      "        model: {}\n",
      "        policy_loss: -0.0008817068301141262\n",
      "        total_loss: 7089499136.0\n",
      "        vf_explained_var: 1.6763806343078613e-08\n",
      "        vf_loss: 7089499136.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.4488045275211334\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013077491894364357\n",
      "        model: {}\n",
      "        policy_loss: 0.005559580400586128\n",
      "        total_loss: 7062883840.0\n",
      "        vf_explained_var: -5.587935447692871e-09\n",
      "        vf_loss: 7062883840.0\n",
      "    num_steps_sampled: 214200\n",
      "    num_steps_trained: 214200\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.51666666666667\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 8173.0\n",
      "    agent-stage0_train1: 8173.0\n",
      "    agent-stage1_train0: 8173.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -338007.33\n",
      "    agent-stage0_train1: -338007.33\n",
      "    agent-stage1_train0: -338007.33\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -705389.0\n",
      "    agent-stage0_train1: -705389.0\n",
      "    agent-stage1_train0: -705389.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5055151554684336\n",
      "    mean_inference_ms: 1.5396668347963685\n",
      "    mean_processing_ms: 0.36303740729260553\n",
      "  time_since_restore: 674.9880106449127\n",
      "  time_this_iter_s: 12.961299657821655\n",
      "  time_total_s: 674.9880106449127\n",
      "  timers:\n",
      "    learn_throughput: 641.767\n",
      "    learn_time_ms: 6544.435\n",
      "    load_throughput: 302818.573\n",
      "    load_time_ms: 13.87\n",
      "    sample_throughput: 658.289\n",
      "    sample_time_ms: 6380.173\n",
      "    update_time_ms: 4.484\n",
      "  timestamp: 1608025026\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 214200\n",
      "  training_iteration: 51\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         674.988</td><td style=\"text-align: right;\">214200</td><td style=\"text-align: right;\">-1.01402e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-37-19\n",
      "  done: false\n",
      "  episode_len_mean: 520.39\n",
      "  episode_reward_max: 24519.0\n",
      "  episode_reward_mean: -1100448.99\n",
      "  episode_reward_min: -2116167.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 390\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.03003387525677681\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.00030150869861245155\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.8427506549633108e-05\n",
      "        model: {}\n",
      "        policy_loss: 0.0044678812846541405\n",
      "        total_loss: 5578518016.0\n",
      "        vf_explained_var: -7.450580596923828e-09\n",
      "        vf_loss: 5578518016.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.139062523841858\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1917426586151123\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02332846261560917\n",
      "        model: {}\n",
      "        policy_loss: 0.0007780520245432854\n",
      "        total_loss: 5544148992.0\n",
      "        vf_explained_var: 5.587935447692871e-09\n",
      "        vf_loss: 5544148992.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.4398581087589264\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01954675279557705\n",
      "        model: {}\n",
      "        policy_loss: 0.005528891459107399\n",
      "        total_loss: 5547526144.0\n",
      "        vf_explained_var: -1.30385160446167e-08\n",
      "        vf_loss: 5547526144.0\n",
      "    num_steps_sampled: 218400\n",
      "    num_steps_trained: 218400\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.9157894736842\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 8173.0\n",
      "    agent-stage0_train1: 8173.0\n",
      "    agent-stage1_train0: 8173.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -366816.33\n",
      "    agent-stage0_train1: -366816.33\n",
      "    agent-stage1_train0: -366816.33\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -705389.0\n",
      "    agent-stage0_train1: -705389.0\n",
      "    agent-stage1_train0: -705389.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5064882257430035\n",
      "    mean_inference_ms: 1.5387380727414632\n",
      "    mean_processing_ms: 0.36292221075955944\n",
      "  time_since_restore: 687.8823878765106\n",
      "  time_this_iter_s: 12.8943772315979\n",
      "  time_total_s: 687.8823878765106\n",
      "  timers:\n",
      "    learn_throughput: 643.633\n",
      "    learn_time_ms: 6525.462\n",
      "    load_throughput: 301590.926\n",
      "    load_time_ms: 13.926\n",
      "    sample_throughput: 656.149\n",
      "    sample_time_ms: 6400.988\n",
      "    update_time_ms: 4.378\n",
      "  timestamp: 1608025039\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 218400\n",
      "  training_iteration: 52\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         687.882</td><td style=\"text-align: right;\">218400</td><td style=\"text-align: right;\">-1.10045e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-37-32\n",
      "  done: false\n",
      "  episode_len_mean: 514.62\n",
      "  episode_reward_max: 24519.0\n",
      "  episode_reward_mean: -1241114.91\n",
      "  episode_reward_min: -2277585.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 399\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.015016937628388405\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.000326181499985978\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.840771801653318e-07\n",
      "        model: {}\n",
      "        policy_loss: 0.0036945901811122894\n",
      "        total_loss: 11189749760.0\n",
      "        vf_explained_var: -2.0489096641540527e-08\n",
      "        vf_loss: 11189749760.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1426258087158203\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009321312420070171\n",
      "        model: {}\n",
      "        policy_loss: -0.0013370607048273087\n",
      "        total_loss: 11148027904.0\n",
      "        vf_explained_var: -3.725290298461914e-09\n",
      "        vf_loss: 11148027904.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.31332728266716003\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005244197323918343\n",
      "        model: {}\n",
      "        policy_loss: 2.278795000165701e-05\n",
      "        total_loss: 11129694208.0\n",
      "        vf_explained_var: -1.6763806343078613e-08\n",
      "        vf_loss: 11129694208.0\n",
      "    num_steps_sampled: 222600\n",
      "    num_steps_trained: 222600\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.3578947368421\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 8173.0\n",
      "    agent-stage0_train1: 8173.0\n",
      "    agent-stage1_train0: 8173.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -413704.97\n",
      "    agent-stage0_train1: -413704.97\n",
      "    agent-stage1_train0: -413704.97\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -759195.0\n",
      "    agent-stage0_train1: -759195.0\n",
      "    agent-stage1_train0: -759195.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.507659197834766\n",
      "    mean_inference_ms: 1.5378969440563948\n",
      "    mean_processing_ms: 0.3628554403152006\n",
      "  time_since_restore: 701.0653185844421\n",
      "  time_this_iter_s: 13.182930707931519\n",
      "  time_total_s: 701.0653185844421\n",
      "  timers:\n",
      "    learn_throughput: 642.048\n",
      "    learn_time_ms: 6541.568\n",
      "    load_throughput: 300888.807\n",
      "    load_time_ms: 13.959\n",
      "    sample_throughput: 654.078\n",
      "    sample_time_ms: 6421.257\n",
      "    update_time_ms: 4.1\n",
      "  timestamp: 1608025052\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 222600\n",
      "  training_iteration: 53\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         701.065</td><td style=\"text-align: right;\">222600</td><td style=\"text-align: right;\">-1.24111e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-37-45\n",
      "  done: false\n",
      "  episode_len_mean: 506.64\n",
      "  episode_reward_max: -147051.0\n",
      "  episode_reward_mean: -1395357.66\n",
      "  episode_reward_min: -2277585.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 408\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.007508468814194202\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.0003509011585265398\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.09456735231106e-07\n",
      "        model: {}\n",
      "        policy_loss: 0.00014650472439825535\n",
      "        total_loss: 10915186688.0\n",
      "        vf_explained_var: -2.2351741790771484e-08\n",
      "        vf_loss: 10915186688.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1566824913024902\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006119556725025177\n",
      "        model: {}\n",
      "        policy_loss: 9.886431507766247e-05\n",
      "        total_loss: 10938308608.0\n",
      "        vf_explained_var: -3.3527612686157227e-08\n",
      "        vf_loss: 10938308608.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.34091103076934814\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0059267375618219376\n",
      "        model: {}\n",
      "        policy_loss: 0.0033453155774623156\n",
      "        total_loss: 10922995712.0\n",
      "        vf_explained_var: 1.30385160446167e-08\n",
      "        vf_loss: 10922995712.0\n",
      "    num_steps_sampled: 226800\n",
      "    num_steps_trained: 226800\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.69999999999999\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -49017.0\n",
      "    agent-stage0_train1: -49017.0\n",
      "    agent-stage1_train0: -49017.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -465119.22\n",
      "    agent-stage0_train1: -465119.22\n",
      "    agent-stage1_train0: -465119.22\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -759195.0\n",
      "    agent-stage0_train1: -759195.0\n",
      "    agent-stage1_train0: -759195.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5088156150797793\n",
      "    mean_inference_ms: 1.5370884155139848\n",
      "    mean_processing_ms: 0.362757657826888\n",
      "  time_since_restore: 714.033935546875\n",
      "  time_this_iter_s: 12.968616962432861\n",
      "  time_total_s: 714.033935546875\n",
      "  timers:\n",
      "    learn_throughput: 642.466\n",
      "    learn_time_ms: 6537.309\n",
      "    load_throughput: 299251.142\n",
      "    load_time_ms: 14.035\n",
      "    sample_throughput: 652.33\n",
      "    sample_time_ms: 6438.461\n",
      "    update_time_ms: 4.122\n",
      "  timestamp: 1608025065\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 226800\n",
      "  training_iteration: 54\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         714.034</td><td style=\"text-align: right;\">226800</td><td style=\"text-align: right;\">-1.39536e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-37-58\n",
      "  done: false\n",
      "  episode_len_mean: 499.59\n",
      "  episode_reward_max: -147051.0\n",
      "  episode_reward_mean: -1539403.59\n",
      "  episode_reward_min: -2438733.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 417\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.003754234407097101\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.00042951066279783845\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.550639353808947e-06\n",
      "        model: {}\n",
      "        policy_loss: 0.0014148717746138573\n",
      "        total_loss: 10927218688.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 10927218688.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 1.708593726158142\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.182255506515503\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004957713186740875\n",
      "        model: {}\n",
      "        policy_loss: -0.0012035774998366833\n",
      "        total_loss: 10913321984.0\n",
      "        vf_explained_var: -1.862645149230957e-09\n",
      "        vf_loss: 10913321984.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.38745659589767456\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011962749995291233\n",
      "        model: {}\n",
      "        policy_loss: 0.002942962571978569\n",
      "        total_loss: 10922139648.0\n",
      "        vf_explained_var: -2.0489096641540527e-08\n",
      "        vf_loss: 10922139648.0\n",
      "    num_steps_sampled: 231000\n",
      "    num_steps_trained: 231000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.00526315789473\n",
      "    ram_util_percent: 6.205263157894738\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -49017.0\n",
      "    agent-stage0_train1: -49017.0\n",
      "    agent-stage1_train0: -49017.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -513134.53\n",
      "    agent-stage0_train1: -513134.53\n",
      "    agent-stage1_train0: -513134.53\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -812911.0\n",
      "    agent-stage0_train1: -812911.0\n",
      "    agent-stage1_train0: -812911.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5099900543898546\n",
      "    mean_inference_ms: 1.5363417751378685\n",
      "    mean_processing_ms: 0.36263565146601484\n",
      "  time_since_restore: 727.0293390750885\n",
      "  time_this_iter_s: 12.995403528213501\n",
      "  time_total_s: 727.0293390750885\n",
      "  timers:\n",
      "    learn_throughput: 644.253\n",
      "    learn_time_ms: 6519.173\n",
      "    load_throughput: 300921.188\n",
      "    load_time_ms: 13.957\n",
      "    sample_throughput: 651.198\n",
      "    sample_time_ms: 6449.649\n",
      "    update_time_ms: 4.155\n",
      "  timestamp: 1608025078\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 231000\n",
      "  training_iteration: 55\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         727.029</td><td style=\"text-align: right;\">231000</td><td style=\"text-align: right;\">-1.5394e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-38-11\n",
      "  done: false\n",
      "  episode_len_mean: 495.22\n",
      "  episode_reward_max: -147051.0\n",
      "  episode_reward_mean: -1645417.14\n",
      "  episode_reward_min: -2438733.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 424\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.0018771172035485506\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.0004481275682337582\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.240416456013918e-05\n",
      "        model: {}\n",
      "        policy_loss: 0.0025120973587036133\n",
      "        total_loss: 9330694144.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 9330694144.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.210707187652588\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019990479573607445\n",
      "        model: {}\n",
      "        policy_loss: 0.008384473621845245\n",
      "        total_loss: 9317472256.0\n",
      "        vf_explained_var: -1.1175870895385742e-08\n",
      "        vf_loss: 9317472256.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.3502342402935028\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013432059437036514\n",
      "        model: {}\n",
      "        policy_loss: 0.008789118379354477\n",
      "        total_loss: 9339545600.0\n",
      "        vf_explained_var: -1.4901161193847656e-08\n",
      "        vf_loss: 9339545600.0\n",
      "    num_steps_sampled: 235200\n",
      "    num_steps_trained: 235200\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.38333333333334\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -49017.0\n",
      "    agent-stage0_train1: -49017.0\n",
      "    agent-stage1_train0: -49017.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -548472.38\n",
      "    agent-stage0_train1: -548472.38\n",
      "    agent-stage1_train0: -548472.38\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -812911.0\n",
      "    agent-stage0_train1: -812911.0\n",
      "    agent-stage1_train0: -812911.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5110048097754705\n",
      "    mean_inference_ms: 1.5358012050096221\n",
      "    mean_processing_ms: 0.36256826015377314\n",
      "  time_since_restore: 740.0267598628998\n",
      "  time_this_iter_s: 12.99742078781128\n",
      "  time_total_s: 740.0267598628998\n",
      "  timers:\n",
      "    learn_throughput: 642.736\n",
      "    learn_time_ms: 6534.564\n",
      "    load_throughput: 301710.246\n",
      "    load_time_ms: 13.921\n",
      "    sample_throughput: 652.074\n",
      "    sample_time_ms: 6440.991\n",
      "    update_time_ms: 4.159\n",
      "  timestamp: 1608025091\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 235200\n",
      "  training_iteration: 56\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         740.027</td><td style=\"text-align: right;\">235200</td><td style=\"text-align: right;\">-1.64542e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-38-25\n",
      "  done: false\n",
      "  episode_len_mean: 488.92\n",
      "  episode_reward_max: -515571.0\n",
      "  episode_reward_mean: -1774478.01\n",
      "  episode_reward_min: -2531871.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 432\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.0009385586017742753\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.0007136865751817822\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.192923137859907e-06\n",
      "        model: {}\n",
      "        policy_loss: 0.001518347766250372\n",
      "        total_loss: 11336036352.0\n",
      "        vf_explained_var: 1.862645149230957e-09\n",
      "        vf_loss: 11336036352.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1054811477661133\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013394352048635483\n",
      "        model: {}\n",
      "        policy_loss: 0.006299424916505814\n",
      "        total_loss: 11360536576.0\n",
      "        vf_explained_var: -1.1175870895385742e-08\n",
      "        vf_loss: 11360536576.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.4866398870944977\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004086610861122608\n",
      "        model: {}\n",
      "        policy_loss: 0.0037809377536177635\n",
      "        total_loss: 11353637888.0\n",
      "        vf_explained_var: -9.313225746154785e-09\n",
      "        vf_loss: 11353637888.0\n",
      "    num_steps_sampled: 239400\n",
      "    num_steps_trained: 239400\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.60526315789474\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -171857.0\n",
      "    agent-stage0_train1: -171857.0\n",
      "    agent-stage1_train0: -171857.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -591492.67\n",
      "    agent-stage0_train1: -591492.67\n",
      "    agent-stage1_train0: -591492.67\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -843957.0\n",
      "    agent-stage0_train1: -843957.0\n",
      "    agent-stage1_train0: -843957.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5121101469450027\n",
      "    mean_inference_ms: 1.53507178614603\n",
      "    mean_processing_ms: 0.36246279849028346\n",
      "  time_since_restore: 753.2391879558563\n",
      "  time_this_iter_s: 13.212428092956543\n",
      "  time_total_s: 753.2391879558563\n",
      "  timers:\n",
      "    learn_throughput: 640.17\n",
      "    learn_time_ms: 6560.759\n",
      "    load_throughput: 300125.508\n",
      "    load_time_ms: 13.994\n",
      "    sample_throughput: 652.157\n",
      "    sample_time_ms: 6440.166\n",
      "    update_time_ms: 4.168\n",
      "  timestamp: 1608025105\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 239400\n",
      "  training_iteration: 57\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         753.239</td><td style=\"text-align: right;\">239400</td><td style=\"text-align: right;\">-1.77448e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-38-38\n",
      "  done: false\n",
      "  episode_len_mean: 481.79\n",
      "  episode_reward_max: -1218276.0\n",
      "  episode_reward_mean: -1923928.83\n",
      "  episode_reward_min: -2837544.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 442\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.00046927930088713765\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.0028177679050713778\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0001391405676258728\n",
      "        model: {}\n",
      "        policy_loss: -0.0009411005303263664\n",
      "        total_loss: 18696609792.0\n",
      "        vf_explained_var: 1.4901161193847656e-08\n",
      "        vf_loss: 18696609792.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.013275146484375\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008230490610003471\n",
      "        model: {}\n",
      "        policy_loss: 0.0015670652501285076\n",
      "        total_loss: 18739273728.0\n",
      "        vf_explained_var: -2.421438694000244e-08\n",
      "        vf_loss: 18739273728.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.12656250596046448\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.41998234391212463\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005749771371483803\n",
      "        model: {}\n",
      "        policy_loss: -0.0009146546944975853\n",
      "        total_loss: 18655109120.0\n",
      "        vf_explained_var: 1.862645149230957e-09\n",
      "        vf_loss: 18655109120.0\n",
      "    num_steps_sampled: 243600\n",
      "    num_steps_trained: 243600\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.67894736842105\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -406092.0\n",
      "    agent-stage0_train1: -406092.0\n",
      "    agent-stage1_train0: -406092.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -641309.61\n",
      "    agent-stage0_train1: -641309.61\n",
      "    agent-stage1_train0: -641309.61\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -945848.0\n",
      "    agent-stage0_train1: -945848.0\n",
      "    agent-stage1_train0: -945848.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.513495477247148\n",
      "    mean_inference_ms: 1.5341333884988202\n",
      "    mean_processing_ms: 0.36235375949882453\n",
      "  time_since_restore: 766.1763489246368\n",
      "  time_this_iter_s: 12.937160968780518\n",
      "  time_total_s: 766.1763489246368\n",
      "  timers:\n",
      "    learn_throughput: 641.76\n",
      "    learn_time_ms: 6544.505\n",
      "    load_throughput: 297334.312\n",
      "    load_time_ms: 14.126\n",
      "    sample_throughput: 653.071\n",
      "    sample_time_ms: 6431.158\n",
      "    update_time_ms: 4.192\n",
      "  timestamp: 1608025118\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 243600\n",
      "  training_iteration: 58\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         766.176</td><td style=\"text-align: right;\">243600</td><td style=\"text-align: right;\">-1.92393e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-38-51\n",
      "  done: false\n",
      "  episode_len_mean: 478.63\n",
      "  episode_reward_max: -1218276.0\n",
      "  episode_reward_mean: -2050737.51\n",
      "  episode_reward_min: -3023778.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 451\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.00023463965044356883\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.8883939674196881e-06\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.002345799235627055\n",
      "        model: {}\n",
      "        policy_loss: -0.00028402917087078094\n",
      "        total_loss: 21620459520.0\n",
      "        vf_explained_var: -2.0489096641540527e-08\n",
      "        vf_loss: 21620459520.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9773879051208496\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009085163474082947\n",
      "        model: {}\n",
      "        policy_loss: 0.0017613200470805168\n",
      "        total_loss: 21640431616.0\n",
      "        vf_explained_var: -2.421438694000244e-08\n",
      "        vf_loss: 21640431616.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.12656250596046448\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.39169055223464966\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014090083539485931\n",
      "        model: {}\n",
      "        policy_loss: 0.0096231484785676\n",
      "        total_loss: 21710843904.0\n",
      "        vf_explained_var: -3.725290298461914e-09\n",
      "        vf_loss: 21710843904.0\n",
      "    num_steps_sampled: 247800\n",
      "    num_steps_trained: 247800\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.85263157894737\n",
      "    ram_util_percent: 6.205263157894737\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -406092.0\n",
      "    agent-stage0_train1: -406092.0\n",
      "    agent-stage1_train0: -406092.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -683579.17\n",
      "    agent-stage0_train1: -683579.17\n",
      "    agent-stage1_train0: -683579.17\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1007926.0\n",
      "    agent-stage0_train1: -1007926.0\n",
      "    agent-stage1_train0: -1007926.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.514794809283105\n",
      "    mean_inference_ms: 1.5333965017651179\n",
      "    mean_processing_ms: 0.36223639807467933\n",
      "  time_since_restore: 779.2610054016113\n",
      "  time_this_iter_s: 13.084656476974487\n",
      "  time_total_s: 779.2610054016113\n",
      "  timers:\n",
      "    learn_throughput: 641.358\n",
      "    learn_time_ms: 6548.606\n",
      "    load_throughput: 297152.751\n",
      "    load_time_ms: 14.134\n",
      "    sample_throughput: 652.233\n",
      "    sample_time_ms: 6439.414\n",
      "    update_time_ms: 4.203\n",
      "  timestamp: 1608025131\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 247800\n",
      "  training_iteration: 59\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         779.261</td><td style=\"text-align: right;\">247800</td><td style=\"text-align: right;\">-2.05074e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-39-04\n",
      "  done: false\n",
      "  episode_len_mean: 475.71\n",
      "  episode_reward_max: -1218276.0\n",
      "  episode_reward_mean: -2191972.71\n",
      "  episode_reward_min: -3189399.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 461\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 0.00011731982522178441\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.916421297210036e-06\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.1695734380978706e-09\n",
      "        model: {}\n",
      "        policy_loss: 0.0025887195952236652\n",
      "        total_loss: 23777523712.0\n",
      "        vf_explained_var: -3.5390257835388184e-08\n",
      "        vf_loss: 23777523712.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.9044862985610962\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009906403720378876\n",
      "        model: {}\n",
      "        policy_loss: 0.001522660255432129\n",
      "        total_loss: 23692840960.0\n",
      "        vf_explained_var: 1.862645149230957e-09\n",
      "        vf_loss: 23692840960.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.12656250596046448\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.32897478342056274\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009743735194206238\n",
      "        model: {}\n",
      "        policy_loss: 0.0017699268646538258\n",
      "        total_loss: 23739152384.0\n",
      "        vf_explained_var: 2.7939677238464355e-08\n",
      "        vf_loss: 23739152384.0\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.11666666666666\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -406092.0\n",
      "    agent-stage0_train1: -406092.0\n",
      "    agent-stage1_train0: -406092.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -730657.57\n",
      "    agent-stage0_train1: -730657.57\n",
      "    agent-stage1_train0: -730657.57\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1063133.0\n",
      "    agent-stage0_train1: -1063133.0\n",
      "    agent-stage1_train0: -1063133.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5162272416583638\n",
      "    mean_inference_ms: 1.532446065458208\n",
      "    mean_processing_ms: 0.3621095990415391\n",
      "  time_since_restore: 792.3609802722931\n",
      "  time_this_iter_s: 13.099974870681763\n",
      "  time_total_s: 792.3609802722931\n",
      "  timers:\n",
      "    learn_throughput: 640.838\n",
      "    learn_time_ms: 6553.923\n",
      "    load_throughput: 295688.843\n",
      "    load_time_ms: 14.204\n",
      "    sample_throughput: 651.29\n",
      "    sample_time_ms: 6448.736\n",
      "    update_time_ms: 4.051\n",
      "  timestamp: 1608025144\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 60\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         792.361</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">-2.19197e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-39-17\n",
      "  done: false\n",
      "  episode_len_mean: 471.51\n",
      "  episode_reward_max: -1344699.0\n",
      "  episode_reward_mean: -2337016.62\n",
      "  episode_reward_min: -3393903.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 471\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 5.8659912610892206e-05\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.9277908904768992e-06\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -7.457952033718129e-10\n",
      "        model: {}\n",
      "        policy_loss: -0.002257362939417362\n",
      "        total_loss: 28423342080.0\n",
      "        vf_explained_var: -1.4901161193847656e-08\n",
      "        vf_loss: 28423342080.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8801771402359009\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007172096986323595\n",
      "        model: {}\n",
      "        policy_loss: 0.0009524635970592499\n",
      "        total_loss: 28526127104.0\n",
      "        vf_explained_var: 1.30385160446167e-08\n",
      "        vf_loss: 28526127104.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.12656250596046448\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.2762840986251831\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02029517851769924\n",
      "        model: {}\n",
      "        policy_loss: 0.00648241862654686\n",
      "        total_loss: 28545705984.0\n",
      "        vf_explained_var: -5.587935447692871e-09\n",
      "        vf_loss: 28545705984.0\n",
      "    num_steps_sampled: 256200\n",
      "    num_steps_trained: 256200\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.5263157894737\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -448233.0\n",
      "    agent-stage0_train1: -448233.0\n",
      "    agent-stage1_train0: -448233.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -779005.54\n",
      "    agent-stage0_train1: -779005.54\n",
      "    agent-stage1_train0: -779005.54\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1131301.0\n",
      "    agent-stage0_train1: -1131301.0\n",
      "    agent-stage1_train0: -1131301.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5178363464295255\n",
      "    mean_inference_ms: 1.53181476718514\n",
      "    mean_processing_ms: 0.36200189056598087\n",
      "  time_since_restore: 805.6063883304596\n",
      "  time_this_iter_s: 13.245408058166504\n",
      "  time_total_s: 805.6063883304596\n",
      "  timers:\n",
      "    learn_throughput: 638.889\n",
      "    learn_time_ms: 6573.911\n",
      "    load_throughput: 296701.814\n",
      "    load_time_ms: 14.156\n",
      "    sample_throughput: 650.431\n",
      "    sample_time_ms: 6457.259\n",
      "    update_time_ms: 4.082\n",
      "  timestamp: 1608025157\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256200\n",
      "  training_iteration: 61\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         805.606</td><td style=\"text-align: right;\">256200</td><td style=\"text-align: right;\">-2.33702e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-39-31\n",
      "  done: false\n",
      "  episode_len_mean: 467.0\n",
      "  episode_reward_max: -1344699.0\n",
      "  episode_reward_mean: -2476222.41\n",
      "  episode_reward_min: -3417345.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 480\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 2.9329956305446103e-05\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.9518647604854777e-06\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.52095980254785e-09\n",
      "        model: {}\n",
      "        policy_loss: 0.00013732817023992538\n",
      "        total_loss: 29064099840.0\n",
      "        vf_explained_var: 1.862645149230957e-08\n",
      "        vf_loss: 29064099840.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8437076210975647\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012201134115457535\n",
      "        model: {}\n",
      "        policy_loss: 0.004422151017934084\n",
      "        total_loss: 29054214144.0\n",
      "        vf_explained_var: 3.5390257835388184e-08\n",
      "        vf_loss: 29054214144.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.18984374403953552\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.26602840423583984\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008781458251178265\n",
      "        model: {}\n",
      "        policy_loss: -0.0007216976955533028\n",
      "        total_loss: 28951511040.0\n",
      "        vf_explained_var: 4.284083843231201e-08\n",
      "        vf_loss: 28951511040.0\n",
      "    num_steps_sampled: 260400\n",
      "    num_steps_trained: 260400\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.06315789473685\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -448233.0\n",
      "    agent-stage0_train1: -448233.0\n",
      "    agent-stage1_train0: -448233.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -825407.47\n",
      "    agent-stage0_train1: -825407.47\n",
      "    agent-stage1_train0: -825407.47\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1139115.0\n",
      "    agent-stage0_train1: -1139115.0\n",
      "    agent-stage1_train0: -1139115.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5192386801254534\n",
      "    mean_inference_ms: 1.531159444613454\n",
      "    mean_processing_ms: 0.3618922894660486\n",
      "  time_since_restore: 818.7799954414368\n",
      "  time_this_iter_s: 13.173607110977173\n",
      "  time_total_s: 818.7799954414368\n",
      "  timers:\n",
      "    learn_throughput: 637.277\n",
      "    learn_time_ms: 6590.541\n",
      "    load_throughput: 298827.271\n",
      "    load_time_ms: 14.055\n",
      "    sample_throughput: 649.295\n",
      "    sample_time_ms: 6468.551\n",
      "    update_time_ms: 4.083\n",
      "  timestamp: 1608025171\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260400\n",
      "  training_iteration: 62\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">          818.78</td><td style=\"text-align: right;\">260400</td><td style=\"text-align: right;\">-2.47622e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-39-44\n",
      "  done: false\n",
      "  episode_len_mean: 461.73\n",
      "  episode_reward_max: -1388592.0\n",
      "  episode_reward_mean: -2636850.42\n",
      "  episode_reward_min: -3538908.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 489\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.4664978152723052e-05\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.9791270915447967e-06\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.7513726024986909e-09\n",
      "        model: {}\n",
      "        policy_loss: -0.0025278697721660137\n",
      "        total_loss: 36534001664.0\n",
      "        vf_explained_var: -3.725290298461914e-09\n",
      "        vf_loss: 36534001664.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.78391432762146\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010735800489783287\n",
      "        model: {}\n",
      "        policy_loss: 0.006805330514907837\n",
      "        total_loss: 36710817792.0\n",
      "        vf_explained_var: -1.862645149230957e-08\n",
      "        vf_loss: 36710817792.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.18984374403953552\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.24733960628509521\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005679655820131302\n",
      "        model: {}\n",
      "        policy_loss: 0.0010444782674312592\n",
      "        total_loss: 36651356160.0\n",
      "        vf_explained_var: -1.6763806343078613e-08\n",
      "        vf_loss: 36651356160.0\n",
      "    num_steps_sampled: 264600\n",
      "    num_steps_trained: 264600\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.41052631578947\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -462864.0\n",
      "    agent-stage0_train1: -462864.0\n",
      "    agent-stage1_train0: -462864.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -878950.14\n",
      "    agent-stage0_train1: -878950.14\n",
      "    agent-stage1_train0: -878950.14\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1179636.0\n",
      "    agent-stage0_train1: -1179636.0\n",
      "    agent-stage1_train0: -1179636.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5207305001416342\n",
      "    mean_inference_ms: 1.5304797596772883\n",
      "    mean_processing_ms: 0.36178464344689454\n",
      "  time_since_restore: 831.8539867401123\n",
      "  time_this_iter_s: 13.073991298675537\n",
      "  time_total_s: 831.8539867401123\n",
      "  timers:\n",
      "    learn_throughput: 637.782\n",
      "    learn_time_ms: 6585.318\n",
      "    load_throughput: 300205.808\n",
      "    load_time_ms: 13.99\n",
      "    sample_throughput: 649.85\n",
      "    sample_time_ms: 6463.028\n",
      "    update_time_ms: 4.102\n",
      "  timestamp: 1608025184\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 264600\n",
      "  training_iteration: 63\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         831.854</td><td style=\"text-align: right;\">264600</td><td style=\"text-align: right;\">-2.63685e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-39-57\n",
      "  done: false\n",
      "  episode_len_mean: 458.35\n",
      "  episode_reward_max: -1795209.0\n",
      "  episode_reward_mean: -2780090.16\n",
      "  episode_reward_min: -3538908.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 500\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 7.332489076361526e-06\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 2.0620568648155313e-06\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -5.403909320023104e-09\n",
      "        model: {}\n",
      "        policy_loss: 0.002216194523498416\n",
      "        total_loss: 28056621056.0\n",
      "        vf_explained_var: -3.5390257835388184e-08\n",
      "        vf_loss: 28056621056.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8236452341079712\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008004678413271904\n",
      "        model: {}\n",
      "        policy_loss: 0.0017926145810633898\n",
      "        total_loss: 27948929024.0\n",
      "        vf_explained_var: -1.30385160446167e-08\n",
      "        vf_loss: 27948929024.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.18984374403953552\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.15329430997371674\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.054276540875434875\n",
      "        model: {}\n",
      "        policy_loss: 0.011342035606503487\n",
      "        total_loss: 28022566912.0\n",
      "        vf_explained_var: -2.0489096641540527e-08\n",
      "        vf_loss: 28022566912.0\n",
      "    num_steps_sampled: 268800\n",
      "    num_steps_trained: 268800\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.18947368421054\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -598403.0\n",
      "    agent-stage0_train1: -598403.0\n",
      "    agent-stage1_train0: -598403.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -926696.72\n",
      "    agent-stage0_train1: -926696.72\n",
      "    agent-stage1_train0: -926696.72\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1179636.0\n",
      "    agent-stage0_train1: -1179636.0\n",
      "    agent-stage1_train0: -1179636.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5224997722124347\n",
      "    mean_inference_ms: 1.5295609803619499\n",
      "    mean_processing_ms: 0.361632002157545\n",
      "  time_since_restore: 844.8286738395691\n",
      "  time_this_iter_s: 12.974687099456787\n",
      "  time_total_s: 844.8286738395691\n",
      "  timers:\n",
      "    learn_throughput: 637.166\n",
      "    learn_time_ms: 6591.687\n",
      "    load_throughput: 280955.474\n",
      "    load_time_ms: 14.949\n",
      "    sample_throughput: 650.547\n",
      "    sample_time_ms: 6456.109\n",
      "    update_time_ms: 4.134\n",
      "  timestamp: 1608025197\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268800\n",
      "  training_iteration: 64\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         844.829</td><td style=\"text-align: right;\">268800</td><td style=\"text-align: right;\">-2.78009e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-40-10\n",
      "  done: false\n",
      "  episode_len_mean: 453.12\n",
      "  episode_reward_max: -1795209.0\n",
      "  episode_reward_mean: -2881725.57\n",
      "  episode_reward_min: -3538908.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 510\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 3.666244538180763e-06\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 2.1959890545986127e-06\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -8.564924769416393e-09\n",
      "        model: {}\n",
      "        policy_loss: 0.0007431504782289267\n",
      "        total_loss: 28508162048.0\n",
      "        vf_explained_var: -1.30385160446167e-08\n",
      "        vf_loss: 28508162048.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8213196396827698\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00866896752268076\n",
      "        model: {}\n",
      "        policy_loss: -0.0020941593684256077\n",
      "        total_loss: 28453588992.0\n",
      "        vf_explained_var: 9.313225746154785e-09\n",
      "        vf_loss: 28453588992.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.2847656309604645\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.3404988646507263\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009224983863532543\n",
      "        model: {}\n",
      "        policy_loss: 0.003444511443376541\n",
      "        total_loss: 28434808832.0\n",
      "        vf_explained_var: -7.450580596923828e-09\n",
      "        vf_loss: 28434808832.0\n",
      "    num_steps_sampled: 273000\n",
      "    num_steps_trained: 273000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.6888888888889\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -598403.0\n",
      "    agent-stage0_train1: -598403.0\n",
      "    agent-stage1_train0: -598403.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -960575.19\n",
      "    agent-stage0_train1: -960575.19\n",
      "    agent-stage1_train0: -960575.19\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1179636.0\n",
      "    agent-stage0_train1: -1179636.0\n",
      "    agent-stage1_train0: -1179636.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5242005650569497\n",
      "    mean_inference_ms: 1.5288481892669694\n",
      "    mean_processing_ms: 0.36150121332538865\n",
      "  time_since_restore: 857.8322155475616\n",
      "  time_this_iter_s: 13.003541707992554\n",
      "  time_total_s: 857.8322155475616\n",
      "  timers:\n",
      "    learn_throughput: 636.468\n",
      "    learn_time_ms: 6598.914\n",
      "    load_throughput: 282421.167\n",
      "    load_time_ms: 14.871\n",
      "    sample_throughput: 651.191\n",
      "    sample_time_ms: 6449.725\n",
      "    update_time_ms: 4.119\n",
      "  timestamp: 1608025210\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 273000\n",
      "  training_iteration: 65\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         857.832</td><td style=\"text-align: right;\">273000</td><td style=\"text-align: right;\">-2.88173e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-40-23\n",
      "  done: false\n",
      "  episode_len_mean: 446.35\n",
      "  episode_reward_max: -1869387.0\n",
      "  episode_reward_mean: -2954181.0\n",
      "  episode_reward_min: -3538908.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 519\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.8331222690903815e-06\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 2.259648681501858e-06\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -3.863507735957228e-09\n",
      "        model: {}\n",
      "        policy_loss: -0.0048104845918715\n",
      "        total_loss: 25804167168.0\n",
      "        vf_explained_var: -2.60770320892334e-08\n",
      "        vf_loss: 25804167168.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8919457793235779\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00670514814555645\n",
      "        model: {}\n",
      "        policy_loss: -0.002431051107123494\n",
      "        total_loss: 25835921408.0\n",
      "        vf_explained_var: 3.725290298461914e-09\n",
      "        vf_loss: 25835921408.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.2847656309604645\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.2590666115283966\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005793764255940914\n",
      "        model: {}\n",
      "        policy_loss: 0.0003123106434941292\n",
      "        total_loss: 25892108288.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 25892108288.0\n",
      "    num_steps_sampled: 277200\n",
      "    num_steps_trained: 277200\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.18421052631578\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -623129.0\n",
      "    agent-stage0_train1: -623129.0\n",
      "    agent-stage1_train0: -623129.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -984727.0\n",
      "    agent-stage0_train1: -984727.0\n",
      "    agent-stage1_train0: -984727.0\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1179636.0\n",
      "    agent-stage0_train1: -1179636.0\n",
      "    agent-stage1_train0: -1179636.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.525806276850817\n",
      "    mean_inference_ms: 1.528007327883548\n",
      "    mean_processing_ms: 0.36137792736359686\n",
      "  time_since_restore: 870.832692861557\n",
      "  time_this_iter_s: 13.000477313995361\n",
      "  time_total_s: 870.832692861557\n",
      "  timers:\n",
      "    learn_throughput: 637.608\n",
      "    learn_time_ms: 6587.119\n",
      "    load_throughput: 281984.46\n",
      "    load_time_ms: 14.894\n",
      "    sample_throughput: 649.958\n",
      "    sample_time_ms: 6461.961\n",
      "    update_time_ms: 4.093\n",
      "  timestamp: 1608025223\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 277200\n",
      "  training_iteration: 66\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         870.833</td><td style=\"text-align: right;\">277200</td><td style=\"text-align: right;\">-2.95418e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-40-36\n",
      "  done: false\n",
      "  episode_len_mean: 436.27\n",
      "  episode_reward_max: -2168271.0\n",
      "  episode_reward_mean: -3041234.07\n",
      "  episode_reward_min: -3538908.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 530\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 9.165611345451907e-07\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 2.5851982172753196e-06\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.987611142340029e-08\n",
      "        model: {}\n",
      "        policy_loss: -0.0005034755449742079\n",
      "        total_loss: 24973090816.0\n",
      "        vf_explained_var: 1.862645149230957e-09\n",
      "        vf_loss: 24973090816.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8291411399841309\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014835892245173454\n",
      "        model: {}\n",
      "        policy_loss: 0.009609988890588284\n",
      "        total_loss: 25093818368.0\n",
      "        vf_explained_var: 3.3527612686157227e-08\n",
      "        vf_loss: 25093818368.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.2847656309604645\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.22954882681369781\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00421539694070816\n",
      "        model: {}\n",
      "        policy_loss: 0.004290684591978788\n",
      "        total_loss: 25001504768.0\n",
      "        vf_explained_var: -3.725290298461914e-09\n",
      "        vf_loss: 25001504768.0\n",
      "    num_steps_sampled: 281400\n",
      "    num_steps_trained: 281400\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.65555555555557\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -722757.0\n",
      "    agent-stage0_train1: -722757.0\n",
      "    agent-stage1_train0: -722757.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -1013744.69\n",
      "    agent-stage0_train1: -1013744.69\n",
      "    agent-stage1_train0: -1013744.69\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1179636.0\n",
      "    agent-stage0_train1: -1179636.0\n",
      "    agent-stage1_train0: -1179636.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5277776253295534\n",
      "    mean_inference_ms: 1.52664256423019\n",
      "    mean_processing_ms: 0.36122590752215833\n",
      "  time_since_restore: 883.7905507087708\n",
      "  time_this_iter_s: 12.957857847213745\n",
      "  time_total_s: 883.7905507087708\n",
      "  timers:\n",
      "    learn_throughput: 640.855\n",
      "    learn_time_ms: 6553.746\n",
      "    load_throughput: 282381.329\n",
      "    load_time_ms: 14.874\n",
      "    sample_throughput: 649.152\n",
      "    sample_time_ms: 6469.979\n",
      "    update_time_ms: 4.075\n",
      "  timestamp: 1608025236\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 281400\n",
      "  training_iteration: 67\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         883.791</td><td style=\"text-align: right;\">281400</td><td style=\"text-align: right;\">-3.04123e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-40-49\n",
      "  done: false\n",
      "  episode_len_mean: 430.08\n",
      "  episode_reward_max: -2553417.0\n",
      "  episode_reward_mean: -3106641.45\n",
      "  episode_reward_min: -3538908.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 540\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 4.5828056727259536e-07\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 3.3269809591729427e-06\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.081548230620683e-08\n",
      "        model: {}\n",
      "        policy_loss: 0.0006663128733634949\n",
      "        total_loss: 26785089536.0\n",
      "        vf_explained_var: 3.725290298461914e-09\n",
      "        vf_loss: 26785089536.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8339882493019104\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006022380664944649\n",
      "        model: {}\n",
      "        policy_loss: 0.0012852908112108707\n",
      "        total_loss: 26686056448.0\n",
      "        vf_explained_var: -2.7939677238464355e-08\n",
      "        vf_loss: 26686056448.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.14238281548023224\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.13913920521736145\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007232171017676592\n",
      "        model: {}\n",
      "        policy_loss: 0.0008969954214990139\n",
      "        total_loss: 26745905152.0\n",
      "        vf_explained_var: -2.7939677238464355e-08\n",
      "        vf_loss: 26745905152.0\n",
      "    num_steps_sampled: 285600\n",
      "    num_steps_trained: 285600\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.96842105263158\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -851139.0\n",
      "    agent-stage0_train1: -851139.0\n",
      "    agent-stage1_train0: -851139.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -1035547.15\n",
      "    agent-stage0_train1: -1035547.15\n",
      "    agent-stage1_train0: -1035547.15\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1179636.0\n",
      "    agent-stage0_train1: -1179636.0\n",
      "    agent-stage1_train0: -1179636.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.529883991959676\n",
      "    mean_inference_ms: 1.5258413002636737\n",
      "    mean_processing_ms: 0.36110669107046334\n",
      "  time_since_restore: 896.9517691135406\n",
      "  time_this_iter_s: 13.161218404769897\n",
      "  time_total_s: 896.9517691135406\n",
      "  timers:\n",
      "    learn_throughput: 639.931\n",
      "    learn_time_ms: 6563.211\n",
      "    load_throughput: 285633.996\n",
      "    load_time_ms: 14.704\n",
      "    sample_throughput: 647.84\n",
      "    sample_time_ms: 6483.083\n",
      "    update_time_ms: 4.043\n",
      "  timestamp: 1608025249\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 285600\n",
      "  training_iteration: 68\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         896.952</td><td style=\"text-align: right;\">285600</td><td style=\"text-align: right;\">-3.10664e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-41-02\n",
      "  done: false\n",
      "  episode_len_mean: 424.91\n",
      "  episode_reward_max: -2651694.0\n",
      "  episode_reward_mean: -3137673.81\n",
      "  episode_reward_min: -3538908.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 551\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 2.2914028363629768e-07\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.440271135943476e-06\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -2.219439387829425e-08\n",
      "        model: {}\n",
      "        policy_loss: -0.0012643395457416773\n",
      "        total_loss: 35542650880.0\n",
      "        vf_explained_var: -2.0489096641540527e-08\n",
      "        vf_loss: 35542650880.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.7844488024711609\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005008632317185402\n",
      "        model: {}\n",
      "        policy_loss: 0.0017983922734856606\n",
      "        total_loss: 35657531392.0\n",
      "        vf_explained_var: 1.30385160446167e-08\n",
      "        vf_loss: 35657531392.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.14238281548023224\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.11012015491724014\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0014943876303732395\n",
      "        model: {}\n",
      "        policy_loss: 0.0017440055962651968\n",
      "        total_loss: 35671846912.0\n",
      "        vf_explained_var: -3.725290298461914e-09\n",
      "        vf_loss: 35671846912.0\n",
      "    num_steps_sampled: 289800\n",
      "    num_steps_trained: 289800\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.45263157894736\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -883898.0\n",
      "    agent-stage0_train1: -883898.0\n",
      "    agent-stage1_train0: -883898.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -1045891.27\n",
      "    agent-stage0_train1: -1045891.27\n",
      "    agent-stage1_train0: -1045891.27\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1179636.0\n",
      "    agent-stage0_train1: -1179636.0\n",
      "    agent-stage1_train0: -1179636.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5320956258465697\n",
      "    mean_inference_ms: 1.5246246968668455\n",
      "    mean_processing_ms: 0.36095624492988326\n",
      "  time_since_restore: 910.0657029151917\n",
      "  time_this_iter_s: 13.113933801651001\n",
      "  time_total_s: 910.0657029151917\n",
      "  timers:\n",
      "    learn_throughput: 639.756\n",
      "    learn_time_ms: 6565.005\n",
      "    load_throughput: 286716.267\n",
      "    load_time_ms: 14.649\n",
      "    sample_throughput: 647.697\n",
      "    sample_time_ms: 6484.517\n",
      "    update_time_ms: 4.006\n",
      "  timestamp: 1608025262\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 289800\n",
      "  training_iteration: 69\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         910.066</td><td style=\"text-align: right;\">289800</td><td style=\"text-align: right;\">-3.13767e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-41-15\n",
      "  done: false\n",
      "  episode_len_mean: 420.79\n",
      "  episode_reward_max: -2651694.0\n",
      "  episode_reward_mean: -3160666.53\n",
      "  episode_reward_min: -3538908.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 561\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.1457014181814884e-07\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 2.6490824893699028e-05\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.573944458272308e-07\n",
      "        model: {}\n",
      "        policy_loss: -0.0008458616212010384\n",
      "        total_loss: 33640218624.0\n",
      "        vf_explained_var: 4.470348358154297e-08\n",
      "        vf_loss: 33640218624.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6792170405387878\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010656839236617088\n",
      "        model: {}\n",
      "        policy_loss: 0.0057637859135866165\n",
      "        total_loss: 33696174080.0\n",
      "        vf_explained_var: -5.587935447692871e-08\n",
      "        vf_loss: 33696174080.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.07119140774011612\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.21073462069034576\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012677758932113647\n",
      "        model: {}\n",
      "        policy_loss: 0.009020625613629818\n",
      "        total_loss: 33804242944.0\n",
      "        vf_explained_var: -5.587935447692871e-08\n",
      "        vf_loss: 33804242944.0\n",
      "    num_steps_sampled: 294000\n",
      "    num_steps_trained: 294000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.15263157894738\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -883898.0\n",
      "    agent-stage0_train1: -883898.0\n",
      "    agent-stage1_train0: -883898.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -1053555.51\n",
      "    agent-stage0_train1: -1053555.51\n",
      "    agent-stage1_train0: -1053555.51\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1179636.0\n",
      "    agent-stage0_train1: -1179636.0\n",
      "    agent-stage1_train0: -1179636.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5342052780668247\n",
      "    mean_inference_ms: 1.523892599959747\n",
      "    mean_processing_ms: 0.3608467197289091\n",
      "  time_since_restore: 923.3226189613342\n",
      "  time_this_iter_s: 13.256916046142578\n",
      "  time_total_s: 923.3226189613342\n",
      "  timers:\n",
      "    learn_throughput: 639.554\n",
      "    learn_time_ms: 6567.079\n",
      "    load_throughput: 286790.484\n",
      "    load_time_ms: 14.645\n",
      "    sample_throughput: 646.386\n",
      "    sample_time_ms: 6497.664\n",
      "    update_time_ms: 3.976\n",
      "  timestamp: 1608025275\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 294000\n",
      "  training_iteration: 70\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         923.323</td><td style=\"text-align: right;\">294000</td><td style=\"text-align: right;\">-3.16067e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-41-29\n",
      "  done: false\n",
      "  episode_len_mean: 417.99\n",
      "  episode_reward_max: -2651694.0\n",
      "  episode_reward_mean: -3136739.19\n",
      "  episode_reward_min: -3538908.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 570\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 5.728507090907442e-08\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.950945895256577e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.244698357069865e-05\n",
      "        model: {}\n",
      "        policy_loss: 0.002558558713644743\n",
      "        total_loss: 30092011520.0\n",
      "        vf_explained_var: 4.6566128730773926e-08\n",
      "        vf_loss: 30092011520.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.8488460779190063\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012155868113040924\n",
      "        model: {}\n",
      "        policy_loss: 0.005262152291834354\n",
      "        total_loss: 29981222912.0\n",
      "        vf_explained_var: 3.3527612686157227e-08\n",
      "        vf_loss: 29981222912.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.07119140774011612\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.08311305940151215\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.044186078011989594\n",
      "        model: {}\n",
      "        policy_loss: 0.00939648412168026\n",
      "        total_loss: 30046126080.0\n",
      "        vf_explained_var: 2.0489096641540527e-08\n",
      "        vf_loss: 30046126080.0\n",
      "    num_steps_sampled: 298200\n",
      "    num_steps_trained: 298200\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.45789473684209\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -883898.0\n",
      "    agent-stage0_train1: -883898.0\n",
      "    agent-stage1_train0: -883898.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -1045579.73\n",
      "    agent-stage0_train1: -1045579.73\n",
      "    agent-stage1_train0: -1045579.73\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1179636.0\n",
      "    agent-stage0_train1: -1179636.0\n",
      "    agent-stage1_train0: -1179636.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5359583814948663\n",
      "    mean_inference_ms: 1.522904350505102\n",
      "    mean_processing_ms: 0.3607225109730092\n",
      "  time_since_restore: 936.4868886470795\n",
      "  time_this_iter_s: 13.16426968574524\n",
      "  time_total_s: 936.4868886470795\n",
      "  timers:\n",
      "    learn_throughput: 641.174\n",
      "    learn_time_ms: 6550.487\n",
      "    load_throughput: 286076.979\n",
      "    load_time_ms: 14.681\n",
      "    sample_throughput: 645.629\n",
      "    sample_time_ms: 6505.281\n",
      "    update_time_ms: 3.944\n",
      "  timestamp: 1608025289\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 298200\n",
      "  training_iteration: 71\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         936.487</td><td style=\"text-align: right;\">298200</td><td style=\"text-align: right;\">-3.13674e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-41-42\n",
      "  done: false\n",
      "  episode_len_mean: 413.11\n",
      "  episode_reward_max: -2452098.0\n",
      "  episode_reward_mean: -3071306.64\n",
      "  episode_reward_min: -3538908.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 582\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 2.864253545453721e-08\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.9456107532062106e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.3642615396581956e-18\n",
      "        model: {}\n",
      "        policy_loss: -0.001563320867717266\n",
      "        total_loss: 26029555712.0\n",
      "        vf_explained_var: -2.60770320892334e-08\n",
      "        vf_loss: 26029555712.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.7132092714309692\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005181030370295048\n",
      "        model: {}\n",
      "        policy_loss: -0.0008855655323714018\n",
      "        total_loss: 26018299904.0\n",
      "        vf_explained_var: -1.4901161193847656e-08\n",
      "        vf_loss: 26018299904.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.10678710788488388\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.014232655987143517\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0009419239358976483\n",
      "        model: {}\n",
      "        policy_loss: -0.001869870349764824\n",
      "        total_loss: 26021595136.0\n",
      "        vf_explained_var: -2.2351741790771484e-08\n",
      "        vf_loss: 26021595136.0\n",
      "    num_steps_sampled: 302400\n",
      "    num_steps_trained: 302400\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.39473684210526\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -817366.0\n",
      "    agent-stage0_train1: -817366.0\n",
      "    agent-stage1_train0: -817366.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -1023768.88\n",
      "    agent-stage0_train1: -1023768.88\n",
      "    agent-stage1_train0: -1023768.88\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1179636.0\n",
      "    agent-stage0_train1: -1179636.0\n",
      "    agent-stage1_train0: -1179636.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.538572715978577\n",
      "    mean_inference_ms: 1.521920936915099\n",
      "    mean_processing_ms: 0.36057477913045566\n",
      "  time_since_restore: 949.7845890522003\n",
      "  time_this_iter_s: 13.29770040512085\n",
      "  time_total_s: 949.7845890522003\n",
      "  timers:\n",
      "    learn_throughput: 640.795\n",
      "    learn_time_ms: 6554.358\n",
      "    load_throughput: 284794.946\n",
      "    load_time_ms: 14.747\n",
      "    sample_throughput: 644.78\n",
      "    sample_time_ms: 6513.853\n",
      "    update_time_ms: 3.916\n",
      "  timestamp: 1608025302\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 302400\n",
      "  training_iteration: 72\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         949.785</td><td style=\"text-align: right;\">302400</td><td style=\"text-align: right;\">-3.07131e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-41-55\n",
      "  done: false\n",
      "  episode_len_mean: 407.72\n",
      "  episode_reward_max: -2444025.0\n",
      "  episode_reward_mean: -3003612.54\n",
      "  episode_reward_min: -3425760.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 594\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.4321267727268605e-08\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.945653687612241e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -2.9047644553736237e-18\n",
      "        model: {}\n",
      "        policy_loss: 0.0023324675858020782\n",
      "        total_loss: 30377123840.0\n",
      "        vf_explained_var: -1.862645149230957e-09\n",
      "        vf_loss: 30377123840.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6919741630554199\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01798751950263977\n",
      "        model: {}\n",
      "        policy_loss: 0.00195368193089962\n",
      "        total_loss: 30177972224.0\n",
      "        vf_explained_var: -3.3527612686157227e-08\n",
      "        vf_loss: 30177972224.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.05339355394244194\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.016437996178865433\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0006285271956585348\n",
      "        model: {}\n",
      "        policy_loss: 0.0019097058102488518\n",
      "        total_loss: 30328061952.0\n",
      "        vf_explained_var: -5.587935447692871e-09\n",
      "        vf_loss: 30328061952.0\n",
      "    num_steps_sampled: 306600\n",
      "    num_steps_trained: 306600\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.50526315789473\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -814675.0\n",
      "    agent-stage0_train1: -814675.0\n",
      "    agent-stage1_train0: -814675.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -1001204.18\n",
      "    agent-stage0_train1: -1001204.18\n",
      "    agent-stage1_train0: -1001204.18\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1141920.0\n",
      "    agent-stage0_train1: -1141920.0\n",
      "    agent-stage1_train0: -1141920.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.541085775323576\n",
      "    mean_inference_ms: 1.5209103515059985\n",
      "    mean_processing_ms: 0.36042674343486636\n",
      "  time_since_restore: 962.8587169647217\n",
      "  time_this_iter_s: 13.074127912521362\n",
      "  time_total_s: 962.8587169647217\n",
      "  timers:\n",
      "    learn_throughput: 641.704\n",
      "    learn_time_ms: 6545.076\n",
      "    load_throughput: 282662.704\n",
      "    load_time_ms: 14.859\n",
      "    sample_throughput: 643.858\n",
      "    sample_time_ms: 6523.174\n",
      "    update_time_ms: 3.918\n",
      "  timestamp: 1608025315\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 306600\n",
      "  training_iteration: 73\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         962.859</td><td style=\"text-align: right;\">306600</td><td style=\"text-align: right;\">-3.00361e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-42-08\n",
      "  done: false\n",
      "  episode_len_mean: 402.81\n",
      "  episode_reward_max: -2444025.0\n",
      "  episode_reward_mean: -2986179.09\n",
      "  episode_reward_min: -3367515.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 603\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 7.1606338636343025e-09\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.950295807633955e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -2.3386521771048354e-18\n",
      "        model: {}\n",
      "        policy_loss: -0.00018094293773174286\n",
      "        total_loss: 33254701056.0\n",
      "        vf_explained_var: 3.725290298461914e-09\n",
      "        vf_loss: 33254701056.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.854296863079071\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5663130283355713\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0037777707912027836\n",
      "        model: {}\n",
      "        policy_loss: -0.00012464169412851334\n",
      "        total_loss: 33143103488.0\n",
      "        vf_explained_var: -2.7939677238464355e-08\n",
      "        vf_loss: 33143103488.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.02669677697122097\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.0007653877837583423\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008495011366903782\n",
      "        model: {}\n",
      "        policy_loss: 0.004249614663422108\n",
      "        total_loss: 33316882432.0\n",
      "        vf_explained_var: -1.30385160446167e-08\n",
      "        vf_loss: 33316882432.0\n",
      "    num_steps_sampled: 310800\n",
      "    num_steps_trained: 310800\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.95\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -814675.0\n",
      "    agent-stage0_train1: -814675.0\n",
      "    agent-stage1_train0: -814675.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -995393.03\n",
      "    agent-stage0_train1: -995393.03\n",
      "    agent-stage1_train0: -995393.03\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1122505.0\n",
      "    agent-stage0_train1: -1122505.0\n",
      "    agent-stage1_train0: -1122505.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5432807614032513\n",
      "    mean_inference_ms: 1.5201954205233148\n",
      "    mean_processing_ms: 0.36031695553852744\n",
      "  time_since_restore: 976.0124204158783\n",
      "  time_this_iter_s: 13.153703451156616\n",
      "  time_total_s: 976.0124204158783\n",
      "  timers:\n",
      "    learn_throughput: 641.068\n",
      "    learn_time_ms: 6551.567\n",
      "    load_throughput: 302810.244\n",
      "    load_time_ms: 13.87\n",
      "    sample_throughput: 642.621\n",
      "    sample_time_ms: 6535.738\n",
      "    update_time_ms: 3.872\n",
      "  timestamp: 1608025328\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 310800\n",
      "  training_iteration: 74\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         976.012</td><td style=\"text-align: right;\">310800</td><td style=\"text-align: right;\">-2.98618e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-42-22\n",
      "  done: false\n",
      "  episode_len_mean: 397.59\n",
      "  episode_reward_max: -2444025.0\n",
      "  episode_reward_mean: -2979844.2\n",
      "  episode_reward_min: -3367515.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 615\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 3.5803169318171513e-09\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.946474211816378e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -3.1416418926437497e-18\n",
      "        model: {}\n",
      "        policy_loss: -5.946727469563484e-05\n",
      "        total_loss: 28648558592.0\n",
      "        vf_explained_var: -3.725290298461914e-09\n",
      "        vf_loss: 28648558592.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.4271484315395355\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5911374688148499\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009375945664942265\n",
      "        model: {}\n",
      "        policy_loss: 0.003458264283835888\n",
      "        total_loss: 28684111872.0\n",
      "        vf_explained_var: 9.313225746154785e-09\n",
      "        vf_loss: 28684111872.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.02669677697122097\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.0030284072272479534\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0003866288170684129\n",
      "        model: {}\n",
      "        policy_loss: 0.0008772481232881546\n",
      "        total_loss: 28679364608.0\n",
      "        vf_explained_var: 1.1175870895385742e-08\n",
      "        vf_loss: 28679364608.0\n",
      "    num_steps_sampled: 315000\n",
      "    num_steps_trained: 315000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.8842105263158\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -814675.0\n",
      "    agent-stage0_train1: -814675.0\n",
      "    agent-stage1_train0: -814675.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -993281.4\n",
      "    agent-stage0_train1: -993281.4\n",
      "    agent-stage1_train0: -993281.4\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1122505.0\n",
      "    agent-stage0_train1: -1122505.0\n",
      "    agent-stage1_train0: -1122505.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.54590014410459\n",
      "    mean_inference_ms: 1.5188162089689559\n",
      "    mean_processing_ms: 0.36013987732369945\n",
      "  time_since_restore: 989.1804959774017\n",
      "  time_this_iter_s: 13.168075561523438\n",
      "  time_total_s: 989.1804959774017\n",
      "  timers:\n",
      "    learn_throughput: 642.324\n",
      "    learn_time_ms: 6538.754\n",
      "    load_throughput: 300755.245\n",
      "    load_time_ms: 13.965\n",
      "    sample_throughput: 639.753\n",
      "    sample_time_ms: 6565.038\n",
      "    update_time_ms: 3.921\n",
      "  timestamp: 1608025342\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 315000\n",
      "  training_iteration: 75\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">          989.18</td><td style=\"text-align: right;\">315000</td><td style=\"text-align: right;\">-2.97984e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-42-35\n",
      "  done: false\n",
      "  episode_len_mean: 392.59\n",
      "  episode_reward_max: -2444025.0\n",
      "  episode_reward_mean: -2959092.63\n",
      "  episode_reward_min: -3367515.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 627\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.7901584659085756e-09\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.945782490830332e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -2.1390903048330484e-18\n",
      "        model: {}\n",
      "        policy_loss: -0.0021619093604385853\n",
      "        total_loss: 27020754944.0\n",
      "        vf_explained_var: 7.450580596923828e-09\n",
      "        vf_loss: 27020754944.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.4271484315395355\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6577333807945251\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017749177291989326\n",
      "        model: {}\n",
      "        policy_loss: 0.010332990437746048\n",
      "        total_loss: 27164753920.0\n",
      "        vf_explained_var: 1.862645149230957e-09\n",
      "        vf_loss: 27164753920.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.013348388485610485\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.0014419090002775192\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.132504055742174e-05\n",
      "        model: {}\n",
      "        policy_loss: -0.002732168883085251\n",
      "        total_loss: 26997852160.0\n",
      "        vf_explained_var: -1.1175870895385742e-08\n",
      "        vf_loss: 26997852160.0\n",
      "    num_steps_sampled: 319200\n",
      "    num_steps_trained: 319200\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.08947368421052\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -814675.0\n",
      "    agent-stage0_train1: -814675.0\n",
      "    agent-stage1_train0: -814675.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -986364.21\n",
      "    agent-stage0_train1: -986364.21\n",
      "    agent-stage1_train0: -986364.21\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1122505.0\n",
      "    agent-stage0_train1: -1122505.0\n",
      "    agent-stage1_train0: -1122505.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5487518800493456\n",
      "    mean_inference_ms: 1.5178175012548396\n",
      "    mean_processing_ms: 0.3600007521689515\n",
      "  time_since_restore: 1002.2891280651093\n",
      "  time_this_iter_s: 13.10863208770752\n",
      "  time_total_s: 1002.2891280651093\n",
      "  timers:\n",
      "    learn_throughput: 642.557\n",
      "    learn_time_ms: 6536.386\n",
      "    load_throughput: 301286.599\n",
      "    load_time_ms: 13.94\n",
      "    sample_throughput: 638.49\n",
      "    sample_time_ms: 6578.018\n",
      "    update_time_ms: 3.944\n",
      "  timestamp: 1608025355\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 319200\n",
      "  training_iteration: 76\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         1002.29</td><td style=\"text-align: right;\">319200</td><td style=\"text-align: right;\">-2.95909e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-42-48\n",
      "  done: false\n",
      "  episode_len_mean: 388.72\n",
      "  episode_reward_max: -2444025.0\n",
      "  episode_reward_mean: -2939629.95\n",
      "  episode_reward_min: -3367515.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 638\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 8.950792329542878e-10\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.94742614132382e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -4.642343152029811e-18\n",
      "        model: {}\n",
      "        policy_loss: -7.135444320738316e-05\n",
      "        total_loss: 33194553344.0\n",
      "        vf_explained_var: -9.313225746154785e-09\n",
      "        vf_loss: 33194553344.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.4271484315395355\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6175550222396851\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0025567063130438328\n",
      "        model: {}\n",
      "        policy_loss: 0.002661401405930519\n",
      "        total_loss: 33257934848.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 33257934848.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.0066741942428052425\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 8.615000115241855e-05\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0315265953540802\n",
      "        model: {}\n",
      "        policy_loss: 0.007453778758645058\n",
      "        total_loss: 33320611840.0\n",
      "        vf_explained_var: 3.725290298461914e-09\n",
      "        vf_loss: 33320611840.0\n",
      "    num_steps_sampled: 323400\n",
      "    num_steps_trained: 323400\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.49473684210527\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -814675.0\n",
      "    agent-stage0_train1: -814675.0\n",
      "    agent-stage1_train0: -814675.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -979876.65\n",
      "    agent-stage0_train1: -979876.65\n",
      "    agent-stage1_train0: -979876.65\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1122505.0\n",
      "    agent-stage0_train1: -1122505.0\n",
      "    agent-stage1_train0: -1122505.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.551331460045125\n",
      "    mean_inference_ms: 1.5166275435160657\n",
      "    mean_processing_ms: 0.3598550267227169\n",
      "  time_since_restore: 1015.4793000221252\n",
      "  time_this_iter_s: 13.190171957015991\n",
      "  time_total_s: 1015.4793000221252\n",
      "  timers:\n",
      "    learn_throughput: 642.008\n",
      "    learn_time_ms: 6541.97\n",
      "    load_throughput: 303869.538\n",
      "    load_time_ms: 13.822\n",
      "    sample_throughput: 636.782\n",
      "    sample_time_ms: 6595.663\n",
      "    update_time_ms: 3.962\n",
      "  timestamp: 1608025368\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 323400\n",
      "  training_iteration: 77\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1015.48</td><td style=\"text-align: right;\">323400</td><td style=\"text-align: right;\">-2.93963e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-43-01\n",
      "  done: false\n",
      "  episode_len_mean: 385.43\n",
      "  episode_reward_max: -2444025.0\n",
      "  episode_reward_mean: -2934351.96\n",
      "  episode_reward_min: -3367515.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 648\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 4.475396164771439e-10\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.9506310429456875e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -5.815505704263249e-18\n",
      "        model: {}\n",
      "        policy_loss: 0.0004939334467053413\n",
      "        total_loss: 33835532288.0\n",
      "        vf_explained_var: -1.30385160446167e-08\n",
      "        vf_loss: 33835532288.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.21357421576976776\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5052882432937622\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006594531238079071\n",
      "        model: {}\n",
      "        policy_loss: -0.0013675920199602842\n",
      "        total_loss: 33657237504.0\n",
      "        vf_explained_var: -2.60770320892334e-08\n",
      "        vf_loss: 33657237504.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.01001129113137722\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 2.4085335098789074e-05\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2336041436356027e-05\n",
      "        model: {}\n",
      "        policy_loss: 3.06831207126379e-05\n",
      "        total_loss: 33827436544.0\n",
      "        vf_explained_var: -7.450580596923828e-09\n",
      "        vf_loss: 33827436544.0\n",
      "    num_steps_sampled: 327600\n",
      "    num_steps_trained: 327600\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.6157894736842\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -814675.0\n",
      "    agent-stage0_train1: -814675.0\n",
      "    agent-stage1_train0: -814675.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -978117.32\n",
      "    agent-stage0_train1: -978117.32\n",
      "    agent-stage1_train0: -978117.32\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1122505.0\n",
      "    agent-stage0_train1: -1122505.0\n",
      "    agent-stage1_train0: -1122505.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5539606023735546\n",
      "    mean_inference_ms: 1.5161076175285655\n",
      "    mean_processing_ms: 0.3597635491470827\n",
      "  time_since_restore: 1028.550540447235\n",
      "  time_this_iter_s: 13.071240425109863\n",
      "  time_total_s: 1028.550540447235\n",
      "  timers:\n",
      "    learn_throughput: 643.028\n",
      "    learn_time_ms: 6531.599\n",
      "    load_throughput: 304000.635\n",
      "    load_time_ms: 13.816\n",
      "    sample_throughput: 636.655\n",
      "    sample_time_ms: 6596.981\n",
      "    update_time_ms: 4.042\n",
      "  timestamp: 1608025381\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 327600\n",
      "  training_iteration: 78\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         1028.55</td><td style=\"text-align: right;\">327600</td><td style=\"text-align: right;\">-2.93435e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-43-14\n",
      "  done: false\n",
      "  episode_len_mean: 382.0\n",
      "  episode_reward_max: -2444025.0\n",
      "  episode_reward_mean: -2925854.94\n",
      "  episode_reward_min: -3366960.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 660\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 2.2376980823857195e-10\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.94624913144537e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -8.979723010184796e-18\n",
      "        model: {}\n",
      "        policy_loss: 0.0014704982750117779\n",
      "        total_loss: 32009666560.0\n",
      "        vf_explained_var: -3.725290298461914e-09\n",
      "        vf_loss: 32009666560.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.21357421576976776\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.500436544418335\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022215627133846283\n",
      "        model: {}\n",
      "        policy_loss: 0.008515697903931141\n",
      "        total_loss: 31993090048.0\n",
      "        vf_explained_var: 9.313225746154785e-09\n",
      "        vf_loss: 31993090048.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.00500564556568861\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0199051757808775e-05\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.244562319399847e-07\n",
      "        model: {}\n",
      "        policy_loss: -0.0026475563645362854\n",
      "        total_loss: 31952158720.0\n",
      "        vf_explained_var: -9.313225746154785e-09\n",
      "        vf_loss: 31952158720.0\n",
      "    num_steps_sampled: 331800\n",
      "    num_steps_trained: 331800\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.78947368421052\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -814675.0\n",
      "    agent-stage0_train1: -814675.0\n",
      "    agent-stage1_train0: -814675.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -975284.98\n",
      "    agent-stage0_train1: -975284.98\n",
      "    agent-stage1_train0: -975284.98\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1122320.0\n",
      "    agent-stage0_train1: -1122320.0\n",
      "    agent-stage1_train0: -1122320.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5571116042421282\n",
      "    mean_inference_ms: 1.515045001853018\n",
      "    mean_processing_ms: 0.3596235932316385\n",
      "  time_since_restore: 1041.897799730301\n",
      "  time_this_iter_s: 13.347259283065796\n",
      "  time_total_s: 1041.897799730301\n",
      "  timers:\n",
      "    learn_throughput: 641.635\n",
      "    learn_time_ms: 6545.776\n",
      "    load_throughput: 301101.209\n",
      "    load_time_ms: 13.949\n",
      "    sample_throughput: 635.795\n",
      "    sample_time_ms: 6605.908\n",
      "    update_time_ms: 4.023\n",
      "  timestamp: 1608025394\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 331800\n",
      "  training_iteration: 79\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">          1041.9</td><td style=\"text-align: right;\">331800</td><td style=\"text-align: right;\">-2.92585e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-43-28\n",
      "  done: false\n",
      "  episode_len_mean: 378.34\n",
      "  episode_reward_max: -2444025.0\n",
      "  episode_reward_mean: -2931809.16\n",
      "  episode_reward_min: -3366960.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 672\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.1188490411928598e-10\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.947304710680502e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.2258122673501938e-17\n",
      "        model: {}\n",
      "        policy_loss: 0.004133767448365688\n",
      "        total_loss: 35486212096.0\n",
      "        vf_explained_var: -9.313225746154785e-09\n",
      "        vf_loss: 35486212096.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5605034828186035\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01009310595691204\n",
      "        model: {}\n",
      "        policy_loss: 0.0028066462837159634\n",
      "        total_loss: 35385561088.0\n",
      "        vf_explained_var: 1.862645149230957e-09\n",
      "        vf_loss: 35385561088.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.002502822782844305\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 4.777047251991462e-06\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.3883199301290006e-07\n",
      "        model: {}\n",
      "        policy_loss: -0.00021481700241565704\n",
      "        total_loss: 35329728512.0\n",
      "        vf_explained_var: 1.862645149230957e-09\n",
      "        vf_loss: 35329728512.0\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.69473684210527\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -814675.0\n",
      "    agent-stage0_train1: -814675.0\n",
      "    agent-stage1_train0: -814675.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -977269.72\n",
      "    agent-stage0_train1: -977269.72\n",
      "    agent-stage1_train0: -977269.72\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1122320.0\n",
      "    agent-stage0_train1: -1122320.0\n",
      "    agent-stage1_train0: -1122320.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.560228036963078\n",
      "    mean_inference_ms: 1.5143510145719064\n",
      "    mean_processing_ms: 0.35951660268745955\n",
      "  time_since_restore: 1054.97864985466\n",
      "  time_this_iter_s: 13.08085012435913\n",
      "  time_total_s: 1054.97864985466\n",
      "  timers:\n",
      "    learn_throughput: 641.829\n",
      "    learn_time_ms: 6543.804\n",
      "    load_throughput: 301431.465\n",
      "    load_time_ms: 13.934\n",
      "    sample_throughput: 637.262\n",
      "    sample_time_ms: 6590.691\n",
      "    update_time_ms: 4.029\n",
      "  timestamp: 1608025408\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 80\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         1054.98</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\">-2.93181e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-43-41\n",
      "  done: false\n",
      "  episode_len_mean: 376.7\n",
      "  episode_reward_max: -2444025.0\n",
      "  episode_reward_mean: -2935247.31\n",
      "  episode_reward_min: -3366960.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 682\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 5.594245205964299e-11\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.949782763165935e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -4.922613036668397e-18\n",
      "        model: {}\n",
      "        policy_loss: -0.0002122391015291214\n",
      "        total_loss: 28284686336.0\n",
      "        vf_explained_var: -9.313225746154785e-09\n",
      "        vf_loss: 28284686336.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6094993948936462\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014018429443240166\n",
      "        model: {}\n",
      "        policy_loss: 0.00542169064283371\n",
      "        total_loss: 28276244480.0\n",
      "        vf_explained_var: -1.862645149230957e-09\n",
      "        vf_loss: 28276244480.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.0012514113914221525\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.9595147477957653e-06\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.740127117955126e-07\n",
      "        model: {}\n",
      "        policy_loss: -0.004306372255086899\n",
      "        total_loss: 28145530880.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 28145530880.0\n",
      "    num_steps_sampled: 340200\n",
      "    num_steps_trained: 340200\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.96666666666665\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -814675.0\n",
      "    agent-stage0_train1: -814675.0\n",
      "    agent-stage1_train0: -814675.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -978415.77\n",
      "    agent-stage0_train1: -978415.77\n",
      "    agent-stage1_train0: -978415.77\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1122320.0\n",
      "    agent-stage0_train1: -1122320.0\n",
      "    agent-stage1_train0: -1122320.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.562727746990618\n",
      "    mean_inference_ms: 1.513152626008781\n",
      "    mean_processing_ms: 0.35938064847017787\n",
      "  time_since_restore: 1068.081152677536\n",
      "  time_this_iter_s: 13.102502822875977\n",
      "  time_total_s: 1068.081152677536\n",
      "  timers:\n",
      "    learn_throughput: 642.007\n",
      "    learn_time_ms: 6541.985\n",
      "    load_throughput: 299863.938\n",
      "    load_time_ms: 14.006\n",
      "    sample_throughput: 637.61\n",
      "    sample_time_ms: 6587.097\n",
      "    update_time_ms: 4.067\n",
      "  timestamp: 1608025421\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340200\n",
      "  training_iteration: 81\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1068.08</td><td style=\"text-align: right;\">340200</td><td style=\"text-align: right;\">-2.93525e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-43-54\n",
      "  done: false\n",
      "  episode_len_mean: 375.79\n",
      "  episode_reward_max: -2027904.0\n",
      "  episode_reward_mean: -2878069.65\n",
      "  episode_reward_min: -3366960.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 693\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 2.7971226029821494e-11\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.948558482072764e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.2043724903353706e-17\n",
      "        model: {}\n",
      "        policy_loss: 0.001152952667325735\n",
      "        total_loss: 16683718656.0\n",
      "        vf_explained_var: -5.587935447692871e-09\n",
      "        vf_loss: 16683718656.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6845093965530396\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019511757418513298\n",
      "        model: {}\n",
      "        policy_loss: 0.004416419193148613\n",
      "        total_loss: 16702032896.0\n",
      "        vf_explained_var: 9.313225746154785e-09\n",
      "        vf_loss: 16702032896.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.0006257056957110763\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.1468205229903106e-06\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -6.432736654460314e-08\n",
      "        model: {}\n",
      "        policy_loss: -0.0027880484703928232\n",
      "        total_loss: 16654180352.0\n",
      "        vf_explained_var: 1.6763806343078613e-08\n",
      "        vf_loss: 16654180352.0\n",
      "    num_steps_sampled: 344400\n",
      "    num_steps_trained: 344400\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.8\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -675968.0\n",
      "    agent-stage0_train1: -675968.0\n",
      "    agent-stage1_train0: -675968.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -959356.55\n",
      "    agent-stage0_train1: -959356.55\n",
      "    agent-stage1_train0: -959356.55\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1122320.0\n",
      "    agent-stage0_train1: -1122320.0\n",
      "    agent-stage1_train0: -1122320.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5653739094699186\n",
      "    mean_inference_ms: 1.5122773086478847\n",
      "    mean_processing_ms: 0.3592583552012746\n",
      "  time_since_restore: 1081.0587830543518\n",
      "  time_this_iter_s: 12.977630376815796\n",
      "  time_total_s: 1081.0587830543518\n",
      "  timers:\n",
      "    learn_throughput: 643.897\n",
      "    learn_time_ms: 6522.784\n",
      "    load_throughput: 302407.902\n",
      "    load_time_ms: 13.889\n",
      "    sample_throughput: 638.838\n",
      "    sample_time_ms: 6574.44\n",
      "    update_time_ms: 4.086\n",
      "  timestamp: 1608025434\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 344400\n",
      "  training_iteration: 82\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         1081.06</td><td style=\"text-align: right;\">344400</td><td style=\"text-align: right;\">-2.87807e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-44-07\n",
      "  done: false\n",
      "  episode_len_mean: 375.59\n",
      "  episode_reward_max: -1757979.0\n",
      "  episode_reward_mean: -2744504.01\n",
      "  episode_reward_min: -3366960.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 705\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.3985613014910747e-11\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.947211035612799e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -7.67347991869107e-18\n",
      "        model: {}\n",
      "        policy_loss: -0.0011224765330553055\n",
      "        total_loss: 12929661952.0\n",
      "        vf_explained_var: 5.587935447692871e-09\n",
      "        vf_loss: 12929661952.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.32036131620407104\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6843343377113342\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.027247406542301178\n",
      "        model: {}\n",
      "        policy_loss: 0.0041975658386945724\n",
      "        total_loss: 12978808832.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 12978808832.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.00031285284785553813\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 9.265498874810874e-07\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0042298548285089e-08\n",
      "        model: {}\n",
      "        policy_loss: 0.00011216104030609131\n",
      "        total_loss: 12940384256.0\n",
      "        vf_explained_var: 5.587935447692871e-09\n",
      "        vf_loss: 12940384256.0\n",
      "    num_steps_sampled: 348600\n",
      "    num_steps_trained: 348600\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.63684210526317\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -585993.0\n",
      "    agent-stage0_train1: -585993.0\n",
      "    agent-stage1_train0: -585993.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -914834.67\n",
      "    agent-stage0_train1: -914834.67\n",
      "    agent-stage1_train0: -914834.67\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1122320.0\n",
      "    agent-stage0_train1: -1122320.0\n",
      "    agent-stage1_train0: -1122320.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.568389704851721\n",
      "    mean_inference_ms: 1.511534524586146\n",
      "    mean_processing_ms: 0.3591576647001671\n",
      "  time_since_restore: 1094.1047480106354\n",
      "  time_this_iter_s: 13.04596495628357\n",
      "  time_total_s: 1094.1047480106354\n",
      "  timers:\n",
      "    learn_throughput: 643.269\n",
      "    learn_time_ms: 6529.152\n",
      "    load_throughput: 307491.705\n",
      "    load_time_ms: 13.659\n",
      "    sample_throughput: 639.711\n",
      "    sample_time_ms: 6565.468\n",
      "    update_time_ms: 4.062\n",
      "  timestamp: 1608025447\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 348600\n",
      "  training_iteration: 83\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">          1094.1</td><td style=\"text-align: right;\">348600</td><td style=\"text-align: right;\">-2.7445e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-44-20\n",
      "  done: false\n",
      "  episode_len_mean: 375.1\n",
      "  episode_reward_max: -1155129.0\n",
      "  episode_reward_mean: -2556198.12\n",
      "  episode_reward_min: -3366960.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 717\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 6.9928065074553736e-12\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.948127836969853e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -7.283801749562239e-18\n",
      "        model: {}\n",
      "        policy_loss: -0.0021247537806630135\n",
      "        total_loss: 7752245760.0\n",
      "        vf_explained_var: -2.60770320892334e-08\n",
      "        vf_loss: 7752245760.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.48054200410842896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.678494930267334\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016267970204353333\n",
      "        model: {}\n",
      "        policy_loss: 0.004435881040990353\n",
      "        total_loss: 7793105920.0\n",
      "        vf_explained_var: 1.30385160446167e-08\n",
      "        vf_loss: 7793105920.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 0.00015642642392776906\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.54203745398263e-07\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.4488910515808584e-08\n",
      "        model: {}\n",
      "        policy_loss: -0.0013459413312375546\n",
      "        total_loss: 7768234496.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 7768234496.0\n",
      "    num_steps_sampled: 352800\n",
      "    num_steps_trained: 352800\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.13888888888887\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -385043.0\n",
      "    agent-stage0_train1: -385043.0\n",
      "    agent-stage1_train0: -385043.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -852066.04\n",
      "    agent-stage0_train1: -852066.04\n",
      "    agent-stage1_train0: -852066.04\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1122320.0\n",
      "    agent-stage0_train1: -1122320.0\n",
      "    agent-stage1_train0: -1122320.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.571208159218961\n",
      "    mean_inference_ms: 1.5104843752946249\n",
      "    mean_processing_ms: 0.35903362557347057\n",
      "  time_since_restore: 1107.0951943397522\n",
      "  time_this_iter_s: 12.990446329116821\n",
      "  time_total_s: 1107.0951943397522\n",
      "  timers:\n",
      "    learn_throughput: 644.859\n",
      "    learn_time_ms: 6513.052\n",
      "    load_throughput: 309508.468\n",
      "    load_time_ms: 13.57\n",
      "    sample_throughput: 639.729\n",
      "    sample_time_ms: 6565.279\n",
      "    update_time_ms: 4.085\n",
      "  timestamp: 1608025460\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 352800\n",
      "  training_iteration: 84\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">          1107.1</td><td style=\"text-align: right;\">352800</td><td style=\"text-align: right;\">-2.5562e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-44-33\n",
      "  done: false\n",
      "  episode_len_mean: 374.94\n",
      "  episode_reward_max: -1135629.0\n",
      "  episode_reward_mean: -2461505.76\n",
      "  episode_reward_min: -3366960.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 726\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 3.4964032537276868e-12\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.952356659123415e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.2942613803208956e-17\n",
      "        model: {}\n",
      "        policy_loss: 0.0025616688653826714\n",
      "        total_loss: 11171558400.0\n",
      "        vf_explained_var: -3.725290298461914e-08\n",
      "        vf_loss: 11171558400.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.48054200410842896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.7139196991920471\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010941342450678349\n",
      "        model: {}\n",
      "        policy_loss: -0.0050967601127922535\n",
      "        total_loss: 11110365184.0\n",
      "        vf_explained_var: -5.587935447692871e-08\n",
      "        vf_loss: 11110365184.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 7.821321196388453e-05\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.2793826726920088e-06\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.533056324225072e-08\n",
      "        model: {}\n",
      "        policy_loss: 0.002338855527341366\n",
      "        total_loss: 11155652608.0\n",
      "        vf_explained_var: -3.725290298461914e-08\n",
      "        vf_loss: 11155652608.0\n",
      "    num_steps_sampled: 357000\n",
      "    num_steps_trained: 357000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.46315789473685\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -378543.0\n",
      "    agent-stage0_train1: -378543.0\n",
      "    agent-stage1_train0: -378543.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -820501.92\n",
      "    agent-stage0_train1: -820501.92\n",
      "    agent-stage1_train0: -820501.92\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1122320.0\n",
      "    agent-stage0_train1: -1122320.0\n",
      "    agent-stage1_train0: -1122320.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5731973177329417\n",
      "    mean_inference_ms: 1.5093844084977577\n",
      "    mean_processing_ms: 0.3589033228648832\n",
      "  time_since_restore: 1120.115770816803\n",
      "  time_this_iter_s: 13.020576477050781\n",
      "  time_total_s: 1120.115770816803\n",
      "  timers:\n",
      "    learn_throughput: 644.592\n",
      "    learn_time_ms: 6515.75\n",
      "    load_throughput: 312681.634\n",
      "    load_time_ms: 13.432\n",
      "    sample_throughput: 641.425\n",
      "    sample_time_ms: 6547.923\n",
      "    update_time_ms: 4.047\n",
      "  timestamp: 1608025473\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 357000\n",
      "  training_iteration: 85\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         1120.12</td><td style=\"text-align: right;\">357000</td><td style=\"text-align: right;\">-2.46151e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-44-46\n",
      "  done: false\n",
      "  episode_len_mean: 374.81\n",
      "  episode_reward_max: -1135629.0\n",
      "  episode_reward_mean: -2310637.44\n",
      "  episode_reward_min: -3366960.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 738\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.7482016268638434e-12\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.9495004369202196e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -8.665774053315808e-18\n",
      "        model: {}\n",
      "        policy_loss: 0.0018250406719744205\n",
      "        total_loss: 9922003968.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 9922003968.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.48054200410842896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.7257381677627563\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011161569505929947\n",
      "        model: {}\n",
      "        policy_loss: -0.0007255356758832932\n",
      "        total_loss: 9883209728.0\n",
      "        vf_explained_var: -7.450580596923828e-09\n",
      "        vf_loss: 9883209728.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 3.9106605981942266e-05\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 2.024056357186055e-06\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.205765311686264e-08\n",
      "        model: {}\n",
      "        policy_loss: 0.0018327063880860806\n",
      "        total_loss: 9920164864.0\n",
      "        vf_explained_var: -3.725290298461914e-09\n",
      "        vf_loss: 9920164864.0\n",
      "    num_steps_sampled: 361200\n",
      "    num_steps_trained: 361200\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.33333333333333\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -378543.0\n",
      "    agent-stage0_train1: -378543.0\n",
      "    agent-stage1_train0: -378543.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -770212.48\n",
      "    agent-stage0_train1: -770212.48\n",
      "    agent-stage1_train0: -770212.48\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1122320.0\n",
      "    agent-stage0_train1: -1122320.0\n",
      "    agent-stage1_train0: -1122320.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.576153712630153\n",
      "    mean_inference_ms: 1.5084876355986083\n",
      "    mean_processing_ms: 0.35878995714819034\n",
      "  time_since_restore: 1133.073391675949\n",
      "  time_this_iter_s: 12.957620859146118\n",
      "  time_total_s: 1133.073391675949\n",
      "  timers:\n",
      "    learn_throughput: 644.641\n",
      "    learn_time_ms: 6515.252\n",
      "    load_throughput: 311380.593\n",
      "    load_time_ms: 13.488\n",
      "    sample_throughput: 642.846\n",
      "    sample_time_ms: 6533.451\n",
      "    update_time_ms: 4.065\n",
      "  timestamp: 1608025486\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 361200\n",
      "  training_iteration: 86\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         1133.07</td><td style=\"text-align: right;\">361200</td><td style=\"text-align: right;\">-2.31064e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-44-59\n",
      "  done: false\n",
      "  episode_len_mean: 374.75\n",
      "  episode_reward_max: -959079.0\n",
      "  episode_reward_mean: -2083139.01\n",
      "  episode_reward_min: -3366960.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 750\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 8.741008134319217e-13\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.948368096171276e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.504729215378888e-17\n",
      "        model: {}\n",
      "        policy_loss: 0.0018655555322766304\n",
      "        total_loss: 5780909056.0\n",
      "        vf_explained_var: -1.1175870895385742e-08\n",
      "        vf_loss: 5780909056.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.48054200410842896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6626795530319214\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011544622480869293\n",
      "        model: {}\n",
      "        policy_loss: 0.0050601358525455\n",
      "        total_loss: 5788072448.0\n",
      "        vf_explained_var: -9.313225746154785e-09\n",
      "        vf_loss: 5788072448.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.9553302990971133e-05\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 3.263750727455772e-07\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.51874189668888e-07\n",
      "        model: {}\n",
      "        policy_loss: -0.003491422161459923\n",
      "        total_loss: 5737588736.0\n",
      "        vf_explained_var: 1.4901161193847656e-08\n",
      "        vf_loss: 5737588736.0\n",
      "    num_steps_sampled: 365400\n",
      "    num_steps_trained: 365400\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.24210526315788\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -319693.0\n",
      "    agent-stage0_train1: -319693.0\n",
      "    agent-stage1_train0: -319693.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -694379.67\n",
      "    agent-stage0_train1: -694379.67\n",
      "    agent-stage1_train0: -694379.67\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1122320.0\n",
      "    agent-stage0_train1: -1122320.0\n",
      "    agent-stage1_train0: -1122320.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.578617433666173\n",
      "    mean_inference_ms: 1.5074688541153296\n",
      "    mean_processing_ms: 0.35865221190928176\n",
      "  time_since_restore: 1146.2177877426147\n",
      "  time_this_iter_s: 13.14439606666565\n",
      "  time_total_s: 1146.2177877426147\n",
      "  timers:\n",
      "    learn_throughput: 644.296\n",
      "    learn_time_ms: 6518.744\n",
      "    load_throughput: 313140.74\n",
      "    load_time_ms: 13.412\n",
      "    sample_throughput: 643.641\n",
      "    sample_time_ms: 6525.372\n",
      "    update_time_ms: 4.077\n",
      "  timestamp: 1608025499\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 365400\n",
      "  training_iteration: 87\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         1146.22</td><td style=\"text-align: right;\">365400</td><td style=\"text-align: right;\">-2.08314e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-45-12\n",
      "  done: false\n",
      "  episode_len_mean: 374.97\n",
      "  episode_reward_max: -639504.0\n",
      "  episode_reward_mean: -1829210.67\n",
      "  episode_reward_min: -3306504.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 761\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 4.3705040671596085e-13\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.950915103914879e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -9.986222317468907e-18\n",
      "        model: {}\n",
      "        policy_loss: 0.0013513998128473759\n",
      "        total_loss: 2683984384.0\n",
      "        vf_explained_var: -2.2351741790771484e-08\n",
      "        vf_loss: 2683984384.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.48054200410842896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6255354881286621\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0064020962454378605\n",
      "        model: {}\n",
      "        policy_loss: -0.001504016574472189\n",
      "        total_loss: 2675544064.0\n",
      "        vf_explained_var: -1.862645149230957e-09\n",
      "        vf_loss: 2675544064.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 9.776651495485567e-06\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 4.096078214388399e-07\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -5.067150254944863e-09\n",
      "        model: {}\n",
      "        policy_loss: -0.0003942688927054405\n",
      "        total_loss: 2676863232.0\n",
      "        vf_explained_var: -9.313225746154785e-09\n",
      "        vf_loss: 2676863232.0\n",
      "    num_steps_sampled: 369600\n",
      "    num_steps_trained: 369600\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.01578947368422\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -213168.0\n",
      "    agent-stage0_train1: -213168.0\n",
      "    agent-stage1_train0: -213168.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -609736.89\n",
      "    agent-stage0_train1: -609736.89\n",
      "    agent-stage1_train0: -609736.89\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1102168.0\n",
      "    agent-stage0_train1: -1102168.0\n",
      "    agent-stage1_train0: -1102168.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5807546681663514\n",
      "    mean_inference_ms: 1.5061827474593588\n",
      "    mean_processing_ms: 0.3584969965102742\n",
      "  time_since_restore: 1159.2325565814972\n",
      "  time_this_iter_s: 13.014768838882446\n",
      "  time_total_s: 1159.2325565814972\n",
      "  timers:\n",
      "    learn_throughput: 644.021\n",
      "    learn_time_ms: 6521.527\n",
      "    load_throughput: 313500.739\n",
      "    load_time_ms: 13.397\n",
      "    sample_throughput: 644.451\n",
      "    sample_time_ms: 6517.172\n",
      "    update_time_ms: 4.029\n",
      "  timestamp: 1608025512\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 369600\n",
      "  training_iteration: 88\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         1159.23</td><td style=\"text-align: right;\">369600</td><td style=\"text-align: right;\">-1.82921e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-45-26\n",
      "  done: false\n",
      "  episode_len_mean: 375.16\n",
      "  episode_reward_max: -440304.0\n",
      "  episode_reward_mean: -1590601.32\n",
      "  episode_reward_min: -3150843.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 771\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 2.1852520335798042e-13\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.952245636820953e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.917854315278902e-17\n",
      "        model: {}\n",
      "        policy_loss: 0.002946757711470127\n",
      "        total_loss: 1996440832.0\n",
      "        vf_explained_var: -2.0489096641540527e-08\n",
      "        vf_loss: 1996440832.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.48054200410842896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5948178172111511\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015104593709111214\n",
      "        model: {}\n",
      "        policy_loss: 0.0019522011280059814\n",
      "        total_loss: 1989823488.0\n",
      "        vf_explained_var: -1.1175870895385742e-08\n",
      "        vf_loss: 1989823488.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 4.888325747742783e-06\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.5303837130886677e-07\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.523065134596436e-08\n",
      "        model: {}\n",
      "        policy_loss: -0.004278750624507666\n",
      "        total_loss: 1980170752.0\n",
      "        vf_explained_var: -1.862645149230957e-08\n",
      "        vf_loss: 1980170752.0\n",
      "    num_steps_sampled: 373800\n",
      "    num_steps_trained: 373800\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.4842105263158\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -146768.0\n",
      "    agent-stage0_train1: -146768.0\n",
      "    agent-stage1_train0: -146768.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -530200.44\n",
      "    agent-stage0_train1: -530200.44\n",
      "    agent-stage1_train0: -530200.44\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -1050281.0\n",
      "    agent-stage0_train1: -1050281.0\n",
      "    agent-stage1_train0: -1050281.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5827482537825044\n",
      "    mean_inference_ms: 1.5053076844234352\n",
      "    mean_processing_ms: 0.35837194629113855\n",
      "  time_since_restore: 1172.34357047081\n",
      "  time_this_iter_s: 13.111013889312744\n",
      "  time_total_s: 1172.34357047081\n",
      "  timers:\n",
      "    learn_throughput: 645.975\n",
      "    learn_time_ms: 6501.804\n",
      "    load_throughput: 321788.916\n",
      "    load_time_ms: 13.052\n",
      "    sample_throughput: 644.805\n",
      "    sample_time_ms: 6513.597\n",
      "    update_time_ms: 4.042\n",
      "  timestamp: 1608025526\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 373800\n",
      "  training_iteration: 89\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         1172.34</td><td style=\"text-align: right;\">373800</td><td style=\"text-align: right;\">-1.5906e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-45-39\n",
      "  done: false\n",
      "  episode_len_mean: 375.25\n",
      "  episode_reward_max: -440304.0\n",
      "  episode_reward_mean: -1367586.96\n",
      "  episode_reward_min: -2803704.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 783\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0926260167899021e-13\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.94937076634039e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -8.035346621264643e-18\n",
      "        model: {}\n",
      "        policy_loss: 0.0005210675299167633\n",
      "        total_loss: 3247811584.0\n",
      "        vf_explained_var: -1.862645149230957e-09\n",
      "        vf_loss: 3247811584.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.48054200410842896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6237014532089233\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005208431743085384\n",
      "        model: {}\n",
      "        policy_loss: 0.0013209236785769463\n",
      "        total_loss: 3245578752.0\n",
      "        vf_explained_var: -7.450580596923828e-09\n",
      "        vf_loss: 3245578752.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 2.4441628738713916e-06\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 4.381257951990847e-07\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -8.206720636394493e-09\n",
      "        model: {}\n",
      "        policy_loss: 0.0030033981893211603\n",
      "        total_loss: 3256986624.0\n",
      "        vf_explained_var: 7.450580596923828e-09\n",
      "        vf_loss: 3256986624.0\n",
      "    num_steps_sampled: 378000\n",
      "    num_steps_trained: 378000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.97222222222223\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -146768.0\n",
      "    agent-stage0_train1: -146768.0\n",
      "    agent-stage1_train0: -146768.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -455862.32\n",
      "    agent-stage0_train1: -455862.32\n",
      "    agent-stage1_train0: -455862.32\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -934568.0\n",
      "    agent-stage0_train1: -934568.0\n",
      "    agent-stage1_train0: -934568.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5851884515289463\n",
      "    mean_inference_ms: 1.5045722090382867\n",
      "    mean_processing_ms: 0.3582526446555601\n",
      "  time_since_restore: 1185.4101996421814\n",
      "  time_this_iter_s: 13.06662917137146\n",
      "  time_total_s: 1185.4101996421814\n",
      "  timers:\n",
      "    learn_throughput: 646.164\n",
      "    learn_time_ms: 6499.896\n",
      "    load_throughput: 327303.747\n",
      "    load_time_ms: 12.832\n",
      "    sample_throughput: 644.717\n",
      "    sample_time_ms: 6514.481\n",
      "    update_time_ms: 4.074\n",
      "  timestamp: 1608025539\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 378000\n",
      "  training_iteration: 90\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         1185.41</td><td style=\"text-align: right;\">378000</td><td style=\"text-align: right;\">-1.36759e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-45-52\n",
      "  done: false\n",
      "  episode_len_mean: 375.26\n",
      "  episode_reward_max: -440304.0\n",
      "  episode_reward_mean: -1209697.5\n",
      "  episode_reward_min: -2107704.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 795\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 5.4631300839495106e-14\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.950512214387583e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.5174123757111635e-17\n",
      "        model: {}\n",
      "        policy_loss: 0.0029352072160691023\n",
      "        total_loss: 3744111616.0\n",
      "        vf_explained_var: 7.450580596923828e-09\n",
      "        vf_loss: 3744111616.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.48054200410842896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6564995050430298\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007336181588470936\n",
      "        model: {}\n",
      "        policy_loss: -0.000584358349442482\n",
      "        total_loss: 3723237376.0\n",
      "        vf_explained_var: -2.421438694000244e-08\n",
      "        vf_loss: 3723237376.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.2220814369356958e-06\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 2.1782759063171397e-07\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.6368041144687595e-08\n",
      "        model: {}\n",
      "        policy_loss: -0.0030316058546304703\n",
      "        total_loss: 3727225344.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 3727225344.0\n",
      "    num_steps_sampled: 382200\n",
      "    num_steps_trained: 382200\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.73684210526316\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -146768.0\n",
      "    agent-stage0_train1: -146768.0\n",
      "    agent-stage1_train0: -146768.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -403232.5\n",
      "    agent-stage0_train1: -403232.5\n",
      "    agent-stage1_train0: -403232.5\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -702568.0\n",
      "    agent-stage0_train1: -702568.0\n",
      "    agent-stage1_train0: -702568.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.58738299638172\n",
      "    mean_inference_ms: 1.503609410864154\n",
      "    mean_processing_ms: 0.3581088287775563\n",
      "  time_since_restore: 1198.4933490753174\n",
      "  time_this_iter_s: 13.083149433135986\n",
      "  time_total_s: 1198.4933490753174\n",
      "  timers:\n",
      "    learn_throughput: 646.404\n",
      "    learn_time_ms: 6497.489\n",
      "    load_throughput: 332803.917\n",
      "    load_time_ms: 12.62\n",
      "    sample_throughput: 644.678\n",
      "    sample_time_ms: 6514.877\n",
      "    update_time_ms: 4.059\n",
      "  timestamp: 1608025552\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 382200\n",
      "  training_iteration: 91\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">     reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         1198.49</td><td style=\"text-align: right;\">382200</td><td style=\"text-align: right;\">-1.2097e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-46-05\n",
      "  done: false\n",
      "  episode_len_mean: 375.21\n",
      "  episode_reward_max: -440304.0\n",
      "  episode_reward_mean: -1133387.25\n",
      "  episode_reward_min: -1893729.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 805\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 2.7315650419747553e-14\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.953263485820482e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.4896563302569467e-17\n",
      "        model: {}\n",
      "        policy_loss: 0.0019011124968528748\n",
      "        total_loss: 5992219136.0\n",
      "        vf_explained_var: 9.313225746154785e-09\n",
      "        vf_loss: 5992219136.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.48054200410842896\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6779781579971313\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022751282900571823\n",
      "        model: {}\n",
      "        policy_loss: 0.005720790941268206\n",
      "        total_loss: 5965416448.0\n",
      "        vf_explained_var: 9.313225746154785e-09\n",
      "        vf_loss: 5965416448.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 6.110407184678479e-07\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 8.627809506833728e-07\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.6671695135528353e-08\n",
      "        model: {}\n",
      "        policy_loss: 0.0018309461884200573\n",
      "        total_loss: 5996861440.0\n",
      "        vf_explained_var: -2.60770320892334e-08\n",
      "        vf_loss: 5996861440.0\n",
      "    num_steps_sampled: 386400\n",
      "    num_steps_trained: 386400\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.57368421052632\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -146768.0\n",
      "    agent-stage0_train1: -146768.0\n",
      "    agent-stage1_train0: -146768.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -377795.75\n",
      "    agent-stage0_train1: -377795.75\n",
      "    agent-stage1_train0: -377795.75\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -631243.0\n",
      "    agent-stage0_train1: -631243.0\n",
      "    agent-stage1_train0: -631243.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.589228050971098\n",
      "    mean_inference_ms: 1.502486670688789\n",
      "    mean_processing_ms: 0.35797000064940904\n",
      "  time_since_restore: 1211.546886920929\n",
      "  time_this_iter_s: 13.053537845611572\n",
      "  time_total_s: 1211.546886920929\n",
      "  timers:\n",
      "    learn_throughput: 646.19\n",
      "    learn_time_ms: 6499.634\n",
      "    load_throughput: 335711.856\n",
      "    load_time_ms: 12.511\n",
      "    sample_throughput: 644.132\n",
      "    sample_time_ms: 6520.407\n",
      "    update_time_ms: 4.038\n",
      "  timestamp: 1608025565\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 386400\n",
      "  training_iteration: 92\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         1211.55</td><td style=\"text-align: right;\">386400</td><td style=\"text-align: right;\">-1.13339e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-46-18\n",
      "  done: false\n",
      "  episode_len_mean: 375.4\n",
      "  episode_reward_max: -440304.0\n",
      "  episode_reward_mean: -1106832.0\n",
      "  episode_reward_min: -1893729.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 816\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.3657825209873777e-14\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.951873972316225e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -5.926105542425877e-18\n",
      "        model: {}\n",
      "        policy_loss: -0.002294918056577444\n",
      "        total_loss: 4842622976.0\n",
      "        vf_explained_var: -2.0489096641540527e-08\n",
      "        vf_loss: 4842622976.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.720812976360321\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6916483640670776\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009019928053021431\n",
      "        model: {}\n",
      "        policy_loss: 0.003953162580728531\n",
      "        total_loss: 4864405504.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 4864405504.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 3.0552035923392395e-07\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 3.619559549861151e-07\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.2601278121546784e-08\n",
      "        model: {}\n",
      "        policy_loss: 0.0002463599666953087\n",
      "        total_loss: 4856270848.0\n",
      "        vf_explained_var: -1.30385160446167e-08\n",
      "        vf_loss: 4856270848.0\n",
      "    num_steps_sampled: 390600\n",
      "    num_steps_trained: 390600\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.05555555555556\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -146768.0\n",
      "    agent-stage0_train1: -146768.0\n",
      "    agent-stage1_train0: -146768.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -368944.0\n",
      "    agent-stage0_train1: -368944.0\n",
      "    agent-stage1_train0: -368944.0\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -631243.0\n",
      "    agent-stage0_train1: -631243.0\n",
      "    agent-stage1_train0: -631243.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.591060596160761\n",
      "    mean_inference_ms: 1.5016988999333236\n",
      "    mean_processing_ms: 0.3578311248535826\n",
      "  time_since_restore: 1224.4886057376862\n",
      "  time_this_iter_s: 12.941718816757202\n",
      "  time_total_s: 1224.4886057376862\n",
      "  timers:\n",
      "    learn_throughput: 647.538\n",
      "    learn_time_ms: 6486.103\n",
      "    load_throughput: 333104.724\n",
      "    load_time_ms: 12.609\n",
      "    sample_throughput: 643.837\n",
      "    sample_time_ms: 6523.393\n",
      "    update_time_ms: 4.029\n",
      "  timestamp: 1608025578\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 390600\n",
      "  training_iteration: 93\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         1224.49</td><td style=\"text-align: right;\">390600</td><td style=\"text-align: right;\">-1.10683e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-46-31\n",
      "  done: false\n",
      "  episode_len_mean: 375.6\n",
      "  episode_reward_max: -412704.0\n",
      "  episode_reward_mean: -1004372.25\n",
      "  episode_reward_min: -1823154.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 828\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 6.828912604936888e-15\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.951357024720384e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.1046730728488443e-17\n",
      "        model: {}\n",
      "        policy_loss: 0.0003940993919968605\n",
      "        total_loss: 2412599296.0\n",
      "        vf_explained_var: 1.862645149230957e-08\n",
      "        vf_loss: 2412599296.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.720812976360321\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6169281005859375\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007745283655822277\n",
      "        model: {}\n",
      "        policy_loss: -0.0006107157096266747\n",
      "        total_loss: 2400939520.0\n",
      "        vf_explained_var: 5.587935447692871e-09\n",
      "        vf_loss: 2400939520.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.5276017961696198e-07\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 3.142751836548996e-07\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.6910171868621546e-09\n",
      "        model: {}\n",
      "        policy_loss: -0.00199620658531785\n",
      "        total_loss: 2400503808.0\n",
      "        vf_explained_var: -1.862645149230957e-08\n",
      "        vf_loss: 2400503808.0\n",
      "    num_steps_sampled: 394800\n",
      "    num_steps_trained: 394800\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.64736842105263\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -137568.0\n",
      "    agent-stage0_train1: -137568.0\n",
      "    agent-stage1_train0: -137568.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -334790.75\n",
      "    agent-stage0_train1: -334790.75\n",
      "    agent-stage1_train0: -334790.75\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -607718.0\n",
      "    agent-stage0_train1: -607718.0\n",
      "    agent-stage1_train0: -607718.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.593219288272841\n",
      "    mean_inference_ms: 1.5011121445675712\n",
      "    mean_processing_ms: 0.35771963755459923\n",
      "  time_since_restore: 1237.5295560359955\n",
      "  time_this_iter_s: 13.040950298309326\n",
      "  time_total_s: 1237.5295560359955\n",
      "  timers:\n",
      "    learn_throughput: 647.274\n",
      "    learn_time_ms: 6488.751\n",
      "    load_throughput: 330940.154\n",
      "    load_time_ms: 12.691\n",
      "    sample_throughput: 643.596\n",
      "    sample_time_ms: 6525.836\n",
      "    update_time_ms: 4.005\n",
      "  timestamp: 1608025591\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 394800\n",
      "  training_iteration: 94\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">      reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         1237.53</td><td style=\"text-align: right;\">394800</td><td style=\"text-align: right;\">-1.00437e+06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-46-44\n",
      "  done: false\n",
      "  episode_len_mean: 375.95\n",
      "  episode_reward_max: -412704.0\n",
      "  episode_reward_mean: -887900.46\n",
      "  episode_reward_min: -1668654.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 839\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 3.414456302468444e-15\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.952309821589564e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -5.408606875551983e-18\n",
      "        model: {}\n",
      "        policy_loss: -0.0015148813836276531\n",
      "        total_loss: 1953127040.0\n",
      "        vf_explained_var: -2.2351741790771484e-08\n",
      "        vf_loss: 1953127040.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.720812976360321\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.6262458562850952\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007083968725055456\n",
      "        model: {}\n",
      "        policy_loss: -0.0033575957641005516\n",
      "        total_loss: 1947504768.0\n",
      "        vf_explained_var: -1.862645149230957e-08\n",
      "        vf_loss: 1947504768.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 7.638008980848099e-08\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.10648898044019e-06\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2300873208914709e-08\n",
      "        model: {}\n",
      "        policy_loss: 0.003072756575420499\n",
      "        total_loss: 1963606016.0\n",
      "        vf_explained_var: -1.30385160446167e-08\n",
      "        vf_loss: 1963606016.0\n",
      "    num_steps_sampled: 399000\n",
      "    num_steps_trained: 399000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.32105263157894\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -137568.0\n",
      "    agent-stage0_train1: -137568.0\n",
      "    agent-stage1_train0: -137568.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -295966.82\n",
      "    agent-stage0_train1: -295966.82\n",
      "    agent-stage1_train0: -295966.82\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -556218.0\n",
      "    agent-stage0_train1: -556218.0\n",
      "    agent-stage1_train0: -556218.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.5949308149900174\n",
      "    mean_inference_ms: 1.5000824344176464\n",
      "    mean_processing_ms: 0.35758011005436513\n",
      "  time_since_restore: 1250.4374454021454\n",
      "  time_this_iter_s: 12.907889366149902\n",
      "  time_total_s: 1250.4374454021454\n",
      "  timers:\n",
      "    learn_throughput: 647.832\n",
      "    learn_time_ms: 6483.166\n",
      "    load_throughput: 330010.824\n",
      "    load_time_ms: 12.727\n",
      "    sample_throughput: 644.156\n",
      "    sample_time_ms: 6520.162\n",
      "    update_time_ms: 3.981\n",
      "  timestamp: 1608025604\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 399000\n",
      "  training_iteration: 95\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         1250.44</td><td style=\"text-align: right;\">399000</td><td style=\"text-align: right;\"> -887900</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-46-57\n",
      "  done: false\n",
      "  episode_len_mean: 376.24\n",
      "  episode_reward_max: -68304.0\n",
      "  episode_reward_mean: -801647.46\n",
      "  episode_reward_min: -1424529.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 849\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.707228151234222e-15\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.954109163515021e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.0268150285645355e-17\n",
      "        model: {}\n",
      "        policy_loss: 0.0019818092696368694\n",
      "        total_loss: 641355904.0\n",
      "        vf_explained_var: -1.6763806343078613e-08\n",
      "        vf_loss: 641355904.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.720812976360321\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5236691236495972\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004880206659436226\n",
      "        model: {}\n",
      "        policy_loss: -0.00031528761610388756\n",
      "        total_loss: 641550912.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 641550912.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 3.8190044904240494e-08\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.537943590956274e-07\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.316308750887174e-08\n",
      "        model: {}\n",
      "        policy_loss: 0.0027503534220159054\n",
      "        total_loss: 641277248.0\n",
      "        vf_explained_var: 5.587935447692871e-09\n",
      "        vf_loss: 641277248.0\n",
      "    num_steps_sampled: 403200\n",
      "    num_steps_trained: 403200\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.87222222222222\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: -22768.0\n",
      "    agent-stage0_train1: -22768.0\n",
      "    agent-stage1_train0: -22768.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -267215.82\n",
      "    agent-stage0_train1: -267215.82\n",
      "    agent-stage1_train0: -267215.82\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -474843.0\n",
      "    agent-stage0_train1: -474843.0\n",
      "    agent-stage1_train0: -474843.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.596542538818976\n",
      "    mean_inference_ms: 1.4993949158000925\n",
      "    mean_processing_ms: 0.3574715572261461\n",
      "  time_since_restore: 1263.3915870189667\n",
      "  time_this_iter_s: 12.954141616821289\n",
      "  time_total_s: 1263.3915870189667\n",
      "  timers:\n",
      "    learn_throughput: 648.275\n",
      "    learn_time_ms: 6478.735\n",
      "    load_throughput: 329866.223\n",
      "    load_time_ms: 12.732\n",
      "    sample_throughput: 643.774\n",
      "    sample_time_ms: 6524.028\n",
      "    update_time_ms: 3.986\n",
      "  timestamp: 1608025617\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 403200\n",
      "  training_iteration: 96\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         1263.39</td><td style=\"text-align: right;\">403200</td><td style=\"text-align: right;\"> -801647</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-47-10\n",
      "  done: false\n",
      "  episode_len_mean: 376.49\n",
      "  episode_reward_max: 19296.0\n",
      "  episode_reward_mean: -728839.5\n",
      "  episode_reward_min: -1424529.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 861\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 8.53614075617111e-16\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.952743068777689e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.0409313620261205e-17\n",
      "        model: {}\n",
      "        policy_loss: -0.0012477205600589514\n",
      "        total_loss: 346091904.0\n",
      "        vf_explained_var: 1.30385160446167e-08\n",
      "        vf_loss: 346091904.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.3604064881801605\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5006454586982727\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007304957136511803\n",
      "        model: {}\n",
      "        policy_loss: -0.000924830324947834\n",
      "        total_loss: 344562752.0\n",
      "        vf_explained_var: -1.862645149230957e-08\n",
      "        vf_loss: 344562752.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 1.9095022452120247e-08\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 5.150629363015469e-07\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.4642503209927327e-08\n",
      "        model: {}\n",
      "        policy_loss: 8.186697959899902e-05\n",
      "        total_loss: 346100928.0\n",
      "        vf_explained_var: 1.1175870895385742e-08\n",
      "        vf_loss: 346100928.0\n",
      "    num_steps_sampled: 407400\n",
      "    num_steps_trained: 407400\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.8157894736842\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 6432.0\n",
      "    agent-stage0_train1: 6432.0\n",
      "    agent-stage1_train0: 6432.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -242946.5\n",
      "    agent-stage0_train1: -242946.5\n",
      "    agent-stage1_train0: -242946.5\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -474843.0\n",
      "    agent-stage0_train1: -474843.0\n",
      "    agent-stage1_train0: -474843.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.598667404682155\n",
      "    mean_inference_ms: 1.4987901893691165\n",
      "    mean_processing_ms: 0.3573792357058084\n",
      "  time_since_restore: 1276.294403553009\n",
      "  time_this_iter_s: 12.902816534042358\n",
      "  time_total_s: 1276.294403553009\n",
      "  timers:\n",
      "    learn_throughput: 650.234\n",
      "    learn_time_ms: 6459.212\n",
      "    load_throughput: 328439.394\n",
      "    load_time_ms: 12.788\n",
      "    sample_throughput: 644.244\n",
      "    sample_time_ms: 6519.272\n",
      "    update_time_ms: 3.961\n",
      "  timestamp: 1608025630\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 407400\n",
      "  training_iteration: 97\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         1276.29</td><td style=\"text-align: right;\">407400</td><td style=\"text-align: right;\"> -728840</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-47-23\n",
      "  done: false\n",
      "  episode_len_mean: 376.62\n",
      "  episode_reward_max: 156696.0\n",
      "  episode_reward_mean: -660711.75\n",
      "  episode_reward_min: -1424529.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 873\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 4.268070378085555e-16\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.952297244844363e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -9.799913119381138e-18\n",
      "        model: {}\n",
      "        policy_loss: 0.00039152055978775024\n",
      "        total_loss: 265423072.0\n",
      "        vf_explained_var: -9.313225746154785e-09\n",
      "        vf_loss: 265423072.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.3604064881801605\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.4762568473815918\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004033888690173626\n",
      "        model: {}\n",
      "        policy_loss: 0.0018981783650815487\n",
      "        total_loss: 265004736.0\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 265004736.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 9.547511226060124e-09\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 9.325208338850643e-08\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.042322387931563e-08\n",
      "        model: {}\n",
      "        policy_loss: -0.006373644806444645\n",
      "        total_loss: 262859840.0\n",
      "        vf_explained_var: -1.862645149230957e-09\n",
      "        vf_loss: 262859840.0\n",
      "    num_steps_sampled: 411600\n",
      "    num_steps_trained: 411600\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.88888888888889\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 52232.0\n",
      "    agent-stage0_train1: 52232.0\n",
      "    agent-stage1_train0: 52232.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -220237.25\n",
      "    agent-stage0_train1: -220237.25\n",
      "    agent-stage1_train0: -220237.25\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -474843.0\n",
      "    agent-stage0_train1: -474843.0\n",
      "    agent-stage1_train0: -474843.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.600327089737073\n",
      "    mean_inference_ms: 1.4980405183801895\n",
      "    mean_processing_ms: 0.3572705236160737\n",
      "  time_since_restore: 1289.2230067253113\n",
      "  time_this_iter_s: 12.928603172302246\n",
      "  time_total_s: 1289.2230067253113\n",
      "  timers:\n",
      "    learn_throughput: 651.498\n",
      "    learn_time_ms: 6446.683\n",
      "    load_throughput: 325913.423\n",
      "    load_time_ms: 12.887\n",
      "    sample_throughput: 643.884\n",
      "    sample_time_ms: 6522.916\n",
      "    update_time_ms: 3.94\n",
      "  timestamp: 1608025643\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 411600\n",
      "  training_iteration: 98\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         1289.22</td><td style=\"text-align: right;\">411600</td><td style=\"text-align: right;\"> -660712</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-47-36\n",
      "  done: false\n",
      "  episode_len_mean: 376.72\n",
      "  episode_reward_max: 156696.0\n",
      "  episode_reward_mean: -597623.25\n",
      "  episode_reward_min: -1424529.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 883\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 2.1340351890427776e-16\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.955614036130431e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -8.533402821430784e-18\n",
      "        model: {}\n",
      "        policy_loss: 0.001207276713103056\n",
      "        total_loss: 633695232.0\n",
      "        vf_explained_var: 1.862645149230957e-09\n",
      "        vf_loss: 633695232.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.18020324409008026\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.5037751197814941\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009204337373375893\n",
      "        model: {}\n",
      "        policy_loss: 0.012248830869793892\n",
      "        total_loss: 640910656.0\n",
      "        vf_explained_var: -3.725290298461914e-09\n",
      "        vf_loss: 640910656.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 4.773755613030062e-09\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.0997933230783019e-07\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -8.892415692685063e-10\n",
      "        model: {}\n",
      "        policy_loss: -0.0019319853745400906\n",
      "        total_loss: 633237568.0\n",
      "        vf_explained_var: -2.2351741790771484e-08\n",
      "        vf_loss: 633237568.0\n",
      "    num_steps_sampled: 415800\n",
      "    num_steps_trained: 415800\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.87368421052632\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 52232.0\n",
      "    agent-stage0_train1: 52232.0\n",
      "    agent-stage1_train0: 52232.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -199207.75\n",
      "    agent-stage0_train1: -199207.75\n",
      "    agent-stage1_train0: -199207.75\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -474843.0\n",
      "    agent-stage0_train1: -474843.0\n",
      "    agent-stage1_train0: -474843.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.601850613380373\n",
      "    mean_inference_ms: 1.497079257553157\n",
      "    mean_processing_ms: 0.3571708054124108\n",
      "  time_since_restore: 1302.1332170963287\n",
      "  time_this_iter_s: 12.910210371017456\n",
      "  time_total_s: 1302.1332170963287\n",
      "  timers:\n",
      "    learn_throughput: 652.758\n",
      "    learn_time_ms: 6434.242\n",
      "    load_throughput: 319216.835\n",
      "    load_time_ms: 13.157\n",
      "    sample_throughput: 644.647\n",
      "    sample_time_ms: 6515.194\n",
      "    update_time_ms: 3.943\n",
      "  timestamp: 1608025656\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 415800\n",
      "  training_iteration: 99\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         1302.13</td><td style=\"text-align: right;\">415800</td><td style=\"text-align: right;\"> -597623</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_ADSyncSched_7c8c2_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-12-15_09-47-49\n",
      "  done: true\n",
      "  episode_len_mean: 376.71\n",
      "  episode_reward_max: 156696.0\n",
      "  episode_reward_mean: -519613.5\n",
      "  episode_reward_min: -1424529.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 894\n",
      "  experiment_id: 9b0d68469c944b5fafb3be21d79837ff\n",
      "  experiment_tag: '0'\n",
      "  hostname: cdh-vm\n",
      "  info:\n",
      "    learner:\n",
      "      agent-stage0_train0:\n",
      "        cur_kl_coeff: 1.0670175945213888e-16\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 6.9548143286080055e-12\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -9.03870181504044e-19\n",
      "        model: {}\n",
      "        policy_loss: -0.005149553529918194\n",
      "        total_loss: 352152512.0\n",
      "        vf_explained_var: -9.313225746154785e-09\n",
      "        vf_loss: 352152512.0\n",
      "      agent-stage0_train1:\n",
      "        cur_kl_coeff: 0.18020324409008026\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 0.4274824261665344\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015308693051338196\n",
      "        model: {}\n",
      "        policy_loss: -8.354894816875458e-06\n",
      "        total_loss: 353795968.0\n",
      "        vf_explained_var: -5.587935447692871e-09\n",
      "        vf_loss: 353795968.0\n",
      "      agent-stage1_train0:\n",
      "        cur_kl_coeff: 2.386877806515031e-09\n",
      "        cur_lr: 0.004999999888241291\n",
      "        entropy: 1.807255358698967e-07\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -2.9550712987713723e-09\n",
      "        model: {}\n",
      "        policy_loss: -0.0019403966143727303\n",
      "        total_loss: 353677600.0\n",
      "        vf_explained_var: 1.1175870895385742e-08\n",
      "        vf_loss: 353677600.0\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 172.16.2.4\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.91666666666667\n",
      "    ram_util_percent: 6.2\n",
      "  pid: 29234\n",
      "  policy_reward_max:\n",
      "    agent-stage0_train0: 52232.0\n",
      "    agent-stage0_train1: 52232.0\n",
      "    agent-stage1_train0: 52232.0\n",
      "  policy_reward_mean:\n",
      "    agent-stage0_train0: -173204.5\n",
      "    agent-stage0_train1: -173204.5\n",
      "    agent-stage1_train0: -173204.5\n",
      "  policy_reward_min:\n",
      "    agent-stage0_train0: -474843.0\n",
      "    agent-stage0_train1: -474843.0\n",
      "    agent-stage1_train0: -474843.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 2.6033257238367007\n",
      "    mean_inference_ms: 1.4964296836006292\n",
      "    mean_processing_ms: 0.35707699474934307\n",
      "  time_since_restore: 1315.2446653842926\n",
      "  time_this_iter_s: 13.111448287963867\n",
      "  time_total_s: 1315.2446653842926\n",
      "  timers:\n",
      "    learn_throughput: 653.392\n",
      "    learn_time_ms: 6427.991\n",
      "    load_throughput: 315831.615\n",
      "    load_time_ms: 13.298\n",
      "    sample_throughput: 643.59\n",
      "    sample_time_ms: 6525.898\n",
      "    update_time_ms: 3.921\n",
      "  timestamp: 1608025669\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 100\n",
      "  trial_id: 7c8c2_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>RUNNING </td><td>172.16.2.4:29234</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         1315.24</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\"> -519614</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.4/54.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/32.28 GiB heap, 0.0/11.13 GiB objects<br>Result logdir: /home/cdhubbs/ray_results/ADMultiAgentSync<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_ADSyncSched_7c8c2_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         1315.24</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\"> -519614</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config={\n",
    "    \"log_level\": \"WARN\",\n",
    "    \"num_workers\": 3,\n",
    "    \"num_cpus_for_driver\": 1,\n",
    "    \"num_cpus_per_worker\": 1,\n",
    "    \"lr\": 5e-3,\n",
    "    \"model\":{\"fcnet_hiddens\": [128, 128]},\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policy_graphs,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "    },\n",
    "    \"env\": env_name\n",
    "}\n",
    "\n",
    "exp_name = 'ADMultiAgentSync'\n",
    "\n",
    "exp_dict = {\n",
    "        'name': exp_name,\n",
    "        'run_or_experiment': 'PPO',\n",
    "        \"stop\": {\n",
    "            \"training_iteration\": 100\n",
    "        },\n",
    "        'checkpoint_freq': 20,\n",
    "        \"config\": config,\n",
    "}\n",
    "ray.init(ignore_reinit_error=True)\n",
    "results = tune.run(**exp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = results.dataframe()\n",
    "path = df['logdir'][0]\n",
    "data = pd.read_csv(path + '/progress.csv')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(data['episode_reward_mean'], c='#f57200')\n",
    "# plt.plot(data['episode_reward_mean'])\n",
    "plt.title('Multi-Agent Training Curve')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Reward')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphadow",
   "language": "python",
   "name": "alphadow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
